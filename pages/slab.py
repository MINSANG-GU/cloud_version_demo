import os, glob, cv2, numpy as np
from PIL import Image, ImageDraw, ImageFont
import streamlit as st
from pdf2image import convert_from_bytes
from doclayout_yolo import YOLOv10
from huggingface_hub import hf_hub_download
from collections import defaultdict
import shutil
import subprocess
import json
import re   # ì´ ì¤„ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
import pandas as pd
from streamlit_drawable_canvas import st_canvas
import concurrent.futures
import time
import pickle
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import Rectangle
import numpy as np
from typing import List, Dict
import io
from typing import List, Dict, Any, Tuple
import re
from pathlib import Path
from io import BytesIO
from openpyxl.styles import Border, Side, Alignment
import warnings
import io
import os
import sys, os


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • & ëª¨ë¸ ë¡œë”© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import get_base_dir, get_ocr_results_folder

BASE_DIR = get_base_dir()
SURYA_RESULTS_FOLDER =get_ocr_results_folder()



Slab_raw_data  = os.path.join(BASE_DIR,"slab", "raw_data")
Slab_table     = os.path.join(BASE_DIR,"slab", "Slab_table")
Slab_anchor    = os.path.join(BASE_DIR,"slab", "Slab_anchor_img")
Slab_anchor_ocr    = os.path.join(BASE_DIR,"slab", "Slab_anchor_ocr")
Slab_elements    = os.path.join(BASE_DIR,"slab", "Slab_elements")
table_ocr_folder  = os.path.join(BASE_DIR,"slab", "Slab_table_ocr")
table_ocr_excel = os.path.join(BASE_DIR,"slab", "Slab_table_ocr_results")




Slab_raw_data_SDD  = os.path.join(BASE_DIR,"slab", "raw_data_SDD")
Slab_column_SDD = os.path.join(BASE_DIR,"slab", "raw_data_column_label_SDD")
Slab_OCR_SDD =os.path.join(BASE_DIR,"slab", "raw_data_OCR_SDD")
Slab_OCR_image_line =os.path.join(BASE_DIR,"slab", "raw_data_line")
Slab_cell = os.path.join(BASE_DIR,"slab", "Slab_cell")
Slab_table_region = os.path.join(BASE_DIR,"slab", "Slab_table_region")
Slab_table_crop_OCR = os.path.join(BASE_DIR,"slab", "Slab_table_crop_OCR")
Slab_same_line = os.path.join(BASE_DIR,"slab", "Slab_same_line")
Slab_table_excel = os.path.join(Slab_table_region,"slab","Slab_excel")
Slab_text_clean = os.path.join(Slab_table_region,"slab","Slab_text_clean")




os.makedirs(Slab_anchor_ocr, exist_ok=True)
os.makedirs(Slab_elements, exist_ok=True)
os.makedirs(table_ocr_folder, exist_ok=True)
os.makedirs(table_ocr_excel, exist_ok=True)
os.makedirs(Slab_raw_data_SDD, exist_ok=True)
os.makedirs(Slab_column_SDD, exist_ok=True)
os.makedirs(Slab_OCR_SDD, exist_ok=True)
os.makedirs(Slab_OCR_image_line, exist_ok=True)
os.makedirs(Slab_cell, exist_ok=True)
os.makedirs(Slab_table_region, exist_ok=True)
os.makedirs(Slab_table_crop_OCR, exist_ok=True)
os.makedirs(Slab_same_line, exist_ok=True)
os.makedirs(Slab_table_excel, exist_ok=True)
os.makedirs(Slab_text_clean, exist_ok=True)





for d in (Slab_raw_data, Slab_table, Slab_anchor):
    os.makedirs(d, exist_ok=True)

CONF_THRESH = 0.3
IOU_THRESH  = 0.3
IMAGE_SIZE  = 1024

model_file = hf_hub_download(
    repo_id="juliozhao/DocLayout-YOLO-DocStructBench",
    filename="doclayout_yolo_docstructbench_imgsz1024.pt"
)
yolo_model = YOLOv10(model_file)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•¨ìˆ˜ ì •ì˜ë¶€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_iou(b1, b2):
    x1,y1,x2,y2 = max(b1[0],b2[0]), max(b1[1],b2[1]), min(b1[2],b2[2]), min(b1[3],b2[3])
    inter = max(0,x2-x1)*max(0,y2-y1)
    a1 = (b1[2]-b1[0])*(b1[3]-b1[1]); a2 = (b2[2]-b2[0])*(b2[3]-b2[1])
    return inter/(a1+a2-inter) if (a1+a2-inter)>0 else 0

def convert_pdf_to_images(uploaded):
    paths=[]
    for idx, img in enumerate(convert_from_bytes(uploaded.read(), dpi=300),1):
        p=os.path.join(Slab_raw_data, f"page_{idx}.png")
        img.save(p,"PNG"); paths.append(p)
    return paths

def analyze_first_page():
    files = sorted(f for f in os.listdir(Slab_raw_data) if f.endswith(".png"))
    if not files: return None, []
    path = os.path.join(Slab_raw_data, files[0])
    img = Image.open(path).convert("RGB")
    res = yolo_model.predict(path, imgsz=IMAGE_SIZE, conf=CONF_THRESH, iou=IOU_THRESH)[0]
    boxes = [list(map(int,b.xyxy[0].tolist())) for b in res.boxes]
    return img, boxes

def extract_with_offset(anchors, margin_right=150):
    top_idx = min(range(len(anchors)), key=lambda i: anchors[i][1])

    img_paths = sorted(glob.glob(os.path.join(Slab_raw_data, "*.png")))
    progress_bar = st.progress(0)
    status_text  = st.empty()
    shown = 0

    for idx, p in enumerate(img_paths, start=1):
        status_text.text(f"ğŸ”„ ì²˜ë¦¬ ì¤‘: {os.path.basename(p)} ({idx}/{len(img_paths)})")
        page_im = Image.open(p).convert("RGB")
        page_np = np.array(page_im)
        page_gray = cv2.cvtColor(page_np, cv2.COLOR_RGB2GRAY)

        # layout ë¶„ì„
        res = yolo_model.predict(p, imgsz=IMAGE_SIZE, conf=CONF_THRESH, iou=IOU_THRESH)[0]

        # í…Œì´ë¸” ì €ì¥: í˜ì´ì§€ë³„ ì¹´ìš´í„° ì‚¬ìš©
        page_name = os.path.splitext(os.path.basename(p))[0]
        table_idx = 1
        for b in res.boxes:
            if res.names[int(b.cls.item())] == "table":
                x1,y1,x2,y2 = map(int, b.xyxy[0].tolist())
                crop = page_im.crop((x1, y1, x2, y2))
                fn = f"Salb_{page_name}_table_{table_idx}.png"
                crop.save(os.path.join(Slab_table, fn))
                table_idx += 1

        # IoU ë§¤ì¹­ â†’ ì¤‘ì‹¬ ì´ë™ëŸ‰ ìˆ˜ì§‘
        shifts = []
        page_boxes = [list(map(int,b.xyxy[0].tolist())) for b in res.boxes]
        for anc in anchors:
            best_iou, best_box = 0, None
            for pb in page_boxes:
                iou = compute_iou(anc, pb)
                if iou > best_iou:
                    best_iou, best_box = iou, pb
            if best_box and best_iou >= 0.1:
                cx_a = (anc[0]+anc[2])/2; cy_a = (anc[1]+anc[3])/2
                cx_b = (best_box[0]+best_box[2])/2; cy_b = (best_box[1]+best_box[3])/2
                shifts.append((cx_b-cx_a, cy_b-cy_a))

        # í‰ê·  ì˜¤í”„ì…‹ ê³„ì‚°
        if shifts:
            dx = sum(s[0] for s in shifts)/len(shifts)
            dy = sum(s[1] for s in shifts)/len(shifts)
        else:
            dx = dy = 0

        # ì•µì»¤ í¬ë¡­ & ì €ì¥
        for i, (x1,y1,x2,y2) in enumerate(anchors):
            nx1, ny1 = int(x1+dx), int(y1+dy)
            nx2, ny2 = int(x2+dx), int(y2+dy)
            if i == top_idx:
                nx2 += margin_right
            # clamp
            nx1,ny1 = max(0,nx1), max(0,ny1)
            nx2 = min(page_im.width, nx2); ny2 = min(page_im.height, ny2)
            crop = page_im.crop((nx1,ny1,nx2,ny2))
            fn = f"Slab_{page_name}_box{i}.png"
            crop.save(os.path.join(Slab_anchor, fn))

            if shown < 2:
                st.image(crop, caption=f"ì˜ˆì‹œ Crop {i} on {page_name}", width=200)
                shown += 1

        progress_bar.progress(idx/len(img_paths))

    status_text.text("âœ… ëª¨ë“  í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ")
    st.success(f"í…Œì´ë¸”: '{Slab_table}', ì•µì»¤ í¬ë¡­: '{Slab_anchor}' ì €ì¥ ì™„ë£Œ")






def apply_surya_ocr_to_anchors():
    anchor_folder       = Slab_anchor
    surya_output_folder = Slab_anchor_ocr

    os.makedirs(surya_output_folder, exist_ok=True)

    # ê¸°ì¡´ OCR ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìƒëµ
    existing = [f for f in os.listdir(surya_output_folder) if f.endswith(".json")]
    if existing:
        st.info(f"â„¹ï¸ ì´ë¯¸ {len(existing)}ê°œì˜ OCR ê²°ê³¼ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. Surya OCR ìƒëµ.")
        return

    # ì…ë ¥ ì´ë¯¸ì§€ í™•ì¸
    images = [f for f in os.listdir(anchor_folder) if f.lower().endswith((".png", ".jpg"))]
    if not images:
        st.error("âŒ í¬ë¡­ ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    progress_bar = st.progress(0)
    status_text  = st.empty()
    st.write("ğŸ” OCR ì‹¤í–‰ ì¤‘...")

    for idx, img_file in enumerate(images):
        in_path = os.path.join(anchor_folder, img_file)
        # surya_ocr ì»¤ë§¨ë“œ ì‹¤í–‰
        cmd = ["surya_ocr", in_path]
        result = subprocess.run(cmd, capture_output=True, text=True)

        stderr = result.stderr.strip()
        if stderr and all(x not in stderr for x in ["Detecting bboxes", "Recognizing Text"]):
            st.warning(f"âš ï¸ ì˜¤ë¥˜: {img_file} â†’ {stderr}")

        pct = int((idx + 1) / len(images) * 100)
        progress_bar.progress(pct)
        status_text.write(f"ğŸ–¼ï¸ OCR ì¤‘: {img_file} ({pct}%)")

    progress_bar.empty()
    status_text.write("âœ… OCR ì™„ë£Œ. ê²°ê³¼ ì´ë™ ì¤‘...")

    # ê²°ê³¼ JSON ì´ë™
    moved = skipped = 0
    for folder in os.listdir(SURYA_RESULTS_FOLDER):
        src = os.path.join(SURYA_RESULTS_FOLDER, folder, "results.json")
        if os.path.exists(src):
            dst = os.path.join(surya_output_folder, f"{folder}.json")
            try:
                shutil.move(src, dst)
                moved += 1
            except Exception as e:
                st.error(f"âŒ ì´ë™ ì˜¤ë¥˜: {folder} â†’ {e}")
        else:
            skipped += 1

    st.success(f"ğŸ“ OCR ê²°ê³¼ {moved}ê°œ ì´ë™ ì™„ë£Œ ({skipped}ê°œ ëˆ„ë½)")



#------------------------------------------------------------------------------------

def extract_field_from_json(ocr_data: dict, key: str, y_tol: int = 10) -> str | None:
    records = next(iter(ocr_data.values()))
    page_rec = records[0]
    lines = page_rec["text_lines"]

    # ìŒìˆ˜ ë¶€í˜¸ì™€ ìˆ«ìÂ·ë¬¸ì ì¡°í•©ì„ ëª¨ë‘ ë§¤ì¹­
    token_pat = r"-?[A-Za-z0-9\.Ã—/Â²Â³Î¼Î¼%]+"

    for line in lines:
        txt = line["text"]
        m = re.search(fr"\b{key}\b", txt, re.IGNORECASE)
        if not m:
            continue

        tail = txt[m.end():]
        # ë¶€í˜¸ëŠ” ë‚¨ê¸°ê³ , ê·¸ ë°–ì˜ ì„ í–‰ ë¬¸ìë¥¼ ì œê±°
        tail = re.sub(r'^[^A-Za-z0-9\-]+', '', tail)

        parts = tail.split()
        if parts:
            first = parts[0]
            # 1) "-" í† í°ë§Œ ë‚˜ì˜¬ ë•ŒëŠ” ë‹¤ìŒ í† í°ê³¼ í•©ì¹œë‹¤
            if first in ("-", "â€“") and len(parts) > 1:
                tok = "-" + parts[1]
            # 2) ìˆœìˆ˜ ìˆ«ì ë‹¤ìŒ ë¬¸ì ì¡°í•© í•©ì¹˜ê¸° (ì˜ˆ: "1" + "S5A")
            elif len(parts) > 1 and re.fullmatch(r"-?\d+", first) and re.fullmatch(r"[A-Za-z0-9]+", parts[1]):
                tok = first + parts[1]
            else:
                tok = first

            # ìµœì¢… í† í°ì—ì„œ ìŒìˆ˜Â·ë¬¸ì ì¡°í•© íŒ¨í„´ ì¶”ì¶œ
            vm = re.match(token_pat, tok)
            return vm.group(0) if vm else tok

        # ë¶„ë¦¬ëœ ë°•ìŠ¤ ì¼€ì´ìŠ¤ì—ì„œë„ ë™ì¼ ë¡œì§ ì ìš©
        kb = line["bbox"]
        ky = (kb[1] + kb[3]) / 2
        candidates = []
        for o in lines:
            ob = o["bbox"]
            oy = (ob[1] + ob[3]) / 2
            if abs(oy - ky) <= y_tol and ob[0] > kb[2]:
                text2 = re.sub(r'^[^A-Za-z0-9\-]+', '', o["text"])
                p2 = text2.split()
                if not p2:
                    continue
                first2 = p2[0]
                if first2 in ("-", "â€“") and len(p2) > 1:
                    tok2 = "-" + p2[1]
                elif len(p2) > 1 and re.fullmatch(r"-?\d+", first2) and re.fullmatch(r"[A-Za-z0-9]+", p2[1]):
                    tok2 = first2 + p2[1]
                else:
                    tok2 = first2

                vm2 = re.match(token_pat, tok2)
                candidates.append((ob[0], vm2.group(0) if vm2 else tok2))

        if candidates:
            candidates.sort(key=lambda x: x[0])
            return candidates[0][1]

    return None

def extract_info_for_page(page_num: int, keys: list[str]) -> dict[str, str | None]:
    """
    page_{page_num}_box*.json ë“¤ì„ ìŠ¬ë™ì•µì»¤OCR í´ë”ì—ì„œ ì°¾ì•„,
    í‚¤ì›Œë“œë³„ ìµœì´ˆ ê°’ì„ ë½‘ì•„ dictë¡œ ë°˜í™˜
    """
    # í•¨ìˆ˜ ë‚´ì—ì„œ OCR ê²°ê³¼ í´ë” ì •ì˜
    anchor_ocr = os.path.join(BASE_DIR, "Slab_anchor_ocr")

    results = {k: None for k in keys}
    pattern = os.path.join(anchor_ocr, f"*page_{page_num}_box*.json")
    for jf in sorted(glob.glob(pattern)):
        ocr_data = json.load(open(jf, encoding="utf-8"))
        for k in keys:
            if results[k] is None:
                val = extract_field_from_json(ocr_data, k)
                if val:
                    results[k] = val
        if all(results.values()):
            break
    return results

def extract_all_pages(keys: list[str]) -> dict[int, dict[str, str | None]]:
    """
    ì•µì»¤ í¬ë¡­ ì´ë¯¸ì§€ê°€ ë“¤ì–´ ìˆëŠ” í´ë”ì—ì„œ page ë²ˆí˜¸ë¥¼ ì¶”ì¶œ,
    ëª¨ë“  í˜ì´ì§€ì— ëŒ€í•´ extract_info_for_pageë¥¼ ì‹¤í–‰,
    Slab_elements í´ë”ì— JSONìœ¼ë¡œ ì €ì¥í•œ ë’¤ ê²°ê³¼ dict ë¦¬í„´
    """

    elements_folder = os.path.join(BASE_DIR, "Slab_elements")
    os.makedirs(elements_folder, exist_ok=True)

    # page ë²ˆí˜¸ ëª©ë¡
    files = glob.glob(os.path.join(anchor_folder, "*page_*_box*.png"))
    page_nums = sorted({
        int(re.search(r"page_(\d+)_box", os.path.basename(f)).group(1))
        for f in files
    })

    all_info = {}
    for pnum in page_nums:
        info = extract_info_for_page(pnum, keys)
        outp = os.path.join(elements_folder, f"page_{pnum}_elements.json")
        with open(outp, "w", encoding="utf-8") as fp:
            json.dump(info, fp, ensure_ascii=False, indent=2)
        all_info[pnum] = info

    return all_info











def apply_surya_ocr_to_tables():
    """
    Slab_table í´ë”ì˜ ëª¨ë“  PNG/JPG ì´ë¯¸ì§€ì— ëŒ€í•´ surya_ocrì„ ì‹¤í–‰í•˜ê³ ,
    ê²°ê³¼ JSONì„ Slab_table_ocr í´ë”ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
    """





    # 1) ì´ë¯¸ OCR ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
    existing = glob.glob(os.path.join(table_ocr_folder, "*.json"))
    if existing:
        st.info(f"â„¹ï¸ ì´ë¯¸ {len(existing)}ê°œì˜ í…Œì´ë¸” OCR ê²°ê³¼ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ìƒëµí•©ë‹ˆë‹¤.")
        return

    # 2) ì…ë ¥ ì´ë¯¸ì§€ ìˆ˜ì§‘
    imgs = sorted(f for f in os.listdir(Slab_table)
                  if f.lower().endswith((".png", ".jpg", ".jpeg")))
    if not imgs:
        st.error("âŒ Slab_table í´ë”ì— ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    progress = st.progress(0)
    status   = st.empty()
    st.write("ğŸ” í…Œì´ë¸” OCR ì‹¤í–‰ ì¤‘...")

    # 3) ì´ë¯¸ì§€ë³„ OCR í˜¸ì¶œ
    for idx, img_name in enumerate(imgs, start=1):
        in_path = os.path.join(Slab_table, img_name)
        cmd = ["surya_ocr", in_path]
        res = subprocess.run(cmd, capture_output=True, text=True)

        stderr = res.stderr.strip()
        if stderr and all(x not in stderr for x in ("Detecting bboxes", "Recognizing Text")):
            st.warning(f"âš ï¸ ì˜¤ë¥˜: {img_name} â†’ {stderr}")

        pct = int(idx / len(imgs) * 100)
        progress.progress(pct)
        status.text(f"ğŸ–¼ï¸ OCR ì¤‘: {img_name} ({pct}%)")

    # 4) ê²°ê³¼ ì´ë™
    progress.empty()
    status.text("âœ… OCR ì™„ë£Œ. ê²°ê³¼ ì´ë™ ì¤‘...")

    moved = skipped = 0
    for folder in os.listdir(SURYA_RESULTS_FOLDER):
        src_json = os.path.join(SURYA_RESULTS_FOLDER, folder, "results.json")
        if os.path.exists(src_json):
            dst_json = os.path.join(table_ocr_folder, f"{folder}.json")
            try:
                shutil.move(src_json, dst_json)
                moved += 1
            except Exception as e:
                st.error(f"âŒ ì´ë™ ì˜¤ë¥˜: {folder} â†’ {e}")
        else:
            skipped += 1

    st.success(f"ğŸ“ í…Œì´ë¸” OCR ê²°ê³¼ {moved}ê°œ ì´ë™ ì™„ë£Œ ({skipped}ê°œ ëˆ„ë½)")








def parse_ocr_jsons_to_excel(y_tol: int = 10, x_tol: int = 20) -> str:
    """
    OCR JSON ê²°ê³¼ë“¤ì„ wide í˜•ì‹ì˜ Excel íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    - ì…ì¶œë ¥ ê²½ë¡œëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì •ì˜ë©ë‹ˆë‹¤.
    - y_tol: í–‰ í´ëŸ¬ìŠ¤í„°ë§ ì‹œ y ì¤‘ì‹¬ ì°¨ í—ˆìš©ì¹˜
    - x_tol: ì—´ í´ëŸ¬ìŠ¤í„°ë§ ì‹œ x ì¢Œí‘œ ì°¨ í—ˆìš©ì¹˜
    """
    # â–¶ ì‹¤ì œ ê²½ë¡œë¥¼ ì—¬ê¸°ì— ì§€ì •í•˜ì„¸ìš”
    input_folder = table_ocr_folder
    output_file = table_ocr_excel

    # JSON íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    json_files = sorted(glob.glob(os.path.join(input_folder, "*.json")))
    print(f"[INFO] JSON íŒŒì¼ ìˆ˜: {len(json_files)} (í´ë”: {input_folder})")
    if not json_files:
        print(f"[WARN] JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return ""

    # 1) í…ìŠ¤íŠ¸ ì¡°ê° ìˆ˜ì§‘
    items_all = []
    for jf in json_files:
        fname = os.path.basename(jf)
        try:
            with open(jf, encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            print(f"[ERROR] JSON ë¡œë“œ ì‹¤íŒ¨: {fname} - {e}")
            continue

        val = next(iter(data.values()))
        rec = val[0] if isinstance(val, list) else val

        if isinstance(rec.get("text_lines"), list):
            lines = rec["text_lines"]
        elif isinstance(rec.get("results"), list):
            lines = rec["results"]
        else:
            lines = next((v for v in rec.values()
                          if isinstance(v, list) and v and isinstance(v[0], dict)), [])

        for line in lines:
            if not isinstance(line, dict):
                continue
            bbox = line.get("bbox") or line.get("box")
            if not bbox or len(bbox) != 4:
                continue
            x1, y1, x2, y2 = bbox
            yc = (y1 + y2) / 2

            text = (line.get("text") or line.get("value")
                    or (line.get("words") if isinstance(line.get("words"), str) else None))
            if not text:
                for v in line.values():
                    if isinstance(v, str) and v.strip():
                        text = v.strip()
                        break
            if not text:
                continue

            items_all.append({"file": fname, "text": text, "x1": x1, "yc": yc})
    print(f"[INFO] ì¶”ì¶œëœ í…ìŠ¤íŠ¸ í•­ëª© ìˆ˜: {len(items_all)}")
    if not items_all:
        print("[WARN] ë³€í™˜í•  í…ìŠ¤íŠ¸ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return ""

    # 2) yì¶• í´ëŸ¬ìŠ¤í„°ë§ â†’ í–‰ ê·¸ë£¹í•‘
    rows = []
    for itm in items_all:
        placed = False
        for row in rows:
            avg_y = sum(r["yc"] for r in row) / len(row)
            if abs(itm["yc"] - avg_y) <= y_tol:
                row.append(itm)
                placed = True
                break
        if not placed:
            rows.append([itm])
    print(f"[INFO] ê·¸ë£¹í•‘ëœ í–‰ ìˆ˜: {len(rows)}")

    # 3) xì¶• í´ëŸ¬ìŠ¤í„°ë§ â†’ ì—´ ì„¼í„° ê³„ì‚°
    xs = sorted(it["x1"] for it in items_all)
    bins = []
    for x in xs:
        for b in bins:
            if abs(b[0] - x) <= x_tol:
                b[0] = (b[0] * b[1] + x) / (b[1] + 1)
                b[1] += 1
                break
        else:
            bins.append([x, 1])
    bins.sort(key=lambda b: b[0])
    col_centers = [b[0] for b in bins]
    print(f"[INFO] ì—´ ì„¼í„° ìˆ˜: {len(col_centers)}")

    # 4) long í¬ë§· â†’ wide ì¤€ë¹„
    records = []
    for ridx, row in enumerate(rows, start=1):
        for itm in row:
            diffs = [abs(itm["x1"] - cx) for cx in col_centers]
            cidx = diffs.index(min(diffs)) + 1
            records.append({"file": itm["file"], "row": ridx, "col": cidx, "text": itm["text"]})
    print(f"[INFO] ìƒì„±ëœ ë ˆì½”ë“œ ìˆ˜: {len(records)}")

    # 5) pivot â†’ DataFrame wide
    df_long = pd.DataFrame(records)
    df_wide = df_long.pivot(index=["file", "row"], columns="col", values="text")
    df_wide.columns = [f"col{c}" for c in df_wide.columns]
    df_wide = df_wide.reset_index().fillna("")

    # 6) ì—‘ì…€ ì €ì¥
    if not output_file.lower().endswith(".xlsx"):
        output_file += ".xlsx"
    df_wide.to_excel(output_file, index=False)
    print(f"[DONE] ì €ì¥ ì™„ë£Œ â†’ {output_file}")
    return output_file

############################################################################################################
#############################################################################################################
#êµ¬ì¡°ê³„ì‚°ì„œ ë ì•„ë˜ë¶€í„° êµ¬ì¡°ë„ë©´ ì‹œì‘
###########################################################################################################
###########################################################################################################

def convert_pdf_to_images_SDD(uploaded):
    paths=[]
    for idx, img in enumerate(convert_from_bytes(uploaded.read(), dpi=300),1):
        p=os.path.join(Slab_raw_data_SDD, f"page_{idx}.png")
        img.save(p,"PNG"); paths.append(p)
    return paths


def draw_and_save_bounding_boxes_slab_SDD(canvas_key: str = "box_saver") -> None:
    """
    â€¢ RAW_DATA_FOLDERì—ì„œ ì´ë¯¸ì§€ë¥¼ ì„ íƒ
    â€¢ ì„ íƒëœ ì´ë¯¸ì§€ë¥¼ ì£¼ì–´ì§„ MAX í¬ê¸°ì— ë§ì¶° ì¶•ì†Œí•´ ìº”ë²„ìŠ¤ ë°°ê²½ìœ¼ë¡œ ë„ìš°ê³ 
    â€¢ ì‚¬ê°í˜•ì„ ê·¸ë ¤ ì›ë³¸ ì¢Œí‘œë¡œ ì—­ìŠ¤ì¼€ì¼í•˜ì—¬ BOX_COORD_FOLDERì— ì €ì¥
    â€¢ ê° ë°”ìš´ë”© ë°•ìŠ¤ ì•„ë˜ ì˜ì—­ì„ ì˜ë¼ë‚´ì–´ ê°™ì€ í´ë”ì— ì´ë¯¸ì§€ë¡œ ì €ì¥
    â€¢ cropëœ ì´ë¯¸ì§€ì— ë§ëŠ” ë³€í™˜ëœ box ì¢Œí‘œë„ ê°ê° jsonìœ¼ë¡œ ì €ì¥
    """
    imgs = [f for f in os.listdir(Slab_raw_data_SDD) if f.lower().endswith((".png", ".jpg", ".jpeg"))]
    if not imgs:
        st.warning(f"ì´ë¯¸ì§€ ì—†ìŒ: {Slab_raw_data_SDD}")
        return

    selected = st.selectbox("ì´ë¯¸ì§€ ì„ íƒ", imgs, key=canvas_key + "_sel")
    img_path = os.path.join(Slab_raw_data_SDD, selected)
    image = Image.open(img_path).convert("RGB")

    MAX_W, MAX_H = 800, 600
    scale = min(1.0, MAX_W / image.width, MAX_H / image.height)
    disp_w, disp_h = int(image.width * scale), int(image.height * scale)
    img_small = image.resize((disp_w, disp_h), Image.LANCZOS)

    offset_x = (MAX_W - disp_w) // 2
    offset_y = (MAX_H - disp_h) // 2

    canvas_res = st_canvas(
        background_image=img_small,
        drawing_mode="rect",
        stroke_width=2,
        stroke_color="#FF0000",
        fill_color="rgba(255,0,0,0.3)",
        update_streamlit=True,
        key=canvas_key,
        width=MAX_W,
        height=MAX_H
    )

    if canvas_res and canvas_res.json_data and canvas_res.json_data.get("objects"):
        boxes = []
        for o in canvas_res.json_data["objects"]:
            if o.get("type") == "rect":
                boxes.append({
                    "left": round((o["left"] - offset_x) / scale, 2),
                    "top": round((o["top"] - offset_y) / scale, 2),
                    "width": round(o["width"] / scale, 2),
                    "height": round(o["height"] / scale, 2)
                })

        os.makedirs(Slab_column_SDD, exist_ok=True)

        # ì „ì²´ box ì¢Œí‘œ ì €ì¥ (ì›ë³¸ ì¢Œí‘œ ê¸°ì¤€)
        out_json = os.path.join(Slab_column_SDD, f"{os.path.splitext(selected)[0]}_boxes.json")
        with open(out_json, "w", encoding="utf-8") as f:
            json.dump(boxes, f, ensure_ascii=False, indent=2)

        # ë°•ìŠ¤ë³„ë¡œ ì˜ë¼ë‚´ê³ , ë³€í™˜ëœ box ì¢Œí‘œ json ì €ì¥
        for idx, b in enumerate(boxes):
            y_start = int(b["top"])
            cropped = image.crop((0, y_start, image.width, image.height))
            crop_name = f"{os.path.splitext(selected)[0]}_{idx+1}.png"
            cropped.save(os.path.join(Slab_column_SDD, crop_name))

            # ë³€í™˜ëœ box ì¢Œí‘œ (ì´ cropì´ë¯¸ì§€ ê¸°ì¤€ì—ì„œëŠ” topë§Œ -y_start)
            converted_box = {
                "left": b["left"],
                "top": 0,  # í•­ìƒ 0, ì™œëƒë©´ cropëœ ì´ë¯¸ì§€ì—ì„œ ì´ boxì˜ topì€ 0ì„
                "width": b["width"],
                "height": b["height"]
            }
            out_json_crop = os.path.join(Slab_column_SDD, f"{os.path.splitext(selected)[0]}_{idx+1}_boxes.json")
            with open(out_json_crop, "w", encoding="utf-8") as f:
                json.dump([converted_box], f, ensure_ascii=False, indent=2)

        st.success(f"âœ… {len(boxes)}ê°œ ë°•ìŠ¤ë³„ crop/ì¢Œí‘œ ì €ì¥ ì™„ë£Œ: {Slab_column_SDD}")




def apply_surya_ocr_Wall_slab_SDD():
    surya_output_folder = Slab_OCR_SDD

    os.makedirs(surya_output_folder, exist_ok=True)

    # âœ… ê¸°ì¡´ OCR ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìƒëµ
    existing_jsons = [f for f in os.listdir(surya_output_folder) if f.endswith(".json")]
    if existing_jsons:
        st.info(f"â„¹ï¸ ì´ë¯¸ {len(existing_jsons)}ê°œì˜ OCR ê²°ê³¼ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. Surya OCR ìƒëµ.")
        return

    # âœ… ì…ë ¥ ì´ë¯¸ì§€ í™•ì¸
    image_files = [f for f in os.listdir(Slab_column_SDD) if f.lower().endswith(('.jpg', '.png'))]
    if not image_files:
        st.error("âŒ plain text ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    st.write("ğŸ”OCR ì‹¤í–‰ ì¤‘...")
    progress_bar = st.progress(0)
    status_text = st.empty()


    for idx, image_file in enumerate(image_files):
        input_path = os.path.join(Slab_column_SDD, image_file)
        command = ["surya_ocr", input_path]
        result = subprocess.run(command, capture_output=True, text=True)

        stderr = result.stderr.strip()
        if stderr and all(x not in stderr for x in ["Detecting bboxes", "Recognizing Text"]):
            st.warning(f"âš ï¸ ì˜¤ë¥˜: {image_file} â†’ {stderr}")

        progress = int((idx + 1) / len(image_files) * 100)
        progress_bar.progress(progress)
        status_text.write(f"ğŸ–¼ï¸ OCR ì‹¤í–‰ ì¤‘: {image_file} ({progress}%)")

    progress_bar.empty()
    status_text.write("âœ… OCR ì™„ë£Œ. ê²°ê³¼ ì´ë™ ì¤‘...")

    # âœ… ê²°ê³¼ ì´ë™
    moved, skipped = 0, 0
    for folder_name in os.listdir(SURYA_RESULTS_FOLDER):
        folder_path = os.path.join(SURYA_RESULTS_FOLDER, folder_name)
        if os.path.isdir(folder_path):
            json_file = os.path.join(folder_path, "results.json")
            if os.path.exists(json_file):
                dst_file = os.path.join(surya_output_folder, f"{folder_name}.json")
                try:
                    shutil.move(json_file, dst_file)
                    moved += 1
                except Exception as e:
                    st.error(f"âŒ Move Error: {folder_name} â†’ {e}")
            else:
                skipped += 1

    st.success(f"ğŸ“ OCR ê²°ê³¼ {moved}ê°œ ì´ë™ ì™„ë£Œ ({skipped}ê°œëŠ” ëˆ„ë½ë¨)")
#---------------------------------------------------------------------------------------------
def draw_and_save_bounding_boxes_Slab_SDD(canvas_key: str = "box_line_scroll") -> None:
    """
    ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ ìº”ë²„ìŠ¤ ë²„ì „ (UI ë²„íŠ¼ë“¤ ë³´ì´ë„ë¡ ê°œì„ )
    """
    
    imgs = [f for f in os.listdir(Slab_column_SDD) if f.lower().endswith((".png", ".jpg", ".jpeg"))]
    if not imgs:
        st.warning(f"ì´ë¯¸ì§€ ì—†ìŒ: {Slab_column_SDD}")
        return

    selected = st.selectbox("ì´ë¯¸ì§€ ì„ íƒ", imgs, key=canvas_key + "_sel")
    img_path = os.path.join(Slab_column_SDD, selected)
    image = Image.open(img_path).convert("RGB")

    # ìŠ¤ì¼€ì¼ë§ ì˜µì…˜ ì œê³µ
    col1, col2 = st.columns([3, 1])
    with col1:
        scale = st.slider("ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •", 0.3, 2.5, 1.0, 0.1, key=canvas_key + "_scale")
    with col2:
        st.write("**ê¶Œì¥ í¬ê¸°:**")
        if max(image.width, image.height) < 500:
            recommended = 1.5
            st.info(f"ì‘ì€ì´ë¯¸ì§€ â†’ Ã—{recommended}")
        elif max(image.width, image.height) > 1500:
            recommended = 0.6
            st.info(f"í°ì´ë¯¸ì§€ â†’ Ã—{recommended}")
        else:
            recommended = 1.0
            st.info(f"ì¤‘ê°„ì´ë¯¸ì§€ â†’ Ã—{recommended}")
    
    canvas_width = int(image.width * scale)
    canvas_height = int(image.height * scale)
    
    # ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ
    img_resized = image.resize((canvas_width, canvas_height), Image.LANCZOS)
    
    st.write(f"ğŸ“ **ì´ë¯¸ì§€ ì •ë³´**: ì›ë³¸ {image.width}Ã—{image.height} â†’ ìº”ë²„ìŠ¤ {canvas_width}Ã—{canvas_height} (Ã—{scale:.2f})")
    
    # ìº”ë²„ìŠ¤ ë†’ì´ ì œí•œ (UI ë²„íŠ¼ë“¤ì´ ë³´ì´ë„ë¡)
    max_canvas_height = 700  # UI ë²„íŠ¼ ê³µê°„ í™•ë³´
    
    if canvas_height > max_canvas_height:
        st.warning(f"âš ï¸ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ ({canvas_height}px). ìŠ¤ì¼€ì¼ì„ ì¤„ì´ê±°ë‚˜ ìŠ¤í¬ë¡¤í•´ì„œ ì‘ì—…í•˜ì„¸ìš”.")
        
        # ë°©ë²• 1: ì»¨í…Œì´ë„ˆë¡œ ê°ì‹¸ê¸° (ì•½ê°„ì˜ ì¢Œí‘œ ì˜¤ì°¨ ìˆì„ ìˆ˜ ìˆìŒ)
        use_container = st.checkbox("ìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆ ì‚¬ìš© (UI ë²„íŠ¼ ë³´ì„)", value=True, key=canvas_key + "_container")
        
        if use_container:
            st.markdown(f"""
            <div style="
                width: 100%; 
                height: {max_canvas_height}px; 
                overflow: auto; 
                border: 2px solid #ddd; 
                border-radius: 8px;
                background: #f8f9fa;
                padding: 5px;
                margin: 10px 0;
            ">
            """, unsafe_allow_html=True)
            
            canvas_res = st_canvas(
                background_image=img_resized,
                drawing_mode="rect",
                stroke_width=max(1, int(2 * scale)),
                stroke_color="#FF0000",
                fill_color="rgba(255,0,0,0.3)",
                update_streamlit=True,
                key=canvas_key + "_scroll",
                width=canvas_width,
                height=canvas_height,
            )
            
            st.markdown("</div>", unsafe_allow_html=True)
            
        else:
            # ë°©ë²• 2: ê·¸ëƒ¥ ì›ë³¸ í¬ê¸°ë¡œ (UIëŠ” ì˜ë¦´ ìˆ˜ ìˆì§€ë§Œ ì¢Œí‘œ ì •í™•)
            st.info("ğŸ’¡ í˜ì´ì§€ë¥¼ ì•„ë˜ë¡œ ìŠ¤í¬ë¡¤í•˜ì—¬ UI ë²„íŠ¼ë“¤ì„ ì°¾ìœ¼ì„¸ìš”.")
            canvas_res = st_canvas(
                background_image=img_resized,
                drawing_mode="rect",
                stroke_width=max(1, int(2 * scale)),
                stroke_color="#FF0000",
                fill_color="rgba(255,0,0,0.3)",
                update_streamlit=True,
                key=canvas_key + "_full",
                width=canvas_width,
                height=canvas_height,
            )
    else:
        # ì ë‹¹í•œ í¬ê¸°ë©´ ê·¸ëƒ¥ í‘œì‹œ
        canvas_res = st_canvas(
            background_image=img_resized,
            drawing_mode="rect",
            stroke_width=max(1, int(2 * scale)),
            stroke_color="#FF0000",
            fill_color="rgba(255,0,0,0.3)",
            update_streamlit=True,
            key=canvas_key,
            width=canvas_width,
            height=canvas_height,
        )

    # ì¢Œí‘œ ë³€í™˜
    if canvas_res and canvas_res.json_data and canvas_res.json_data.get("objects"):
        boxes = []
        invalid_boxes = 0
        
        for o in canvas_res.json_data["objects"]:
            if o.get("type") == "rect":
                # ìŠ¤ì¼€ì¼ ë˜ëŒë¦¬ê¸°
                orig_left = o["left"] / scale
                orig_top = o["top"] / scale
                orig_width = o["width"] / scale
                orig_height = o["height"] / scale
                
                # ê²½ê³„ ì²´í¬
                orig_left = max(0, min(orig_left, image.width))
                orig_top = max(0, min(orig_top, image.height))
                orig_right = max(orig_left, min(orig_left + orig_width, image.width))
                orig_bottom = max(orig_top, min(orig_top + orig_height, image.height))
                
                final_width = orig_right - orig_left
                final_height = orig_bottom - orig_top
                
                if final_width > 1 and final_height > 1:
                    boxes.append({
                        "left": round(orig_left, 2),
                        "top": round(orig_top, 2),
                        "width": round(final_width, 2),
                        "height": round(final_height, 2),
                        "right": round(orig_right, 2),
                        "bottom": round(orig_bottom, 2)
                    })
                else:
                    invalid_boxes += 1
        
        if boxes:
            out_file = os.path.join(Slab_same_line, f"{os.path.splitext(selected)[0]}_boxes.json")
            with open(out_file, "w", encoding="utf-8") as f:
                json.dump(boxes, f, ensure_ascii=False, indent=2)
            st.success(f"âœ… {len(boxes)}ê°œ ë°•ìŠ¤ ì €ì¥ì™„ë£Œ!")
            
            # ë°•ìŠ¤ ì •ë³´ ë¯¸ë¦¬ë³´ê¸° (expander ëŒ€ì‹  ê°„ë‹¨ í‘œì‹œ)
            st.info("ğŸ“¦ **ì €ì¥ëœ ë°•ìŠ¤ ì¢Œí‘œ:**")
            for i, box in enumerate(boxes):
                st.write(f"ë°•ìŠ¤ {i+1}: ({box['left']:.1f}, {box['top']:.1f}) ~ ({box['right']:.1f}, {box['bottom']:.1f}) | í¬ê¸°: {box['width']:.1f}Ã—{box['height']:.1f}")
        else:
            st.info("ë°•ìŠ¤ë¥¼ ê·¸ë ¤ì£¼ì„¸ìš”! ğŸ¯")
        
        if invalid_boxes > 0:
            st.warning(f"âš ï¸ {invalid_boxes}ê°œ ë°•ìŠ¤ ì œì™¸ë¨")

# --------------------------------------------------------------------------------







def detect_and_draw_lines_batch(
    img_dir, save_dir=None,
    min_line_length=80, max_line_gap=10,
    hough_threshold=100, morph_kernel_scale=30,
    max_examples=5, return_imgs=False,
    mode="contour",
    block_size=15, C=-2,
    tol=2
):
    img_files = [f for f in os.listdir(img_dir)
                 if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    out_imgs = []
    for i, img_file in enumerate(img_files):
        if max_examples is not None and i >= max_examples:
            break
        img_path = os.path.join(img_dir, img_file)
        img = cv2.imread(img_path)
        if img is None:
            continue

        # 1. ì´ì§„í™” (ë¦¬ì‚¬ì´ì¦ˆ ì œê±°)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        binary = cv2.adaptiveThreshold(
            ~gray, 255,
            cv2.ADAPTIVE_THRESH_MEAN_C,
            cv2.THRESH_BINARY,
            int(block_size) | 1, C
        )

        # 2. ìˆ˜í‰/ìˆ˜ì§ ì„  ì¶”ì¶œ
        hs = max(1, binary.shape[1] // morph_kernel_scale)
        vs = max(1, binary.shape[0] // morph_kernel_scale)
        hor_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (hs, 1))
        ver_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vs))
        horizontal = cv2.dilate(cv2.erode(binary, hor_kernel), hor_kernel)
        vertical   = cv2.dilate(cv2.erode(binary, ver_kernel), ver_kernel)

        result_img = img.copy()  # ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©

        # 3. ì„  ê·¸ë¦¬ê¸°
        if mode == "hough":
            lines_h = cv2.HoughLinesP(horizontal, 1, np.pi/180, hough_threshold,
                                      minLineLength=min_line_length, maxLineGap=max_line_gap)
            if lines_h is not None:
                for l in lines_h:
                    x1, y1, x2, y2 = l[0]
                    cv2.line(result_img, (x1, y1), (x2, y2), (255,0,0), 2)
            lines_v = cv2.HoughLinesP(vertical, 1, np.pi/180, hough_threshold,
                                      minLineLength=min_line_length, maxLineGap=max_line_gap)
            if lines_v is not None:
                for l in lines_v:
                    x1, y1, x2, y2 = l[0]
                    cv2.line(result_img, (x1, y1), (x2, y2), (0,255,0), 2)
        else:
            contours_h, _ = cv2.findContours(horizontal, cv2.RETR_EXTERNAL,
                                             cv2.CHAIN_APPROX_SIMPLE)
            for cnt in contours_h:
                x, y, w, h = cv2.boundingRect(cnt)
                if w > min_line_length and h < (binary.shape[0] // 10):
                    cv2.line(result_img, (x, y + h//2), (x + w, y + h//2), (255,0,0), 2)
            contours_v, _ = cv2.findContours(vertical, cv2.RETR_EXTERNAL,
                                             cv2.CHAIN_APPROX_SIMPLE)
            for cnt in contours_v:
                x, y, w, h = cv2.boundingRect(cnt)
                if h > min_line_length and w < (binary.shape[1] // 10):
                    cv2.line(result_img, (x + w//2, y), (x + w//2, y + h), (0,255,0), 2)

        # 4. êµì°¨ì  ê²€ì¶œ
        kh = np.ones((1, 2*tol+1), np.uint8)
        kv = np.ones((2*tol+1, 1), np.uint8)
        hor_dil = cv2.dilate(horizontal, kh)
        ver_dil = cv2.dilate(vertical, kv)
        mask = cv2.bitwise_and(hor_dil, ver_dil)
        pts = cv2.findNonZero(mask)

        # 5. ì‹œê°í™” ë° ë¦¬ìŠ¤íŠ¸ ì¶•ì 
        intersections = []
        if pts is not None:
            for p in pts:
                x_i, y_i = p[0]
                cv2.circle(result_img, (x_i, y_i), 4, (0,0,255), -1)
                intersections.append((int(x_i), int(y_i)))

        # 6. ê²°ê³¼ ì €ì¥ (ì›ë³¸ í•´ìƒë„ ê·¸ëŒ€ë¡œ)
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            base = os.path.splitext(img_file)[0]

            with open(os.path.join(save_dir, f"{base}_lines.pkl"), "wb") as f:
                pickle.dump({
                    "horizontal": horizontal,
                    "vertical": vertical,
                    "intersections": intersections
                }, f)

            cv2.imwrite(os.path.join(save_dir, f"lines_{img_file}"), result_img)

        if return_imgs:
            out_imgs.append((img_file, result_img, len(intersections)))

    return out_imgs if return_imgs else None


def load_coordinate_files(boxes_file=None, ocr_file=None):
    """
    ì¢Œí‘œ íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    boxes_data = None
    ocr_data = None
    
    if boxes_file and os.path.exists(boxes_file):
        with open(boxes_file, 'r', encoding='utf-8') as f:
            boxes_data = json.load(f)
    
    if ocr_file and os.path.exists(ocr_file):
        with open(ocr_file, 'r', encoding='utf-8') as f:
            ocr_data = json.load(f)
    
    return boxes_data, ocr_data


def is_point_on_line(point_x, point_y, line_start, line_end, tolerance=10):
    """
    ì ì´ ì„  ìœ„ì— ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    """
    x1, y1 = line_start
    x2, y2 = line_end
    
    # ìˆ˜ì§ì„ ì¸ ê²½ìš°
    if abs(x2 - x1) < tolerance:
        return abs(point_x - x1) < tolerance and min(y1, y2) <= point_y <= max(y1, y2)
    
    # ìˆ˜í‰ì„ ì¸ ê²½ìš°
    if abs(y2 - y1) < tolerance:
        return abs(point_y - y1) < tolerance and min(x1, x2) <= point_x <= max(x1, x2)
    
    return False


def find_texts_on_lines(horizontal_lines, vertical_lines, ocr_data, tolerance=15):
    """
    ì„  ìœ„ì— ìˆëŠ” í…ìŠ¤íŠ¸ë“¤ì„ ì°¾ëŠ” í•¨ìˆ˜
    """
    if not ocr_data:
        return []
    
    # OCR ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
    text_boxes = []
    for page_key, page_data in ocr_data.items():
        for item in page_data:
            if 'text_lines' in item:
                for text_line in item['text_lines']:
                    bbox = text_line['bbox']  # [x1, y1, x2, y2]
                    center_x = (bbox[0] + bbox[2]) / 2
                    center_y = (bbox[1] + bbox[3]) / 2
                    
                    text_boxes.append({
                        'text': text_line['text'],
                        'bbox': bbox,
                        'center': (center_x, center_y),
                        'confidence': text_line['confidence']
                    })
    
    # ì„ ê³¼ í…ìŠ¤íŠ¸ ë§¤ì¹­
    texts_on_lines = []
    
    # ìˆ˜í‰ì„ ê³¼ í…ìŠ¤íŠ¸ ë§¤ì¹­
    for line in horizontal_lines:
        for text_box in text_boxes:
            center_x, center_y = text_box['center']
            if is_point_on_line(center_x, center_y, line[0], line[1], tolerance):
                texts_on_lines.append({
                    'text': text_box['text'],
                    'bbox': text_box['bbox'],
                    'line_type': 'horizontal',
                    'line_coords': line,
                    'confidence': text_box['confidence']
                })
    
    # ìˆ˜ì§ì„ ê³¼ í…ìŠ¤íŠ¸ ë§¤ì¹­
    for line in vertical_lines:
        for text_box in text_boxes:
            center_x, center_y = text_box['center']
            if is_point_on_line(center_x, center_y, line[0], line[1], tolerance):
                texts_on_lines.append({
                    'text': text_box['text'],
                    'bbox': text_box['bbox'],
                    'line_type': 'vertical',
                    'line_coords': line,
                    'confidence': text_box['confidence']
                })
    
    return texts_on_lines


def draw_texts_on_image(img, texts_on_lines, user_boxes=None):
    """
    ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ë¥¼ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
    """
    result_img = img.copy()
    
    # OCR í…ìŠ¤íŠ¸ í‘œì‹œ (ì„  ìœ„ì˜ í…ìŠ¤íŠ¸)
    for text_info in texts_on_lines:
        bbox = text_info['bbox']
        text = text_info['text']
        line_type = text_info['line_type']
        
        # í…ìŠ¤íŠ¸ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
        color = (255, 255, 0) if line_type == 'horizontal' else (0, 255, 255)  # ë…¸ë€ìƒ‰/ì‹œì•ˆìƒ‰
        cv2.rectangle(result_img, 
                     (int(bbox[0]), int(bbox[1])), 
                     (int(bbox[2]), int(bbox[3])), 
                     color, 2)
        
        # í…ìŠ¤íŠ¸ í‘œì‹œ
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        thickness = 2
        
        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)
        
        # í…ìŠ¤íŠ¸ ë°°ê²½ ê·¸ë¦¬ê¸°
        cv2.rectangle(result_img,
                     (int(bbox[0]), int(bbox[1]) - text_height - 5),
                     (int(bbox[0]) + text_width, int(bbox[1])),
                     (0, 0, 0), -1)
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        cv2.putText(result_img, text,
                   (int(bbox[0]), int(bbox[1]) - 5),
                   font, font_scale, (255, 255, 255), thickness)
    
    # ì‚¬ìš©ì ì§€ì • ë°•ìŠ¤ í‘œì‹œ
    if user_boxes:
        for box in user_boxes:
            cv2.rectangle(result_img,
                         (int(box['left']), int(box['top'])),
                         (int(box['right']), int(box['bottom'])),
                         (255, 0, 255), 3)  # ë§ˆì  íƒ€ìƒ‰
            
            # ë°•ìŠ¤ ë¼ë²¨
            cv2.putText(result_img, "USER_BOX",
                       (int(box['left']), int(box['top']) - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
    
    return result_img
































def load_coordinate_files(boxes_file=None, ocr_file=None):
    """
    ì¢Œí‘œ íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    boxes_data = None
    ocr_data = None
    
    if boxes_file and os.path.exists(boxes_file):
        with open(boxes_file, 'r', encoding='utf-8') as f:
            boxes_data = json.load(f)
    
    if ocr_file and os.path.exists(ocr_file):
        with open(ocr_file, 'r', encoding='utf-8') as f:
            ocr_data = json.load(f)
    
    return boxes_data, ocr_data


def find_texts_in_same_vertical_line(user_boxes, ocr_data, tolerance=10):
    """
    ì‚¬ìš©ì ë°•ìŠ¤ì™€ ë™ì¼í•œ ìˆ˜ì§ì„ ìƒ(Xì¢Œí‘œ ë²”ìœ„)ì— ìˆëŠ” í…ìŠ¤íŠ¸ë“¤ì„ ì°¾ëŠ” í•¨ìˆ˜
    """
    if not ocr_data or not user_boxes:
        return []
    
    print(f"\n=== find_texts_in_same_vertical_line ë””ë²„ê¹… ===")
    print(f"user_boxes ê°œìˆ˜: {len(user_boxes)}")
    
    # OCR ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
    text_boxes = []
    for page_key, page_data in ocr_data.items():
        for item in page_data:
            if 'text_lines' in item:
                for text_line in item['text_lines']:
                    bbox = text_line['bbox']  # [x1, y1, x2, y2]
                    text_boxes.append({
                        'text': text_line['text'],
                        'bbox': bbox,
                        'confidence': text_line['confidence']
                    })
    
    print(f"ì „ì²´ OCR í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(text_boxes)}")
    print("ëª¨ë“  OCR í…ìŠ¤íŠ¸ë“¤:")
    for i, text_box in enumerate(text_boxes):
        bbox = text_box['bbox']
        center_x = (bbox[0] + bbox[2]) / 2
        print(f"  [{i}] '{text_box['text']}' - bbox:{bbox}, center_x:{center_x:.1f}, conf:{text_box['confidence']:.3f}")
    
    # ì‚¬ìš©ì ë°•ìŠ¤ì™€ ë™ì¼í•œ ìˆ˜ì§ì„ ìƒì˜ í…ìŠ¤íŠ¸ ì°¾ê¸°
    aligned_texts = []
    
    for box_idx, user_box in enumerate(user_boxes):
        user_left = user_box['left']
        user_right = user_box['right']
        print(f"\n--- User Box [{box_idx}]: left={user_left}, right={user_right} ---")
        
        box_aligned_texts = []
        for text_idx, text_box in enumerate(text_boxes):
            text_left = text_box['bbox'][0]  # x1
            text_right = text_box['bbox'][2]  # x2
            text_center_x = (text_left + text_right) / 2
            
            # í…ìŠ¤íŠ¸ê°€ ì‚¬ìš©ì ë°•ìŠ¤ì˜ X ë²”ìœ„ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            condition1 = user_left <= text_center_x <= user_right
            condition2 = text_left <= user_right and text_right >= user_left
            
            if condition1 or condition2:
                aligned_texts.append({
                    'text': text_box['text'],
                    'bbox': text_box['bbox'],
                    'confidence': text_box['confidence'],
                    'aligned_with_box': user_box
                })
                box_aligned_texts.append(text_box['text'])
                print(f"  âœ“ ë§¤ì¹­: [{text_idx}] '{text_box['text']}' (center_x:{text_center_x:.1f})")
            else:
                print(f"  âœ— ë¶ˆì¼ì¹˜: [{text_idx}] '{text_box['text']}' (center_x:{text_center_x:.1f})")
        
        print(f"  ì´ ë°•ìŠ¤ì™€ ë§¤ì¹­ëœ í…ìŠ¤íŠ¸: {box_aligned_texts}")
    
    print(f"\nì´ aligned_texts ê°œìˆ˜: {len(aligned_texts)}")
    print("aligned_texts ë¦¬ìŠ¤íŠ¸:")
    for i, text in enumerate(aligned_texts):
        print(f"  [{i}] '{text['text']}' - bbox:{text['bbox']}")
    
    return aligned_texts


def draw_texts_and_boxes_on_image(img, aligned_texts, user_boxes):
    """
    ì´ë¯¸ì§€ì— ì‚¬ìš©ì ë°•ìŠ¤ì™€ ìˆ˜ì§ ì •ë ¬ëœ í…ìŠ¤íŠ¸ë¥¼ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
    """
    result_img = img.copy()
    
    # ì‚¬ìš©ì ì§€ì • ë°•ìŠ¤ í‘œì‹œ (ë§ˆì  íƒ€ìƒ‰)
    if user_boxes:
        for box in user_boxes:
            cv2.rectangle(result_img,
                         (int(box['left']), int(box['top'])),
                         (int(box['right']), int(box['bottom'])),
                         (255, 0, 255), 3)  # ë§ˆì  íƒ€ìƒ‰
            
            # ë°•ìŠ¤ ë¼ë²¨
            cv2.putText(result_img, "USER_BOX",
                       (int(box['left']), int(box['top']) - 10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)
    
    # ìˆ˜ì§ ì •ë ¬ëœ í…ìŠ¤íŠ¸ í‘œì‹œ (ì´ˆë¡ìƒ‰)
    for text_info in aligned_texts:
        bbox = text_info['bbox']
        text = text_info['text']
        
        # í…ìŠ¤íŠ¸ ë°•ìŠ¤ ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)
        cv2.rectangle(result_img, 
                     (int(bbox[0]), int(bbox[1])), 
                     (int(bbox[2]), int(bbox[3])), 
                     (0, 255, 0), 2)  # ì´ˆë¡ìƒ‰
        
        # í…ìŠ¤íŠ¸ í‘œì‹œ
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        thickness = 2
        
        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)
        
        # í…ìŠ¤íŠ¸ ë°°ê²½ ê·¸ë¦¬ê¸°
        cv2.rectangle(result_img,
                     (int(bbox[0]), int(bbox[1]) - text_height - 5),
                     (int(bbox[0]) + text_width, int(bbox[1])),
                     (0, 0, 0), -1)
        
        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (í°ìƒ‰)
        cv2.putText(result_img, text,
                   (int(bbox[0]), int(bbox[1]) - 5),
                   font, font_scale, (255, 255, 255), thickness)
    
    return result_img


def find_nearby_intersections(text_center, intersections, max_distance=100):
    """
    í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ì£¼ë³€ì˜ êµì°¨ì ë“¤ë§Œ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜ (ë””ë²„ê¹… ì¶”ê°€)
    """
    nearby = []
    tx, ty = text_center
    
    for intersection in intersections:
        ix, iy = intersection
        distance = max(abs(ix - tx), abs(iy - ty))  # ë§¨í•˜íƒ„ ê±°ë¦¬
        if distance <= max_distance:
            nearby.append(intersection)
    
    return nearby


def is_point_on_line_segment(point, line_start, line_end, tolerance=5):
    """
    ì ì´ ì„ ë¶„ ìœ„ì— ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    """
    px, py = point
    x1, y1 = line_start
    x2, y2 = line_end
    
    # ìˆ˜ì§ì„ ì¸ ê²½ìš°
    if abs(x2 - x1) < tolerance:
        return abs(px - x1) < tolerance and min(y1, y2) <= py <= max(y1, y2)
    
    # ìˆ˜í‰ì„ ì¸ ê²½ìš°
    if abs(y2 - y1) < tolerance:
        return abs(py - y1) < tolerance and min(x1, x2) <= px <= max(x1, x2)
    
    return False


def find_line_segment_between_points(point1, point2, horizontal_lines, vertical_lines, tolerance=5):
    """
    ë‘ ì  ì‚¬ì´ì— ì‹¤ì œ ì„ ë¶„ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    """
    x1, y1 = point1
    x2, y2 = point2
    
    # ìˆ˜í‰ ì—°ê²°ì¸ ê²½ìš° (ê°™ì€ Yì¢Œí‘œ)
    if abs(y1 - y2) < tolerance:
        for line_start, line_end in horizontal_lines:
            # ë‘ ì ì´ ëª¨ë‘ ì´ ìˆ˜í‰ì„  ìœ„ì— ìˆê³ , ì„ ë¶„ì´ ë‘ ì ì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
            if (is_point_on_line_segment(point1, line_start, line_end, tolerance) and
                is_point_on_line_segment(point2, line_start, line_end, tolerance)):
                return True
    
    # ìˆ˜ì§ ì—°ê²°ì¸ ê²½ìš° (ê°™ì€ Xì¢Œí‘œ)
    if abs(x1 - x2) < tolerance:
        for line_start, line_end in vertical_lines:
            # ë‘ ì ì´ ëª¨ë‘ ì´ ìˆ˜ì§ì„  ìœ„ì— ìˆê³ , ì„ ë¶„ì´ ë‘ ì ì„ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸
            if (is_point_on_line_segment(point1, line_start, line_end, tolerance) and
                is_point_on_line_segment(point2, line_start, line_end, tolerance)):
                return True
    
    return False


def is_valid_rectangle_on_grid(corners, horizontal_lines, vertical_lines, tolerance=5):
    """
    4ê°œ ëª¨ì„œë¦¬ê°€ ì‹¤ì œ ê²©ìì„ ìœ¼ë¡œ ì—°ê²°ë  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    """
    # corners = [(x1,y1), (x2,y1), (x2,y2), (x1,y2)] ìˆœì„œë¡œ ì •ë ¬ëœ ì‚¬ê°í˜• ëª¨ì„œë¦¬
    if len(corners) != 4:
        return False
    
    # ì‚¬ê°í˜•ì˜ 4ê°œ ë³€ì´ ëª¨ë‘ ì‹¤ì œ ì„ ë¶„ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ì§€ í™•ì¸
    edges = [
        (corners[0], corners[1]),  # ìœ„ìª½ ê°€ë¡œ
        (corners[1], corners[2]),  # ì˜¤ë¥¸ìª½ ì„¸ë¡œ
        (corners[2], corners[3]),  # ì•„ë˜ìª½ ê°€ë¡œ
        (corners[3], corners[0])   # ì™¼ìª½ ì„¸ë¡œ
    ]
    
    for edge_start, edge_end in edges:
        if not find_line_segment_between_points(edge_start, edge_end, horizontal_lines, vertical_lines, tolerance):
            return False
    
    return True


def point_in_rectangle(point, rect_corners):
    """
    ì ì´ ì‚¬ê°í˜• ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
    """
    px, py = point
    x1, y1, x2, y2 = rect_corners  # (left, top, right, bottom)
    return x1 <= px <= x2 and y1 <= py <= y2


def find_text_cells(intersections, horizontal_lines, vertical_lines, aligned_texts, tolerance=5):
    """
    ê° í…ìŠ¤íŠ¸ë¥¼ ë‘˜ëŸ¬ì‹¼ ìµœì†Œ ì…€ì„ ì°¾ëŠ” í•¨ìˆ˜ (ë””ë²„ê¹… ì¶”ê°€ ë²„ì „)
    """
    print(f"\n=== find_text_cells ë””ë²„ê¹… ===")
    print(f"ì „ì²´ êµì°¨ì  ê°œìˆ˜: {len(intersections)}")
    print(f"ìˆ˜í‰ì„  ê°œìˆ˜: {len(horizontal_lines)}")
    print(f"ìˆ˜ì§ì„  ê°œìˆ˜: {len(vertical_lines)}")
    print(f"ì²˜ë¦¬í•  aligned_texts ê°œìˆ˜: {len(aligned_texts)}")
    
    text_cells = []
    
    for text_idx, text_info in enumerate(aligned_texts):
        print(f"\n--- í…ìŠ¤íŠ¸ [{text_idx}]: '{text_info['text']}' ì²˜ë¦¬ ì¤‘ ---")
        
        bbox = text_info['bbox']
        text_center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)
        print(f"í…ìŠ¤íŠ¸ ì¤‘ì‹¬: {text_center}")
        
        # 1. í…ìŠ¤íŠ¸ ì£¼ë³€ êµì°¨ì ë§Œ í•„í„°ë§
        nearby_intersections = find_nearby_intersections(text_center, intersections, max_distance=300)
        print(f"ì£¼ë³€ êµì°¨ì  ê°œìˆ˜ (300px ë‚´): {len(nearby_intersections)}")
        
        if len(nearby_intersections) < 4:
            print(f"  âš ï¸ ì£¼ë³€ êµì°¨ì ì´ ë¶€ì¡±í•¨ (ìµœì†Œ 4ê°œ í•„ìš”, í˜„ì¬ {len(nearby_intersections)}ê°œ)")
            continue
        
        print(f"ì£¼ë³€ êµì°¨ì ë“¤: {nearby_intersections[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
        
        min_area = float('inf')
        best_cell = None
        valid_rectangles_found = 0
        
        # 2. ì£¼ë³€ êµì°¨ì ë“¤ë¡œë§Œ ì‚¬ê°í˜• ìƒì„± ì‹œë„
        for i, p1 in enumerate(nearby_intersections):
            for j, p2 in enumerate(nearby_intersections):
                if i >= j:
                    continue
                for k, p3 in enumerate(nearby_intersections):
                    if k <= j:
                        continue
                    for l, p4 in enumerate(nearby_intersections):
                        if l <= k:
                            continue
                        
                        # 4ê°œ ì ìœ¼ë¡œ ì‚¬ê°í˜• ë§Œë“¤ê¸° ì‹œë„
                        points = [p1, p2, p3, p4]
                        
                        # X, Y ì¢Œí‘œë¡œ ì •ë ¬í•´ì„œ ì‚¬ê°í˜• ëª¨ì„œë¦¬ ì°¾ê¸°
                        x_coords = sorted(set(p[0] for p in points))
                        y_coords = sorted(set(p[1] for p in points))
                        
                        # ì •í™•íˆ 2ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ Xì¢Œí‘œì™€ 2ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ Yì¢Œí‘œê°€ ìˆì–´ì•¼ í•¨
                        if len(x_coords) == 2 and len(y_coords) == 2:
                            left, right = x_coords
                            top, bottom = y_coords
                            
                            # ì‚¬ê°í˜• ëª¨ì„œë¦¬ ì •ì˜
                            corners = [
                                (left, top),    # ì¢Œìƒ
                                (right, top),   # ìš°ìƒ  
                                (right, bottom), # ìš°í•˜
                                (left, bottom)   # ì¢Œí•˜
                            ]
                            
                            # 4ê°œ ëª¨ì„œë¦¬ê°€ ëª¨ë‘ ì£¼ë³€ êµì°¨ì ë“¤ ì¤‘ì— ìˆëŠ”ì§€ í™•ì¸
                            if all(corner in nearby_intersections for corner in corners):
                                # ë©´ì  ì¡°ê¸° ì²´í¬
                                area = (right - left) * (bottom - top)
                                if area >= min_area:
                                    continue
                                
                                # ì‹¤ì œ ê²©ìì„ ìœ¼ë¡œ ì—°ê²°ë˜ëŠ”ì§€ í™•ì¸
                                if is_valid_rectangle_on_grid(corners, horizontal_lines, vertical_lines, tolerance):
                                    valid_rectangles_found += 1
                                    
                                    # í…ìŠ¤íŠ¸ ì¤‘ì‹¬ì´ ì‚¬ê°í˜• ë‚´ë¶€ì— ìˆëŠ”ì§€ í™•ì¸
                                    rect_bounds = (left, top, right, bottom)
                                    if point_in_rectangle(text_center, rect_bounds):
                                        min_area = area
                                        best_cell = {
                                            'text_info': text_info,
                                            'corners': corners,
                                            'bounds': rect_bounds,
                                            'area': area
                                        }
                                        print(f"  âœ“ ìƒˆë¡œìš´ ìµœì  ì…€ ë°œê²¬: bounds={rect_bounds}, area={area:.1f}")
        
        print(f"  ê²€ì‚¬í•œ ìœ íš¨í•œ ì‚¬ê°í˜• ê°œìˆ˜: {valid_rectangles_found}")
        
        if best_cell:
            text_cells.append(best_cell)
            print(f"  âœ… ìµœì¢… ì…€ í™•ì •: {best_cell['bounds']}")
        else:
            print(f"  âŒ ìœ íš¨í•œ ì…€ì„ ì°¾ì§€ ëª»í•¨")
    
    print(f"\nì´ ì°¾ì€ text_cells ê°œìˆ˜: {len(text_cells)}")
    return text_cells



def draw_text_cells_on_image(img, text_cells):
    """
    ì°¾ì€ í…ìŠ¤íŠ¸ ì…€ë“¤ì„ ì´ë¯¸ì§€ì— ê·¸ë¦¬ëŠ” í•¨ìˆ˜
    """
    result_img = img.copy()
    
    for cell in text_cells:
        corners = cell['corners']
        left, top, right, bottom = cell['bounds']
        
        # ì…€ ì‚¬ê°í˜• ê·¸ë¦¬ê¸° (ì£¼í™©ìƒ‰)
        cv2.rectangle(result_img, 
                     (int(left), int(top)), 
                     (int(right), int(bottom)), 
                     (0, 165, 255), 3)  # ì£¼í™©ìƒ‰
        
        # ì…€ ë¼ë²¨
        cv2.putText(result_img, "CELL",
                   (int(left), int(top) - 10),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)
    
    return result_img


# ê¸°ì¡´ í•¨ìˆ˜ì— ì¶”ê°€í•  ë§¤ê°œë³€ìˆ˜ì™€ ë¡œì§
def detect_and_draw_lines_batch(
    img_dir, save_dir=None,
    min_line_length=80, max_line_gap=10,
    hough_threshold=100, morph_kernel_scale=30,
    max_examples=5, return_imgs=False,
    mode="contour",
    block_size=15, C=-2,
    tol=2,
    boxes_coord_dir=None,
    ocr_coord_dir=None, 
    text_tolerance=15
):
    """
    ì„  ê²€ì¶œ + ìˆ˜ì§ ì •ë ¬ í…ìŠ¤íŠ¸ í‘œì‹œ + ì…€ ì°¾ê¸° + ë””ë²„ê¹… ì¶œë ¥
    """
    img_files = [f for f in os.listdir(img_dir)
                 if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    out_imgs = []

    for i, img_file in enumerate(img_files):
        if max_examples is not None and i >= max_examples:
            break

        print(f"\n{'='*50}")
        print(f"ì²˜ë¦¬ ì¤‘ì¸ ì´ë¯¸ì§€: {img_file}")
        print(f"{'='*50}")

        img_path = os.path.join(img_dir, img_file)
        img = cv2.imread(img_path)
        if img is None:
            continue

        # 1. ì´ì§„í™”
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        binary = cv2.adaptiveThreshold(
            ~gray, 255,
            cv2.ADAPTIVE_THRESH_MEAN_C,
            cv2.THRESH_BINARY,
            int(block_size) | 1, C
        )

        # 2. ìˆ˜í‰/ìˆ˜ì§ ì„  ì¶”ì¶œ
        hs = max(1, binary.shape[1] // morph_kernel_scale)
        vs = max(1, binary.shape[0] // morph_kernel_scale)
        hor_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (hs, 1))
        ver_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, vs))
        horizontal = cv2.dilate(cv2.erode(binary, hor_kernel), hor_kernel)
        vertical   = cv2.dilate(cv2.erode(binary, ver_kernel), ver_kernel)

        result_img = img.copy()
        horizontal_lines, vertical_lines = [], []

        # 3. ì„  ê·¸ë¦¬ê¸°
        if mode == "hough":
            lines_h = cv2.HoughLinesP(horizontal, 1, np.pi/180,
                                      hough_threshold,
                                      minLineLength=min_line_length,
                                      maxLineGap=max_line_gap)
            if lines_h is not None:
                for l in lines_h:
                    x1, y1, x2, y2 = l[0]
                    cv2.line(result_img, (x1, y1), (x2, y2), (255,0,0), 2)
                    horizontal_lines.append(((x1, y1), (x2, y2)))
            lines_v = cv2.HoughLinesP(vertical, 1, np.pi/180,
                                      hough_threshold,
                                      minLineLength=min_line_length,
                                      maxLineGap=max_line_gap)
            if lines_v is not None:
                for l in lines_v:
                    x1, y1, x2, y2 = l[0]
                    cv2.line(result_img, (x1, y1), (x2, y2), (0,255,0), 2)
                    vertical_lines.append(((x1, y1), (x2, y2)))
        else:
            cnts_h, _ = cv2.findContours(horizontal, cv2.RETR_EXTERNAL,
                                         cv2.CHAIN_APPROX_SIMPLE)
            for cnt in cnts_h:
                x, y, w, h = cv2.boundingRect(cnt)
                if w > min_line_length and h < (binary.shape[0] // 10):
                    cv2.line(result_img, (x, y+h//2), (x+w, y+h//2), (255,0,0), 2)
                    horizontal_lines.append(((x, y+h//2), (x+w, y+h//2)))
            cnts_v, _ = cv2.findContours(vertical, cv2.RETR_EXTERNAL,
                                         cv2.CHAIN_APPROX_SIMPLE)
            for cnt in cnts_v:
                x, y, w, h = cv2.boundingRect(cnt)
                if h > min_line_length and w < (binary.shape[1] // 10):
                    cv2.line(result_img, (x+w//2, y), (x+w//2, y+h), (0,255,0), 2)
                    vertical_lines.append(((x+w//2, y), (x+w//2, y+h)))

        # 4. êµì°¨ì  ê²€ì¶œ
        kh = np.ones((1, 2*tol+1), np.uint8)
        kv = np.ones((2*tol+1, 1), np.uint8)
        hor_dil = cv2.dilate(horizontal, kh)
        ver_dil = cv2.dilate(vertical, kv)
        mask = cv2.bitwise_and(hor_dil, ver_dil)
        pts = cv2.findNonZero(mask)

        intersections = []
        if pts is not None:
            for p in pts:
                x_i, y_i = p[0]
                cv2.circle(result_img, (x_i, y_i), 4, (0,0,255), -1)
                intersections.append((int(x_i), int(y_i)))

        # 5. OCR ì¢Œí‘œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì…€ ì°¾ê¸°
        # 5. OCR ì¢Œí‘œ ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì…€ ì°¾ê¸°
        aligned_texts, text_cells = [], []
        if boxes_coord_dir or ocr_coord_dir:
            base_name = os.path.splitext(img_file)[0]
            boxes_file = os.path.join(boxes_coord_dir, f"{base_name}_boxes.json") if boxes_coord_dir else None
            ocr_file   = os.path.join(ocr_coord_dir,   f"{base_name}.json") if ocr_coord_dir else None
            
            print(f"\nì¢Œí‘œ íŒŒì¼ ë¡œë“œ:")
            print(f"  boxes_file: {boxes_file}")
            print(f"  ocr_file: {ocr_file}")
            
            boxes_data, ocr_data = load_coordinate_files(boxes_file, ocr_file)
            
            print(f"  boxes_data ë¡œë“œë¨: {boxes_data is not None}")
            print(f"  ocr_data ë¡œë“œë¨: {ocr_data is not None}")
            
            if boxes_data and ocr_data:
                aligned_texts = find_texts_in_same_vertical_line(
                    boxes_data, ocr_data, text_tolerance)
                result_img = draw_texts_and_boxes_on_image(
                    result_img, aligned_texts, boxes_data)
                if aligned_texts and intersections:
                    text_cells = find_text_cells(
                        intersections, horizontal_lines,
                        vertical_lines, aligned_texts,
                        text_tolerance)
                    result_img = draw_text_cells_on_image(result_img, text_cells)

        # 6. í–‰ ì˜ì—­ ì¸ì‹ ë° OCR í…ìŠ¤íŠ¸ ì €ì¥
        regions = []
        def has_horz_segment(x1, x2, y):
            for (sx, sy), (ex, ey) in horizontal_lines:
                if abs(sy-y) < tol and sx <= x1 and ex >= x2:
                    return True
            return False

        for cell in text_cells:
            # 1) ì…€ ë‚´ë¶€ í…ìŠ¤íŠ¸ë¥¼ ë¼ë²¨ë¡œ ì¶”ì¶œ
            label = cell['text_info']['text']            # ì˜ˆ: "cs1" ë˜ëŠ” "s2a"
            safe_label = re.sub(r'[\\/*?:"<>|]', '_', label)  # íŒŒì¼ëª…ì— ì•ˆì „í•˜ê²Œ ë³€í™˜

            # 2) ê¸°ì¡´ x_end ê³„ì‚° ë¡œì§ ìœ ì§€
            x_start, y_top, _, y_bottom = cell['bounds']
            xs = sorted(set([x for x, y in intersections if x >= x_start]))
            x_end = x_start
            for i in range(len(xs)-1):
                x1, x2 = xs[i], xs[i+1]
                if has_horz_segment(x1, x2, y_top) and has_horz_segment(x1, x2, y_bottom):
                    x_end = x2
                else:
                    break

            # 3) ì…€ í¬ë¡­ í›„ ë¼ë²¨ ê¸°ë°˜ìœ¼ë¡œ ì €ì¥
            crop = img[y_top:y_bottom, x_start:x_end]
            if save_dir:
                os.makedirs(save_dir, exist_ok=True)
                crop_path = os.path.join(save_dir, f"{safe_label}.png")
                cv2.imwrite(crop_path, crop)



            # 4) regionsì— text ê¸°ë°˜ ë¼ë²¨ë¡œ ì¶”ê°€
            region = {
                'cell_label': label,                # 'cell_index' ëŒ€ì‹  ì‹¤ì œ í…ìŠ¤íŠ¸
                'bounds': (x_start, y_top, x_end, y_bottom),
                'texts': []
            }
            # OCR í…ìŠ¤íŠ¸ í•„í„°ë§ ë¡œì§ ê·¸ëŒ€ë¡œ
            if boxes_coord_dir and ocr_coord_dir:
                for page in ocr_data.values():
                    for item in page:
                        for line in item.get('text_lines', []):
                            bx1, by1, bx2, by2 = line['bbox']
                            cx, cy = (bx1+bx2)/2, (by1+by2)/2
                            if x_start <= cx <= x_end and y_top <= cy <= y_bottom:
                                region['texts'].append({
                                    'text': line['text'],
                                    'bbox': line['bbox'],
                                    'confidence': line.get('confidence')
                                })
            regions.append(region)

        # 7. ê²°ê³¼ ì €ì¥
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            base = os.path.splitext(img_file)[0]
            # ì´ë¯¸ì§€ ì €ì¥
            cv2.imwrite(os.path.join(save_dir, f"lines_text_{img_file}"), result_img)
            # êµì°¨ì  ë“± ë°ì´í„° í”¼í´
            with open(os.path.join(save_dir, f"{base}_lines.pkl"), "wb") as f:
                pickle.dump({
                    'horizontal': horizontal,
                    'vertical': vertical,
                    'intersections': intersections,
                    'horizontal_lines': horizontal_lines,
                    'vertical_lines': vertical_lines,
                    'aligned_text_count': len(aligned_texts),
                    'text_cells': text_cells,
                    'regions': regions
                }, f)
            # í–‰ ì˜ì—­ í…ìŠ¤íŠ¸ JSON ì €ì¥
            with open(os.path.join(save_dir, f"{base}_regions.json"), 'w', encoding='utf-8') as jf:
                json.dump(regions, jf, ensure_ascii=False, indent=2)

        if return_imgs:
            out_imgs.append((img_file, result_img, len(intersections), len(aligned_texts), len(text_cells)))

    return out_imgs if return_imgs else None
#---------------------------------------------------------------------------------
def clean_text(raw: str) -> str:
    nospace = ''.join(raw.split())
    return re.sub(r'[^0-9A-Za-z@\-\~\(\),]', '', nospace)

def merge_close_texts(texts: list, merge_tol: int = 10) -> list:
    if not texts:
        return []
    items = []
    for t in texts:
        x1, y1, x2, y2 = t['bbox']
        cx, cy = (x1 + x2) / 2, (y1 + y2) / 2
        items.append((t, cx, cy))
    items.sort(key=lambda it: it[1])
    groups, current = [], [items[0]]
    for txt, cx, cy in items[1:]:
        prev_cx, prev_cy = current[-1][1], current[-1][2]
        if abs(cx - prev_cx) <= merge_tol and abs(cy - prev_cy) <= merge_tol:
            current.append((txt, cx, cy))
        else:
            groups.append(current); current = [(txt, cx, cy)]
    groups.append(current)
    merged = []
    for grp in groups:
        if len(grp) == 1:
            merged.append(grp[0][0])
        else:
            texts = [it[0]['text'] for it in grp]
            bboxes = [it[0]['bbox'] for it in grp]
            confs  = [it[0].get('confidence', 0) for it in grp]
            new_text = "".join(texts)
            x1s = [b[0] for b in bboxes]; y1s = [b[1] for b in bboxes]
            x2s = [b[2] for b in bboxes]; y2s = [b[3] for b in bboxes]
            new_bbox = [min(x1s), min(y1s), max(x2s), max(y2s)]
            new_conf = max(confs)
            merged.append({
                'text': new_text,
                'bbox': new_bbox,
                'confidence': new_conf
            })
    return merged

def filter_regions_text(regions: list, merge_tol: int = 10) -> list:
    for region in regions:
        cleaned = []
        for txtinfo in region.get('texts', []):
            txt = clean_text(txtinfo['text'])
            if txt:
                txtinfo['text'] = txt
                cleaned.append(txtinfo)
        region['texts'] = merge_close_texts(cleaned, merge_tol)
    return regions

#----------------------------------------------------------------------------------


#---------------------------------------------------------------------





# def extract_texts_by_cell():
#     """
#     ì‹¤ì œ ê²©ìì„ ê³¼ êµì°¨ì ì„ ì´ìš©í•´ ì •í™•í•œ ì…€ë³„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
#     """
#     pkl_dir = Slab_table_region
    
#     # í”¼í´ íŒŒì¼ë“¤ ì°¾ê¸°
#     pkl_files = [f for f in os.listdir(pkl_dir) if f.endswith('_lines.pkl')]
    
#     if not pkl_files:
#         st.error("í”¼í´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
#         return
    
#     st.write(f"ë°œê²¬ëœ í”¼í´ íŒŒì¼: {len(pkl_files)}ê°œ")
    
#     for pkl_file in pkl_files:
#         st.write(f"ì²˜ë¦¬ ì¤‘: {pkl_file}")
        
#         try:
#             # í”¼í´ íŒŒì¼ ë¡œë“œ
#             with open(os.path.join(pkl_dir, pkl_file), 'rb') as f:
#                 data = pickle.load(f)
            
#             # ì‹¤ì œ ê²©ì ì…€ ìƒì„±
#             grid_cells = create_grid_cells(data['horizontal_lines'], data['vertical_lines'], data['intersections'])
            
#             # ê° ê²©ì ì…€ì— í…ìŠ¤íŠ¸ í• ë‹¹
#             cell_results = []
#             merge_count = 0
            
#             for i, cell in enumerate(grid_cells):
#                 cell_texts = assign_texts_to_grid_cell(cell, data['regions'])
                
#                 # ê°™ì€ ì…€ ë‚´ì˜ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë“¤ì„ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
#                 if len(cell_texts) > 1:
#                     merge_count += 1
#                     st.write(f"ì…€ {i}: {len(cell_texts)}ê°œ í…ìŠ¤íŠ¸ ë³‘í•© ì¤‘...")
#                     for text in cell_texts:
#                         st.write(f"  - '{text['text']}' at {text['bbox']}")
                
#                 merged_texts = merge_texts_in_cell(cell_texts)
                
#                 if len(cell_texts) > 1:
#                     st.write(f"ë³‘í•© ê²°ê³¼: '{merged_texts[0]['text']}'")
#                     st.write("---")
                
#                 cell_results.append({
#                     'cell_index': i,
#                     'cell_bounds': cell['bounds'],
#                     'cell_area': cell['area'],
#                     'row': cell['row'],
#                     'col': cell['col'],
#                     'extracted_texts': merged_texts
#                 })
            
#             st.write(f"ì´ {merge_count}ê°œ ì…€ì—ì„œ í…ìŠ¤íŠ¸ ë³‘í•© ìˆ˜í–‰ë¨")
            
#             # ê²°ê³¼ ì €ì¥
#             base_name = pkl_file.replace('_lines.pkl', '')
#             output_file = os.path.join(Slab_text_clean, f"{base_name}_grid_cells.json")
            
#             with open(output_file, 'w', encoding='utf-8') as f:
#                 json.dump(cell_results, f, ensure_ascii=False, indent=2)
            
#             st.success(f"ê²©ì ì…€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {output_file}")
            
#             # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
#             st.write(f"### {base_name} ê²©ì ì…€ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
#             st.write(f"ì´ {len(cell_results)}ê°œ ì…€ ìƒì„±ë¨")
            
#             # ì²« ë²ˆì§¸ í–‰ë§Œ í‘œì‹œ (ìµœëŒ€ 6ê°œ)
#             first_row_cells = [cell for cell in cell_results if cell['row'] == 0][:6]
            
#             if first_row_cells:
#                 cols = st.columns(len(first_row_cells))
                
#                 for idx, cell in enumerate(first_row_cells):
#                     with cols[idx]:
#                         st.write(f"**ì…€({cell['row']},{cell['col']})**")
#                         if cell['extracted_texts']:
#                             for text in cell['extracted_texts']:
#                                 st.write(f"`{text['text']}`")
#                         else:
#                             st.write("*ë¹ˆ ì…€*")
#             else:
#                 st.write("ì²« ë²ˆì§¸ í–‰ ì…€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
#             # ì „ì²´ ê²°ê³¼ ìš”ì•½
#             total_texts = sum(len(cell['extracted_texts']) for cell in cell_results)
#             st.write(f"ì „ì²´ ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {total_texts}ê°œ")
                    
#         except Exception as e:
#             st.error(f"ì˜¤ë¥˜ ë°œìƒ ({pkl_file}): {str(e)}")


def create_grid_cells(horizontal_lines, vertical_lines, intersections):
    """
    ìˆ˜í‰ì„ ê³¼ ìˆ˜ì§ì„ ì˜ êµì°¨ì ì„ ì´ìš©í•´ ì‹¤ì œ ê²©ì ì…€ë“¤ì„ ìƒì„±
    """
    # ìˆ˜í‰ì„ ê³¼ ìˆ˜ì§ì„  ì¢Œí‘œ ì¶”ì¶œ ë° ì •ë ¬
    h_coords = sorted(set([line[0][1] for line in horizontal_lines] + [line[1][1] for line in horizontal_lines]))
    v_coords = sorted(set([line[0][0] for line in vertical_lines] + [line[1][0] for line in vertical_lines]))
    
    grid_cells = []
    
    # ê° ê²©ì ì…€ ìƒì„±
    for row in range(len(h_coords) - 1):
        for col in range(len(v_coords) - 1):
            y1, y2 = h_coords[row], h_coords[row + 1]
            x1, x2 = v_coords[col], v_coords[col + 1]
            
            # ì…€ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (4ê°œ ëª¨ì„œë¦¬ êµì°¨ì ì´ ëª¨ë‘ ìˆëŠ”ì§€)
            corners_needed = [(x1, y1), (x2, y1), (x1, y2), (x2, y2)]
            corners_found = []
            
            for corner in corners_needed:
                # êµì°¨ì  ê·¼ì²˜ì— ìˆëŠ”ì§€ í™•ì¸ (tolerance=5)
                found = any(abs(pt[0] - corner[0]) <= 5 and abs(pt[1] - corner[1]) <= 5 
                           for pt in intersections)
                if found:
                    corners_found.append(corner)
            
            # 4ê°œ ëª¨ì„œë¦¬ê°€ ëª¨ë‘ ìˆìœ¼ë©´ ìœ íš¨í•œ ì…€
            if len(corners_found) >= 3:  # ì™„ì „í•˜ì§€ ì•Šì•„ë„ 3ê°œ ì´ìƒì´ë©´ ì…€ë¡œ ì¸ì •
                cell = {
                    'bounds': (x1, y1, x2, y2),
                    'area': (x2 - x1) * (y2 - y1),
                    'row': row,
                    'col': col,
                    'corners': corners_found
                }
                grid_cells.append(cell)
    
    return grid_cells


def merge_texts_in_cell(cell_texts):
    """
    ê°™ì€ ì…€ ë‚´ì˜ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë“¤ì„ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
    """
    if len(cell_texts) <= 1:
        return cell_texts
    
    # X ì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì™¼ìª½ë¶€í„° ì˜¤ë¥¸ìª½ ìˆœì„œ)
    sorted_texts = sorted(cell_texts, key=lambda t: t['bbox'][0])
    
    # í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
    merged_text = " ".join([t['text'].strip() for t in sorted_texts if t['text'].strip()])
    
    # í•©ì³ì§„ bbox ê³„ì‚° (ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ìµœì†Œ ì‚¬ê°í˜•)
    all_bboxes = [t['bbox'] for t in sorted_texts]
    merged_bbox = [
        min([bb[0] for bb in all_bboxes]),  # x1 (ê°€ì¥ ì™¼ìª½)
        min([bb[1] for bb in all_bboxes]),  # y1 (ê°€ì¥ ìœ„ìª½)
        max([bb[2] for bb in all_bboxes]),  # x2 (ê°€ì¥ ì˜¤ë¥¸ìª½)
        max([bb[3] for bb in all_bboxes])   # y2 (ê°€ì¥ ì•„ë˜ìª½)
    ]
    
    # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
    avg_confidence = sum([t['confidence'] for t in sorted_texts]) / len(sorted_texts)
    
    # ë³‘í•©ëœ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜
    return [{
        'text': merged_text,
        'bbox': merged_bbox,
        'confidence': avg_confidence,
        'split_type': 'merged',
        'original_count': len(sorted_texts)
    }]


def assign_texts_to_grid_cell(grid_cell, regions):
    """
    íŠ¹ì • ê²©ì ì…€ì— í•´ë‹¹í•˜ëŠ” í…ìŠ¤íŠ¸ë“¤ì„ í• ë‹¹
    """
    x1, y1, x2, y2 = grid_cell['bounds']
    cell_texts = []
    
    # ëª¨ë“  regionì˜ í…ìŠ¤íŠ¸ë“¤ì„ ê²€ì‚¬
    for region in regions:
        for text_item in region['texts']:
            text_bbox = text_item['bbox']  # [x1, y1, x2, y2]
            text_center_x = (text_bbox[0] + text_bbox[2]) / 2
            text_center_y = (text_bbox[1] + text_bbox[3]) / 2
            
            # í…ìŠ¤íŠ¸ ì¤‘ì‹¬ì ì´ ê²©ì ì…€ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
            if (x1 <= text_center_x <= x2 and y1 <= text_center_y <= y2):
                # ê¸´ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ì…€ ê²½ê³„ì— ë§ì¶° ë¶„í• 
                if is_long_text(text_item['text']) and grid_cell['col'] > 0:
                    split_text = split_long_text_by_position(
                        text_item, grid_cell, regions
                    )
                    if split_text:
                        cell_texts.append(split_text)
                else:
                    cell_texts.append({
                        'text': text_item['text'].strip(),
                        'bbox': text_bbox,
                        'confidence': text_item['confidence'],
                        'split_type': 'original'
                    })
    
    return cell_texts


def is_long_text(text):
    """
    ê¸´ í…ìŠ¤íŠ¸ì¸ì§€ íŒë‹¨ (ì—¬ëŸ¬ ê°œì˜ ê°’ì´ ì—°ê²°ëœ í˜•íƒœ)
    """
    # HD, @, ìˆ«ìê°€ ë°˜ë³µë˜ëŠ” íŒ¨í„´ì´ë©´ ê¸´ í…ìŠ¤íŠ¸ë¡œ íŒë‹¨
    hd_count = text.count('HD')
    at_count = text.count('@')
    return hd_count > 1 or at_count > 1 or len(text) > 20


def split_long_text_by_position(text_item, current_cell, regions):
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ í˜„ì¬ ì…€ ìœ„ì¹˜ì— ë§ëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œ
    """
    text = text_item['text']
    text_bbox = text_item['bbox']
    
    # íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
    import re
    
    # "HD ìˆ«ì @ ìˆ«ì" íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
    pattern = r'(HD\s*\d+\s*@\s*\d+|HD\s*\d+|\d+\s*@\s*\d+|\|\s*HD\s*\||\|)'
    parts = re.findall(pattern, text)
    
    if not parts:
        # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ê³µë°±ìœ¼ë¡œ ë¶„í• 
        parts = text.split()
    
    if len(parts) <= 1:
        return {
            'text': text.strip(),
            'bbox': text_bbox,
            'confidence': text_item['confidence'],
            'split_type': 'single'
        }
    
    # í˜„ì¬ ì…€ì˜ ì»¬ëŸ¼ ìœ„ì¹˜ì— ë§ëŠ” ë¶€ë¶„ ì„ íƒ
    col_index = current_cell['col']
    
    # ì²« ë²ˆì§¸ ì»¬ëŸ¼(NAME)ì´ë©´ ì²« ë²ˆì§¸ ë¶€ë¶„
    if col_index <= 2:
        selected_part = parts[0] if parts else text
    else:
        # ë‚˜ë¨¸ì§€ ì»¬ëŸ¼ë“¤ì€ ìˆœì„œëŒ€ë¡œ ë°°ì •
        part_index = min(col_index - 3, len(parts) - 1)
        selected_part = parts[part_index] if part_index >= 0 else ""
    
    if selected_part.strip():
        return {
            'text': selected_part.strip(),
            'bbox': text_bbox,
            'confidence': text_item['confidence'],
            'split_type': 'pattern_split'
        }
    
    return None














#--------------------------------------------------------------------------------

def get_row_bounds(cells_in_row):
    """
    í–‰ì˜ ì „ì²´ ê²½ê³„ ê³„ì‚°
    """
    if not cells_in_row:
        return [0, 0, 0, 0]
    
    x1 = min(cell['cell_bounds'][0] for cell in cells_in_row)
    y1 = min(cell['cell_bounds'][1] for cell in cells_in_row)
    x2 = max(cell['cell_bounds'][2] for cell in cells_in_row)
    y2 = max(cell['cell_bounds'][3] for cell in cells_in_row)
    
    return [x1, y1, x2, y2]


def add_row_labels_to_cells_fixed(cell_results, regions):
    """
    PICKLE íŒŒì¼ì˜ cell_labelì„ Yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì§ì ‘ ë§¤í•‘í•˜ëŠ” ìˆ˜ì •ëœ í•¨ìˆ˜
    """
    # í–‰ë³„ë¡œ ì…€ë“¤ì„ ê·¸ë£¹í•‘
    rows_dict = {}
    for cell in cell_results:
        row_idx = cell['row']
        if row_idx not in rows_dict:
            rows_dict[row_idx] = []
        rows_dict[row_idx].append(cell)
    
    # â˜… PICKLEì˜ regionsì—ì„œ cell_labelì„ Yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì§ì ‘ ì¶”ì¶œ
    region_labels = []
    for region in regions:
        if 'cell_label' in region and region['cell_label']:
            y_coord = region['bounds'][1]  # Y ì¢Œí‘œ
            label = region['cell_label'].strip()
            region_labels.append({
                'y_coord': y_coord,
                'label': label,
                'bounds': region['bounds']
            })
    
    # Yì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    region_labels.sort(key=lambda x: x['y_coord'])
    
    # ê° í–‰ì˜ ë¼ë²¨ ë§¤í•‘
    row_labels = []
    
    for row_idx, cells_in_row in rows_dict.items():
        if not cells_in_row:
            continue
            
        # í˜„ì¬ í–‰ì˜ Yì¢Œí‘œ ë²”ìœ„ ê³„ì‚°
        row_y_min = min(cell['cell_bounds'][1] for cell in cells_in_row)
        row_y_max = max(cell['cell_bounds'][3] for cell in cells_in_row)
        row_y_center = (row_y_min + row_y_max) / 2
        
        # â˜… ê°€ì¥ ê°€ê¹Œìš´ regionì˜ label ì°¾ê¸°
        best_label = "Unknown"
        min_distance = float('inf')
        
        for region_info in region_labels:
            region_y = region_info['y_coord']
            distance = abs(region_y - row_y_center)
            
            # Yì¢Œí‘œê°€ 50px ì´ë‚´ì´ê³  ê°€ì¥ ê°€ê¹Œìš´ ê²ƒ ì„ íƒ
            if distance < 50 and distance < min_distance:
                min_distance = distance
                best_label = region_info['label']
        
        row_labels.append({
            'row_index': row_idx,
            'label': best_label,  # â˜… ì§ì ‘ ë§¤í•‘ëœ ë¼ë²¨
            'cell_count': len(cells_in_row),
            'row_bounds': get_row_bounds(cells_in_row),
            'y_center': row_y_center  # ë””ë²„ê¹…ìš©
        })
    
    # ë¼ë²¨ ë§¤í•‘ ê²°ê³¼ ê²€ì¦
    preserved_count = sum(1 for row in row_labels if row['label'] != "Unknown")
    total_regions = len([r for r in regions if r.get('cell_label', '').strip()])
    
    st.write(f"ë¼ë²¨ ë³´ì¡´ ê²°ê³¼: {preserved_count}/{total_regions} ê°œ ë³´ì¡´ë¨")
    if preserved_count < total_regions:
        st.write(f"âŒ {total_regions - preserved_count}ê°œ ë¼ë²¨ ì†ì‹¤!")
        lost_labels = []
        for region in regions:
            label = region.get('cell_label', '').strip()
            if label and label not in [row['label'] for row in row_labels]:
                lost_labels.append(label)
        st.write(f"ì†ì‹¤ëœ ë¼ë²¨ë“¤: {lost_labels}")
    
    # í–‰ ë¼ë²¨ ì •ë³´ì™€ ì…€ ì •ë³´ë¥¼ í•¨ê»˜ ë°˜í™˜
    return {
        'row_labels': sorted(row_labels, key=lambda x: x['row_index']),
        'cells': cell_results,
        'metadata': {
            'total_rows': len(rows_dict),
            'total_cells': len(cell_results),
            'preserved_labels': preserved_count,
            'total_original_labels': total_regions
        }
    }


def extract_texts_by_cell_with_proper_labels():
    """
    ë¼ë²¨ ë³´ì¡´ì´ ì œëŒ€ë¡œ ë˜ëŠ” ìˆ˜ì •ëœ ë©”ì¸ í•¨ìˆ˜
    """
    pkl_dir = Slab_table_region
    
    # í”¼í´ íŒŒì¼ë“¤ ì°¾ê¸°
    pkl_files = [f for f in os.listdir(pkl_dir) if f.endswith('_lines.pkl')]
    
    if not pkl_files:
        st.error("í”¼í´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return
    
    st.write(f"ë°œê²¬ëœ í”¼í´ íŒŒì¼: {len(pkl_files)}ê°œ")
    
    for pkl_file in pkl_files:
        st.write(f"ì²˜ë¦¬ ì¤‘: {pkl_file}")
        
        try:
            # í”¼í´ íŒŒì¼ ë¡œë“œ
            with open(os.path.join(pkl_dir, pkl_file), 'rb') as f:
                data = pickle.load(f)
            
            # â˜… ì›ë³¸ ë¼ë²¨ í™•ì¸
            st.write("ğŸ“‹ ì›ë³¸ ë¼ë²¨ë“¤:")
            original_labels = []
            for i, region in enumerate(data['regions']):
                label = region.get('cell_label', '').strip()
                if label:
                    original_labels.append(label)
                    st.write(f"  {i}: '{label}' (Y: {region['bounds'][1]})")
            
            # ì‹¤ì œ ê²©ì ì…€ ìƒì„±
            grid_cells = create_grid_cells(data['horizontal_lines'], data['vertical_lines'], data['intersections'])
            
            # ê° ê²©ì ì…€ì— í…ìŠ¤íŠ¸ í• ë‹¹
            cell_results = []
            merge_count = 0
            
            for i, cell in enumerate(grid_cells):
                cell_texts = assign_texts_to_grid_cell(cell, data['regions'])
                
                # ê°™ì€ ì…€ ë‚´ì˜ ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë“¤ì„ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
                if len(cell_texts) > 1:
                    merge_count += 1
                
                merged_texts = merge_texts_in_cell(cell_texts)
                
                cell_results.append({
                    'cell_index': i,
                    'cell_bounds': cell['bounds'],
                    'cell_area': cell['area'],
                    'row': cell['row'],
                    'col': cell['col'],
                    'extracted_texts': merged_texts
                })
            
            st.write(f"ì´ {merge_count}ê°œ ì…€ì—ì„œ í…ìŠ¤íŠ¸ ë³‘í•© ìˆ˜í–‰ë¨")
            
            # â˜… ìˆ˜ì •ëœ ë¼ë²¨ ë§¤í•‘ í•¨ìˆ˜ ì‚¬ìš©
            final_results = add_row_labels_to_cells_fixed(cell_results, data['regions'])
            
            # â˜… ë¼ë²¨ ë³´ì¡´ ê²°ê³¼ í™•ì¸
            st.write("ğŸ·ï¸ ë¼ë²¨ ë³´ì¡´ ê²°ê³¼:")
            preserved_labels = [row['label'] for row in final_results['row_labels'] if row['label'] != "Unknown"]
            unknown_count = len([row for row in final_results['row_labels'] if row['label'] == "Unknown"])
            
            st.write(f"âœ… ë³´ì¡´ëœ ë¼ë²¨: {len(preserved_labels)}ê°œ")
            st.write(f"âŒ Unknown ë¼ë²¨: {unknown_count}ê°œ")
            
            if len(preserved_labels) < len(original_labels):
                lost_labels = set(original_labels) - set(preserved_labels)
                st.error(f"ğŸš¨ ì†ì‹¤ëœ ë¼ë²¨ë“¤: {lost_labels}")
            else:
                st.success("ğŸ‰ ëª¨ë“  ë¼ë²¨ì´ ì„±ê³µì ìœ¼ë¡œ ë³´ì¡´ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ê²°ê³¼ ì €ì¥
            base_name = pkl_file.replace('_lines.pkl', '')
            output_file = os.path.join(Slab_text_clean, f"{base_name}_grid_cells.json")
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_results, f, ensure_ascii=False, indent=2)
            
            st.success(f"ê²©ì ì…€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {output_file}")
            
            # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            st.write(f"### {base_name} ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
            st.write(f"ì´ {len(final_results['cells'])}ê°œ ì…€, {len(final_results['row_labels'])}ê°œ í–‰")
            
            # ë³´ì¡´ëœ ë¼ë²¨ë“¤ í‘œì‹œ
            if preserved_labels:
                st.write("**ë³´ì¡´ëœ ë¼ë²¨ë“¤:**")
                for label in preserved_labels[:10]:  # ì²˜ìŒ 10ê°œë§Œ
                    st.write(f"- {label}")
                    
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ ({pkl_file}): {str(e)}")


def grid_cells_to_dataframe_with_labels(grid_data: dict) -> pd.DataFrame:
    """
    ë¼ë²¨ ì •ë³´ê°€ í¬í•¨ëœ grid_dataì—ì„œ DataFrameì„ ìƒì„± (ìˆ˜ì •ëœ ë²„ì „)
    """
    # row_labelsì—ì„œ ë¼ë²¨ ì •ë³´ ì¶”ì¶œ
    row_label_map = {}
    if 'row_labels' in grid_data:
        for row_info in grid_data['row_labels']:
            row_label_map[row_info['row_index']] = row_info['label']
    
    # cells ë°ì´í„° ì²˜ë¦¬
    cells = grid_data.get('cells', grid_data)  # ì´ì „ ë²„ì „ í˜¸í™˜ì„±
    
    # rowë³„ë¡œ ë°ì´í„° êµ¬ì¡°í™”
    rows = {}
    max_col = 0
    
    for cell in cells:
        row_idx = cell['row']
        col_idx = cell['col']
        max_col = max(max_col, col_idx)
        
        # í–‰ ì´ˆê¸°í™”
        if row_idx not in rows:
            rows[row_idx] = {}
        
        # ì…€ì— í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if cell['extracted_texts']:
            # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš© (ì´ë¯¸ ë³‘í•©ë˜ì—ˆìŒ)
            text = cell['extracted_texts'][0]['text'].strip()
            rows[row_idx][col_idx] = text
    
    # DataFrame ìƒì„±
    df = pd.DataFrame.from_dict(rows, orient='index')
    
    # ì»¬ëŸ¼ ì •ë ¬ (0ë¶€í„° max_colê¹Œì§€)
    all_cols = list(range(max_col + 1))
    df = df.reindex(columns=all_cols, fill_value='')
    
    # í–‰ ì¸ë±ìŠ¤ ì •ë ¬
    df = df.sort_index()
    
    # â˜… ë¼ë²¨ ì •ë³´ë¥¼ í–‰ ì¸ë±ìŠ¤ë¡œ ì¶”ê°€
    if row_label_map:
        new_index = []
        for row_idx in df.index:
            label = row_label_map.get(row_idx, f"Row_{row_idx}")
            new_index.append(f"{label} (Row {row_idx})")
        df.index = new_index
    
    # ì»¬ëŸ¼ëª…ì„ ë” ì˜ë¯¸ìˆê²Œ ë³€ê²½
    col_names = []
    for i in range(len(df.columns)):
        if i == 0:
            col_names.append('TYPE/FLOOR')
        elif i == 1:
            col_names.append('NAME')
        elif i == 2:
            col_names.append('THK(mm)')
        else:
            col_names.append(f'Column_{i}')
    
    df.columns = col_names[:len(df.columns)]
    
    return df


def preview_first_grid_file_with_labels(input_folder: str) -> pd.DataFrame:
    """
    ì²« ë²ˆì§¸ grid_cells íŒŒì¼ì„ ë¼ë²¨ ì •ë³´ì™€ í•¨ê»˜ ë¯¸ë¦¬ë³´ê¸°
    """
    files = sorted(f for f in os.listdir(input_folder) 
                  if f.lower().endswith("_grid_cells.json"))
    
    if not files:
        print("grid_cells.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return pd.DataFrame()
    
    first_path = os.path.join(input_folder, files[0])
    
    with open(first_path, encoding="utf-8") as f:
        grid_data = json.load(f)
    
    return grid_cells_to_dataframe_with_labels(grid_data)


def save_all_grid_files_with_labels(input_folder: str, output_folder: str) -> list:
    """
    ëª¨ë“  grid_cells íŒŒì¼ì„ ë¼ë²¨ ì •ë³´ì™€ í•¨ê»˜ ì—‘ì…€ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
    """
    os.makedirs(output_folder, exist_ok=True)
    saved_paths = []
    
    files = sorted(f for f in os.listdir(input_folder) 
                  if f.lower().endswith("_grid_cells.json"))
    
    for fname in files:
        path = os.path.join(input_folder, fname)
        
        with open(path, encoding="utf-8") as f:
            grid_data = json.load(f)
        
        df = grid_cells_to_dataframe_with_labels(grid_data)
        
        # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
        base = fname.replace('_grid_cells.json', '')
        out_path = os.path.join(output_folder, f"{base}_table_with_labels.xlsx")
        
        # ì—‘ì…€ ì €ì¥
        df.to_excel(out_path, index=True, index_label='Row_Label')
        saved_paths.append(out_path)
        print(f"ì €ì¥ë¨: {out_path}")
    
    return saved_paths

















#------------------------------------------------------------------------------

def grid_cells_to_dataframe(grid_data: dict) -> pd.DataFrame:
    """
    ìƒˆë¡œìš´ ê²©ì ì…€ êµ¬ì¡°ë¥¼ ì§ì ‘ í™œìš©í•˜ì—¬ DataFrameì„ ìƒì„±
    í´ëŸ¬ìŠ¤í„°ë§ ì—†ì´ ì •í™•í•œ row, col ì •ë³´ ì‚¬ìš©
    """
    # ìƒˆë¡œìš´ JSON êµ¬ì¡°ì—ì„œ cells ì¶”ì¶œ
    grid_cells = grid_data['cells']
    row_labels_info = {row['row_index']: row['label'] for row in grid_data['row_labels']}
    
    # row_labelsì—ì„œ ëª¨ë“  í–‰ ë²ˆí˜¸ ê°€ì ¸ì˜¤ê¸° (ëˆ„ë½ ë°©ì§€)
    all_row_indices = sorted([row['row_index'] for row in grid_data['row_labels']])
    
    # ìµœëŒ€ ì»¬ëŸ¼ ë²ˆí˜¸ ì°¾ê¸°
    max_col = 0
    for cell in grid_cells:
        max_col = max(max_col, cell['col'])
    
    # ëª¨ë“  í–‰ì— ëŒ€í•´ ë°ì´í„° êµ¬ì¡°í™” (ë¹ˆ í–‰ë„ í¬í•¨)
    rows = {}
    
    # ë¨¼ì € ëª¨ë“  í–‰ì„ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì´ˆê¸°í™”
    for row_idx in all_row_indices:
        rows[row_idx] = {}
    
    # ì…€ ë°ì´í„°ë¡œ ì±„ìš°ê¸°
    for cell in grid_cells:
        row_idx = cell['row']
        col_idx = cell['col']
        
        # ì…€ì— í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if cell['extracted_texts']:
            # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ ì‚¬ìš© (ì´ë¯¸ ë³‘í•©ë˜ì—ˆìŒ)
            text = cell['extracted_texts'][0]['text'].strip()
            rows[row_idx][col_idx] = text
    
    # DataFrame ìƒì„±
    df = pd.DataFrame.from_dict(rows, orient='index')
    
    # ì»¬ëŸ¼ ì •ë ¬ (0ë¶€í„° max_colê¹Œì§€)
    all_cols = list(range(max_col + 1))
    df = df.reindex(columns=all_cols, fill_value='')
    
    # í–‰ ì¸ë±ìŠ¤ë¥¼ row_labels ìˆœì„œë¡œ ì •ë ¬
    df = df.reindex(all_row_indices, fill_value='')
    
    # í–‰ ë¼ë²¨ ì •ë³´ë¥¼ ì²« ë²ˆì§¸ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
    row_label_column = []
    for row_idx in df.index:
        label = row_labels_info.get(row_idx, 'Unknown')
        row_label_column.append(label)
    
    # í–‰ ë¼ë²¨ ì»¬ëŸ¼ì„ DataFrameì˜ ì²« ë²ˆì§¸ ì»¬ëŸ¼ìœ¼ë¡œ ì‚½ì…
    df.insert(0, 'ROW_LABEL', row_label_column)
    
    # ì»¬ëŸ¼ëª…ì„ ë” ì˜ë¯¸ìˆê²Œ ë³€ê²½
    col_names = ['ROW_LABEL']  # ì²« ë²ˆì§¸ëŠ” í–‰ ë¼ë²¨
    for i in range(1, len(df.columns)):
        original_col_idx = i - 1  # ROW_LABEL ë•Œë¬¸ì— 1 ë¹¼ê¸°
        if original_col_idx == 0:
            col_names.append('TYPE')
        elif original_col_idx == 1:
            col_names.append('NAME')
        elif original_col_idx == 2:
            col_names.append('THK(mm)')
        else:
            col_names.append(f'Column_{original_col_idx}')
    
    df.columns = col_names[:len(df.columns)]
    
    print(f"DataFrame ìƒì„± ì™„ë£Œ: {len(df)}í–‰ Ã— {len(df.columns)}ì»¬ëŸ¼")
    print(f"í–‰ ì¸ë±ìŠ¤ ë²”ìœ„: {df.index.min()} ~ {df.index.max()}")
    
    return df


def preview_first_grid_file(input_folder: str) -> pd.DataFrame:
    """
    ì²« ë²ˆì§¸ grid_cells íŒŒì¼ì„ ë¯¸ë¦¬ë³´ê¸°
    """
    files = sorted(f for f in os.listdir(input_folder) 
                  if f.lower().endswith("_grid_cells.json"))
    
    if not files:
        print("grid_cells.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return pd.DataFrame()
    
    first_path = os.path.join(input_folder, files[0])
    
    with open(first_path, encoding="utf-8") as f:
        grid_data = json.load(f)  # ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ë¡œë“œ
    
    return grid_cells_to_dataframe(grid_data)


def save_all_grid_files(input_folder: str, output_folder: str) -> list:
    """
    ëª¨ë“  grid_cells íŒŒì¼ì„ ì—‘ì…€ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
    """
    os.makedirs(output_folder, exist_ok=True)
    saved_paths = []
    
    files = sorted(f for f in os.listdir(input_folder) 
                  if f.lower().endswith("_grid_cells.json"))
    
    for fname in files:
        path = os.path.join(input_folder, fname)
        
        with open(path, encoding="utf-8") as f:
            grid_data = json.load(f)  # ìƒˆë¡œìš´ êµ¬ì¡°ë¡œ ë¡œë“œ
        
        df = grid_cells_to_dataframe(grid_data)
        
        # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
        base = fname.replace('_grid_cells.json', '')
        out_path = os.path.join(output_folder, f"{base}_table.xlsx")
        
        # ì—‘ì…€ ì €ì¥ (í–‰ ë¼ë²¨ì´ ìˆìœ¼ë¯€ë¡œ indexëŠ” ì œê±°)
        df.to_excel(out_path, index=False)
        saved_paths.append(out_path)
        print(f"ì €ì¥ë¨: {out_path}")
        
        # ë¯¸ë¦¬ë³´ê¸° ì •ë³´ ì¶œë ¥
        print(f"  - ì´ {len(df)}ê°œ í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼")
        print(f"  - í–‰ ë¼ë²¨: {df['ROW_LABEL'].nunique()}ê°œ (Unknown ì œì™¸: {df[df['ROW_LABEL'] != 'Unknown']['ROW_LABEL'].nunique()}ê°œ)")
    
    return saved_paths


def analyze_grid_structure(grid_data: dict) -> dict:
    """
    ê²©ì êµ¬ì¡° ë¶„ì„ ì •ë³´ ì œê³µ
    """
    grid_cells = grid_data['cells']
    row_labels = grid_data['row_labels']
    
    if not grid_cells:
        return {}
    
    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í–‰ê³¼ ì—´ ì •ë³´
    rows = sorted(set(cell['row'] for cell in grid_cells))
    cols = sorted(set(cell['col'] for cell in grid_cells))
    
    # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ì…€ ê°œìˆ˜
    filled_cells = sum(1 for cell in grid_cells if cell['extracted_texts'])
    
    # ê° í–‰ë³„ í…ìŠ¤íŠ¸ ê°œìˆ˜
    row_text_counts = {}
    for cell in grid_cells:
        row = cell['row']
        if row not in row_text_counts:
            row_text_counts[row] = 0
        if cell['extracted_texts']:
            row_text_counts[row] += 1
    
    # í–‰ ë¼ë²¨ í†µê³„
    label_stats = {}
    for row_label in row_labels:
        label = row_label['label']
        if label not in label_stats:
            label_stats[label] = 0
        label_stats[label] += 1
    
    # ì‹¤ì œ í–‰ ê°œìˆ˜ ì •í™•íˆ ê³„ì‚°
    actual_row_count = len(rows)
    missing_rows = []
    if rows:
        expected_rows = list(range(min(rows), max(rows) + 1))
        missing_rows = [r for r in expected_rows if r not in rows]
    
    return {
        'total_cells': len(grid_cells),
        'filled_cells': filled_cells,
        'empty_cells': len(grid_cells) - filled_cells,
        'rows_range': (min(rows) if rows else 0, max(rows) if rows else 0),
        'cols_range': (min(cols) if cols else 0, max(cols) if cols else 0),
        'rows_count': actual_row_count,  # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í–‰ ê°œìˆ˜
        'cols_count': len(cols),
        'missing_rows': missing_rows,  # ëˆ„ë½ëœ í–‰ ë²ˆí˜¸ë“¤
        'actual_rows': rows,  # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í–‰ ë²ˆí˜¸ë“¤
        'row_text_counts': row_text_counts,
        'total_row_labels': len(row_labels),
        'unique_labels': len(label_stats),
        'label_distribution': label_stats
    }


def display_excel_preview(input_folder: str, max_rows: int = 10):
    """
    Excel ë³€í™˜ ê²°ê³¼ë¥¼ ë¯¸ë¦¬ë³´ê¸°ë¡œ í‘œì‹œ
    """
    df = preview_first_grid_file(input_folder)
    
    if df.empty:
        print("ë¯¸ë¦¬ë³´ê¸°í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\n### Excel ë³€í™˜ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ {min(max_rows, len(df))}í–‰)")
    print(f"ì „ì²´ í¬ê¸°: {len(df)}í–‰ Ã— {len(df.columns)}ì»¬ëŸ¼")
    print("\n" + "="*100)
    
    # ì²˜ìŒ ëª‡ í–‰ë§Œ ì¶œë ¥
    preview_df = df.head(max_rows)
    
    # ì»¬ëŸ¼ ë„ˆë¹„ ì¡°ì •í•´ì„œ ì¶œë ¥
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', 15)
    
    print(preview_df.to_string(index=False))
    
    # í†µê³„ ì •ë³´
    print("\n" + "="*100)
    print("### í†µê³„ ì •ë³´")
    print(f"- ë¹„ì–´ìˆì§€ ì•Šì€ í–‰ ë¼ë²¨: {len(df[df['ROW_LABEL'] != 'Unknown'])}")
    print(f"- Unknown í–‰: {len(df[df['ROW_LABEL'] == 'Unknown'])}")
    print(f"- ê³ ìœ  í–‰ ë¼ë²¨: {df['ROW_LABEL'].nunique()}ê°œ")
    print(f"- ì‹¤ì œ í–‰ ë²ˆí˜¸ ë²”ìœ„: {df.index.min()} ~ {df.index.max()}")
    
    # í–‰ ë¼ë²¨ë³„ ê°œìˆ˜
    label_counts = df['ROW_LABEL'].value_counts()
    print(f"- í–‰ ë¼ë²¨ ë¶„í¬:")
    for label, count in label_counts.head(10).items():
        if label != 'Unknown':
            print(f"  â€¢ {label}: {count}í–‰")
    if 'Unknown' in label_counts:
        print(f"  â€¢ Unknown: {label_counts['Unknown']}í–‰")










# def visualize_saved_bounding_boxes(selectbox_key: str = "viz_select") -> None:
#     """
#     ì €ì¥ëœ ë°”ìš´ë”©ë°•ìŠ¤ JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ
#     ì›ë³¸ ì´ë¯¸ì§€ ìœ„ì— ë°•ìŠ¤ë“¤ì„ ì‹œê°í™”í•´ì„œ ë³´ì—¬ì£¼ëŠ” í•¨ìˆ˜
#     """
    
#     # JSON íŒŒì¼ë“¤ ì°¾ê¸°
#     json_files = [f for f in os.listdir(Slab_same_line) if f.endswith("_boxes.json")]
    
#     if not json_files:
#         st.warning(f"ì €ì¥ëœ ë°•ìŠ¤ íŒŒì¼ì´ ì—†ì–´ìš”: {Slab_same_line}")
#         return
    
#     # íŒŒì¼ ì„ íƒ
#     selected_json = st.selectbox("ë°•ìŠ¤ íŒŒì¼ ì„ íƒ", json_files, key=selectbox_key)
    
#     # JSON íŒŒì¼ ì½ê¸°
#     json_path = os.path.join(Slab_same_line, selected_json)
#     try:
#         with open(json_path, "r", encoding="utf-8") as f:
#             boxes = json.load(f)
#     except Exception as e:
#         st.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
#         return
    
#     if not boxes:
#         st.warning("ë°•ìŠ¤ ë°ì´í„°ê°€ ì—†ì–´ìš”!")
#         return
    
#     # í•´ë‹¹í•˜ëŠ” ì›ë³¸ ì´ë¯¸ì§€ ì°¾ê¸°
#     img_name = selected_json.replace("_boxes.json", "")
#     img_extensions = [".png", ".jpg", ".jpeg"]
#     img_path = None
    
#     for ext in img_extensions:
#         test_path = os.path.join(Slab_column_SDD, img_name + ext)
#         if os.path.exists(test_path):
#             img_path = test_path
#             break
    
#     if not img_path:
#         st.error(f"ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ìš”: {img_name}")
#         return
    
#     # ì´ë¯¸ì§€ ì—´ê¸°
#     image = Image.open(img_path).convert("RGB")
#     img_array = np.array(image)
    
#     # matplotlibìœ¼ë¡œ ì‹œê°í™”
#     fig, ax = plt.subplots(1, 1, figsize=(12, 8))
#     ax.imshow(img_array)
    
#     # ê° ë°•ìŠ¤ ê·¸ë¦¬ê¸°
#     colors = ['red', 'blue', 'green', 'orange', 'purple', 'cyan', 'yellow', 'pink']
    
#     for i, box in enumerate(boxes):
#         color = colors[i % len(colors)]
        
#         # Rectangle ê·¸ë¦¬ê¸°
#         rect = Rectangle(
#             (box['left'], box['top']),
#             box['width'],
#             box['height'],
#             linewidth=2,
#             edgecolor=color,
#             facecolor='none',
#             alpha=0.8
#         )
#         ax.add_patch(rect)
        
#         # ë°•ìŠ¤ ë²ˆí˜¸ í‘œì‹œ
#         ax.text(
#             box['left'], 
#             box['top'] - 5,
#             f'#{i+1}',
#             fontsize=12,
#             color=color,
#             fontweight='bold',
#             bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.7)
#         )
    
#     ax.set_xlim(0, image.width)
#     ax.set_ylim(image.height, 0)  # yì¶• ë’¤ì§‘ê¸°
#     ax.set_title(f"ë°”ìš´ë”©ë°•ìŠ¤ ì‹œê°í™”: {img_name} ({len(boxes)}ê°œ)", fontsize=14, fontweight='bold')
#     ax.axis('off')
    
#     # Streamlitì— í‘œì‹œ
#     st.pyplot(fig)
#     plt.close()  # ë©”ëª¨ë¦¬ ì •ë¦¬
    
#     # ë°•ìŠ¤ ì •ë³´ í‘œì‹œ
#     with st.expander(f"ğŸ“Š ë°•ìŠ¤ ì •ë³´ ({len(boxes)}ê°œ)", expanded=False):
#         for i, box in enumerate(boxes):
#             col1, col2 = st.columns(2)
#             with col1:
#                 st.write(f"**ë°•ìŠ¤ #{i+1}**")
#                 st.write(f"ìœ„ì¹˜: ({box['left']:.1f}, {box['top']:.1f})")
#             with col2:
#                 st.write(f"í¬ê¸°: {box['width']:.1f} Ã— {box['height']:.1f}")
#                 st.write(f"ëì : ({box['right']:.1f}, {box['bottom']:.1f})")


####################################################################





# def load_pickle_files(input_dir="./"):
#     """pickle íŒŒì¼ë“¤ì„ ë¡œë“œ"""
#     pickle_files = []
    
#     # í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  .pkl íŒŒì¼ ì°¾ê¸°
#     for file_path in Path(input_dir).glob("*.pkl"):
#         pickle_files.append(str(file_path))
    
#     # linesê°€ í¬í•¨ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ìš°ì„ ì ìœ¼ë¡œ ì„ íƒ
#     lines_files = [f for f in pickle_files if "lines" in f]
#     if lines_files:
#         return lines_files
    
#     return pickle_files

# def load_pickle_data(file_path):
#     """ê°œë³„ pickle íŒŒì¼ ë¡œë“œ"""
#     with open(file_path, 'rb') as f:
#         data = pickle.load(f)
#     return data

# def create_grid_from_lines(horizontal_lines, vertical_lines):
#     """ìˆ˜í‰ì„ ê³¼ ìˆ˜ì§ì„ ìœ¼ë¡œ ê²©ì ìƒì„±"""
#     # ìˆ˜í‰ì„ ê³¼ ìˆ˜ì§ì„  ì •ë ¬
#     h_lines = sorted(horizontal_lines, key=lambda x: x[0][1])  # yì¢Œí‘œë¡œ ì •ë ¬
#     v_lines = sorted(vertical_lines, key=lambda x: x[0][0])   # xì¢Œí‘œë¡œ ì •ë ¬
    
#     # ê²©ì ì…€ ì •ì˜ (ê° ì…€ì€ (x1,y1,x2,y2) í˜•íƒœ)
#     grid_cells = []
    
#     for i in range(len(h_lines)-1):
#         row_cells = []
#         for j in range(len(v_lines)-1):
#             # í˜„ì¬ ì…€ì˜ ê²½ê³„ ê³„ì‚°
#             y1 = h_lines[i][0][1]
#             y2 = h_lines[i+1][0][1]
#             x1 = v_lines[j][0][0]  
#             x2 = v_lines[j+1][0][0]
            
#             cell_bounds = (x1, y1, x2, y2)
#             row_cells.append(cell_bounds)
#         grid_cells.append(row_cells)
    
#     return grid_cells

# def assign_texts_to_grid(grid_cells, regions, vertical_lines):
#     """í…ìŠ¤íŠ¸ë“¤ì„ ê²©ì ì…€ì— í• ë‹¹ (ìˆ˜ì§ì„  ë¶„í•  í¬í•¨)"""
    
#     def split_text_by_vertical_lines(text_info, vertical_lines):
#         """í…ìŠ¤íŠ¸ bboxë¥¼ ìˆ˜ì§ì„ ìœ¼ë¡œ ë¬¼ë¦¬ì ìœ¼ë¡œ ë¶„í•  (ë‚´ë¶€ í•¨ìˆ˜)"""
#         text_bbox = text_info['bbox']
#         text = text_info['text'].strip()
        
#         # í˜„ì¬ í…ìŠ¤íŠ¸ê°€ ê±¸ì³ìˆëŠ” ìˆ˜ì§ì„ ë“¤ ì°¾ê¸°
#         text_left = text_bbox[0]
#         text_right = text_bbox[2]
        
#         # í…ìŠ¤íŠ¸ bbox ë‚´ì— ìˆëŠ” ìˆ˜ì§ì„ ë“¤ ì°¾ê¸°
#         intersecting_v_lines = []
#         for v_line in vertical_lines:
#             v_x = v_line[0][0]  # ìˆ˜ì§ì„ ì˜ x ì¢Œí‘œ
#             if text_left < v_x < text_right:
#                 intersecting_v_lines.append(v_x)
        
#         if not intersecting_v_lines:
#             return [text_info]  # ë‚˜ëˆŒ ìˆ˜ì§ì„ ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        
#         # ìˆ˜ì§ì„ ë“¤ì„ ì •ë ¬
#         intersecting_v_lines.sort()
        
#         # bbox ë¶„í•  í¬ì¸íŠ¸ë“¤ = [ì‹œì‘ì ] + [ìˆ˜ì§ì„ ë“¤] + [ëì ]
#         split_points = [text_left] + intersecting_v_lines + [text_right]
        
#         # ë¶„í• ëœ bboxë“¤ ìƒì„±
#         split_text_parts = []
        
#         for i in range(len(split_points) - 1):
#             segment_left = split_points[i]
#             segment_right = split_points[i + 1]
            
#             # ìƒˆë¡œìš´ bbox ìƒì„± (ì›ë³¸ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ, bboxë§Œ ë¶„í• )
#             new_bbox = [
#                 segment_left,
#                 text_bbox[1],  # y1 ê·¸ëŒ€ë¡œ
#                 segment_right,
#                 text_bbox[3]   # y2 ê·¸ëŒ€ë¡œ
#             ]
            
#             # ë¶„í• ëœ í…ìŠ¤íŠ¸ ê°ì²´ ìƒì„±
#             new_text_info = text_info.copy()
#             new_text_info['bbox'] = new_bbox
#             new_text_info['text'] = text  # ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ
            
#             split_text_parts.append(new_text_info)
        
#         return split_text_parts
    
#     # ëª¨ë“  regionì˜ í…ìŠ¤íŠ¸ë“¤ì„ í•˜ë‚˜ë¡œ ëª¨ìŒ
#     all_texts = []
#     for region in regions:
#         for text_info in region['texts']:
#             text_info['cell_label'] = region['cell_label']
#             all_texts.append(text_info)
    
#     # ê²©ìì™€ ê°™ì€ í¬ê¸°ì˜ ë¹ˆ í…Œì´ë¸” ìƒì„±
#     table = []
#     for i in range(len(grid_cells)):
#         row = []
#         for j in range(len(grid_cells[i])):
#             row.append([])  # ê° ì…€ì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì‹œì‘
#         table.append(row)
    
#     # ê° í…ìŠ¤íŠ¸ë¥¼ í•´ë‹¹í•˜ëŠ” ì…€ì— í• ë‹¹
#     for text_info in all_texts:
#         text_bbox = text_info['bbox']
#         text_center_x = (text_bbox[0] + text_bbox[2]) / 2
#         text_center_y = (text_bbox[1] + text_bbox[3]) / 2
        
#         # í…ìŠ¤íŠ¸ê°€ ì†í•˜ëŠ” ì…€ ì°¾ê¸°
#         assigned = False
#         for i, row in enumerate(grid_cells):
#             for j, cell_bounds in enumerate(row):
#                 x1, y1, x2, y2 = cell_bounds
#                 if x1 <= text_center_x <= x2 and y1 <= text_center_y <= y2:
#                     # í…ìŠ¤íŠ¸ê°€ ì—¬ëŸ¬ ì—´ì— ê±¸ì³ìˆëŠ”ì§€ í™•ì¸
#                     text_width = text_bbox[2] - text_bbox[0]
#                     cell_width = x2 - x1
                    
#                     # í…ìŠ¤íŠ¸ê°€ ì…€ ë„ˆë¹„ì˜ 1.2ë°° ì´ìƒì´ë©´ ë¶„í•  ì‹œë„
#                     if text_width > cell_width * 1.2:
#                         split_texts = split_text_by_vertical_lines(text_info, vertical_lines)
                        
#                         # ë¶„í• ëœ í…ìŠ¤íŠ¸ë“¤ì„ ê°ê° í•´ë‹¹ ì…€ì— ë°°ì •
#                         for split_text in split_texts:
#                             split_center_x = (split_text['bbox'][0] + split_text['bbox'][2]) / 2
#                             split_center_y = (split_text['bbox'][1] + split_text['bbox'][3]) / 2
                            
#                             # ë¶„í• ëœ í…ìŠ¤íŠ¸ì˜ ì…€ ì°¾ê¸°
#                             for ii, rrow in enumerate(grid_cells):
#                                 for jj, ccell_bounds in enumerate(rrow):
#                                     xx1, yy1, xx2, yy2 = ccell_bounds
#                                     if xx1 <= split_center_x <= xx2 and yy1 <= split_center_y <= yy2:
#                                         table[ii][jj].append(split_text)
#                                         break
#                     else:
#                         table[i][j].append(text_info)
                    
#                     assigned = True
#                     break
#             if assigned:
#                 break
    
#     return table

# def merge_texts_in_cell(cell_texts):
#     """ê°™ì€ ì…€ ë‚´ì˜ í…ìŠ¤íŠ¸ë“¤ì„ í•©ì¹˜ê¸°"""
#     if not cell_texts:
#         return ""
    
#     if len(cell_texts) == 1:
#         # ë‹¨ì¼ í…ìŠ¤íŠ¸ì¸ ê²½ìš° - ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì…€ ì˜ì—­ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
#         text_info = cell_texts[0]
#         return extract_text_for_cell(text_info)
    
#     # ì—¬ëŸ¬ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° - xì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•´ì„œ í•©ì¹˜ê¸°
#     sorted_texts = sorted(cell_texts, key=lambda x: x['bbox'][0])
    
#     merged_text = ""
#     for text_info in sorted_texts:
#         cell_text = extract_text_for_cell(text_info)
#         if cell_text.strip():
#             if merged_text and not merged_text.endswith(' '):
#                 merged_text += " " + cell_text.strip()
#             else:
#                 merged_text += cell_text.strip()
    
#     return merged_text.strip()

# def extract_text_for_cell(text_info):
#     """í…ìŠ¤íŠ¸ì—ì„œ í•´ë‹¹ ì…€ ì˜ì—­ì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ì¶”ì¶œ"""
#     original_text = text_info['text'].strip()
    
#     # ê³µë°±ìœ¼ë¡œ í† í°í™”
#     tokens = original_text.split()
#     if not tokens:
#         return original_text
    
#     # HD + ìˆ«ì + @ + ìˆ«ì íŒ¨í„´ ì¡°í•© ì°¾ê¸°
#     result_tokens = []
#     i = 0
    
#     while i < len(tokens):
#         current_token = tokens[i]
        
#         # HDë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°
#         if current_token.startswith('HD'):
#             pattern_tokens = [current_token]
#             j = i + 1
            
#             # HD ë‹¤ìŒì— ìˆ«ìê°€ ì˜¬ ë•Œê¹Œì§€ ìˆ˜ì§‘
#             while j < len(tokens) and (tokens[j].isdigit() or tokens[j] in ['10', '13', '16']):
#                 pattern_tokens.append(tokens[j])
#                 j += 1
            
#             # @ ê¸°í˜¸ ì°¾ê¸°
#             if j < len(tokens) and tokens[j] == '@':
#                 pattern_tokens.append(tokens[j])
#                 j += 1
                
#                 # @ ë‹¤ìŒ ìˆ«ì ìˆ˜ì§‘
#                 while j < len(tokens) and (tokens[j].isdigit() or tokens[j] in ['200', '300', '400']):
#                     pattern_tokens.append(tokens[j])
#                     j += 1
            
#             # ì™„ì„±ëœ íŒ¨í„´ì´ ìˆìœ¼ë©´ ì¡°í•©í•´ì„œ ë°˜í™˜
#             if len(pattern_tokens) >= 2:  # HD + ìˆ«ì ìµœì†Œ
#                 result_tokens.extend(pattern_tokens)
#                 return ' '.join(result_tokens)
            
#             i = j
#         else:
#             # HDê°€ ì•„ë‹Œ ì¼ë°˜ í† í°
#             result_tokens.append(current_token)
#             i += 1
            
#             # í•˜ë‚˜ì˜ ì˜ë¯¸ìˆëŠ” í† í°ì„ ì°¾ìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
#             if len(result_tokens) >= 1:
#                 break
    
#     return ' '.join(result_tokens) if result_tokens else original_text

# def process_table_with_labels(table, regions):
#     """í…Œì´ë¸” ì²˜ë¦¬í•  ë•Œ cell_labelë„ í•¨ê»˜ ì²˜ë¦¬"""
#     # cell_labelë³„ë¡œ í…ìŠ¤íŠ¸ë“¤ì„ ê·¸ë£¹í•‘
#     label_to_texts = {}
    
#     # ë¨¼ì € ëª¨ë“  ì…€ì˜ í…ìŠ¤íŠ¸ì—ì„œ cell_label ì •ë³´ ìˆ˜ì§‘
#     for row_idx, row in enumerate(table):
#         for col_idx, cell_texts in enumerate(row):
#             for text_info in cell_texts:
#                 if 'cell_label' in text_info:
#                     label = text_info['cell_label']
#                     if label not in label_to_texts:
#                         label_to_texts[label] = []
                    
#                     # ì…€ ìœ„ì¹˜ì™€ í…ìŠ¤íŠ¸ ì •ë³´ ì €ì¥
#                     label_to_texts[label].append({
#                         'row': row_idx,
#                         'col': col_idx,
#                         'text_info': text_info,
#                         'merged_text': merge_texts_in_cell([text_info])
#                     })
    
#     # ê° labelë³„ë¡œ í–‰ êµ¬ì„±
#     result_table = []
    
#     for label, text_infos in label_to_texts.items():
#         # í•´ë‹¹ labelì˜ í…ìŠ¤íŠ¸ë“¤ì„ ì—´ ìˆœì„œëŒ€ë¡œ ì •ë ¬
#         text_infos.sort(key=lambda x: (x['row'], x['col']))
        
#         # í–‰ë³„ë¡œ ê·¸ë£¹í•‘
#         rows_by_row_idx = {}
#         for text_info in text_infos:
#             row_idx = text_info['row']
#             if row_idx not in rows_by_row_idx:
#                 rows_by_row_idx[row_idx] = []
#             rows_by_row_idx[row_idx].append(text_info)
        
#         # ê° í–‰ì„ ì²˜ë¦¬
#         for row_idx in sorted(rows_by_row_idx.keys()):
#             row_texts = rows_by_row_idx[row_idx]
            
#             # í•´ë‹¹ í–‰ì˜ ëª¨ë“  ì—´ ì²˜ë¦¬
#             max_col = max([t['col'] for t in row_texts]) if row_texts else 0
#             processed_row = [''] * (max_col + 2)  # +2ëŠ” label ì—´ ì¶”ê°€ìš©
            
#             # ì²« ë²ˆì§¸ ì—´ì— label ë„£ê¸°
#             processed_row[0] = label
            
#             # ë‚˜ë¨¸ì§€ ì—´ì— í…ìŠ¤íŠ¸ ë„£ê¸°
#             for text_info in row_texts:
#                 col_idx = text_info['col'] + 1  # label ì—´ ë•Œë¬¸ì— +1
#                 if col_idx < len(processed_row):
#                     processed_row[col_idx] = text_info['merged_text']
            
#             result_table.append(processed_row)
    
#     return result_table

# def save_to_excel(processed_tables, output_dir="./output"):
#     """ì²˜ë¦¬ëœ í…Œì´ë¸”ë“¤ì„ ì—‘ì…€ë¡œ ì €ì¥"""
#     os.makedirs(output_dir, exist_ok=True)
    
#     with pd.ExcelWriter(f"{output_dir}/converted_tables.xlsx", engine='openpyxl') as writer:
#         for page_name, table_data in processed_tables.items():
#             # ë¹ˆ í–‰ë“¤ ì œê±°
#             filtered_table = [row for row in table_data if any(cell.strip() for cell in row)]
            
#             if filtered_table:
#                 df = pd.DataFrame(filtered_table)
#                 # ì‹œíŠ¸ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
#                 safe_sheet_name = page_name.replace('/', '_').replace('\\', '_')[:31]
#                 df.to_excel(writer, sheet_name=safe_sheet_name, index=False, header=False)
    
#     return f"{output_dir}/converted_tables.xlsx"

# def convert_uploaded_files(file_paths):
#     """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ì§ì ‘ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
#     output_dir = "./output"
    
#     if not file_paths:
#         st.error("ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
#         return None
    
#     st.write(f"ì²˜ë¦¬í•  íŒŒì¼ë“¤: {file_paths}")
    
#     processed_tables = {}
    
#     # ê° pickle íŒŒì¼ ì²˜ë¦¬
#     for pickle_file in file_paths:
#         st.write(f"ì²˜ë¦¬ ì¤‘: {pickle_file}")
        
#         try:
#             # pickle ë°ì´í„° ë¡œë“œ
#             data = load_pickle_data(pickle_file)
            
#             # ë°ì´í„° êµ¬ì¡° ê²€ì¦
#             required_keys = ['horizontal_lines', 'vertical_lines', 'regions']
#             missing_keys = [key for key in required_keys if key not in data]
            
#             if missing_keys:
#                 st.warning(f"{pickle_file}: í•„ìš”í•œ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤ ({missing_keys}). ê±´ë„ˆëœë‹ˆë‹¤.")
#                 continue
            
#             # ê²©ì ìƒì„±
#             grid_cells = create_grid_from_lines(data['horizontal_lines'], data['vertical_lines'])
#             st.write(f"ê²©ì í¬ê¸°: {len(grid_cells)} x {len(grid_cells[0]) if grid_cells else 0}")
            
#             # í…ìŠ¤íŠ¸ë¥¼ ê²©ìì— í• ë‹¹
#             table = assign_texts_to_grid(grid_cells, data['regions'], data['vertical_lines'])
            
#             # í…ìŠ¤íŠ¸ ë³‘í•© ì²˜ë¦¬ (cell_label í¬í•¨)
#             processed_table = process_table_with_labels(table, data['regions'])
            
#             # í˜ì´ì§€ëª… ìƒì„±
#             page_name = Path(pickle_file).stem
#             processed_tables[page_name] = processed_table
            
#             st.success(f"{page_name} ì²˜ë¦¬ ì™„ë£Œ")
            
#         except Exception as e:
#             st.error(f"{pickle_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
#             import traceback
#             st.text(traceback.format_exc())
#             continue
    
#     if processed_tables:
#         # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
#         excel_path = save_to_excel(processed_tables, output_dir)
#         st.success(f"ë³€í™˜ ì™„ë£Œ! ì €ì¥ ê²½ë¡œ: {excel_path}")
#         return excel_path
    
#     return None

# def convert_pickle_to_excel():
#     """ë©”ì¸ ë³€í™˜ í•¨ìˆ˜"""
#     return convert_uploaded_files(load_pickle_files("./"))












# ==================== í•¨ìˆ˜ë¶€ ====================



# ==================== í•¨ìˆ˜ë¶€ ====================


# ==================== í•¨ìˆ˜ë¶€ ====================


def load_pickle_data(pickle_file):
    """ì—…ë¡œë“œëœ í”¼í´ íŒŒì¼ ë¡œë“œ"""
    try:
        data = pickle.load(pickle_file)
        return data
    except Exception as e:
        st.error(f"í”¼í´ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None

def extract_line_coordinates(horizontal_lines, vertical_lines):
    """ê²©ìì„ ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ"""
    
    def extract_coords(lines, is_horizontal=True):
        """ì„  ë°ì´í„°ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ"""
        coords = []
        for line in lines:
            if isinstance(line, (list, tuple)) and len(line) >= 2:
                if isinstance(line[0], (list, tuple)):
                    # [[x1,y1], [x2,y2]] í˜•íƒœ
                    coord = line[0][1] if is_horizontal else line[0][0]
                else:
                    # [x1, y1, x2, y2] í˜•íƒœ
                    coord = line[1] if is_horizontal else line[0]
                coords.append(coord)
            elif isinstance(line, (int, float)):
                coords.append(line)
        return sorted(list(set(coords)))
    
    h_coords = extract_coords(horizontal_lines, True)   # yì¢Œí‘œë“¤
    v_coords = extract_coords(vertical_lines, False)    # xì¢Œí‘œë“¤
    
    return h_coords, v_coords

def calculate_bbox_area(bbox):
    """bboxì˜ ë©´ì  ê³„ì‚°"""
    x1, y1, x2, y2 = bbox
    return max(0, x2 - x1) * max(0, y2 - y1)

def calculate_bbox_intersection(bbox1, bbox2):
    """ë‘ bboxì˜ êµì§‘í•© ì˜ì—­ ê³„ì‚°"""
    x1_1, y1_1, x2_1, y2_1 = bbox1
    x1_2, y1_2, x2_2, y2_2 = bbox2
    
    # êµì§‘í•© ì¢Œí‘œ ê³„ì‚°
    x1_int = max(x1_1, x1_2)
    y1_int = max(y1_1, y1_2)
    x2_int = min(x2_1, x2_2)
    y2_int = min(y2_1, y2_2)
    
    # êµì§‘í•©ì´ ì—†ìœ¼ë©´ 0
    if x1_int >= x2_int or y1_int >= y2_int:
        return 0
    
    return (x2_int - x1_int) * (y2_int - y1_int)

def calculate_overlap_ratio(bbox1, bbox2):
    """ë‘ bboxì˜ ê²¹ì¹¨ ë¹„ìœ¨ ê³„ì‚° (ì‘ì€ ë°•ìŠ¤ ê¸°ì¤€)"""
    intersection_area = calculate_bbox_intersection(bbox1, bbox2)
    if intersection_area == 0:
        return 0
    
    area1 = calculate_bbox_area(bbox1)
    area2 = calculate_bbox_area(bbox2)
    
    # ì‘ì€ ë°•ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ê²¹ì¹¨ ë¹„ìœ¨ ê³„ì‚°
    smaller_area = min(area1, area2)
    if smaller_area == 0:
        return 0
    
    return intersection_area / smaller_area

def remove_duplicate_texts(texts, overlap_threshold=0.7, show_details=False):
    """ì¤‘ë³µëœ bboxë¥¼ ê°€ì§„ í…ìŠ¤íŠ¸ë“¤ ì œê±° (í° ë°•ìŠ¤ë§Œ ìœ ì§€)"""
    if len(texts) <= 1:
        return texts, []
    
    removal_log = []
    texts_to_keep = []
    texts_to_remove = set()
    
    # ëª¨ë“  í…ìŠ¤íŠ¸ ìŒì— ëŒ€í•´ ê²¹ì¹¨ ê²€ì‚¬
    for i in range(len(texts)):
        if i in texts_to_remove:
            continue
            
        current_text = texts[i]
        current_bbox = current_text['bbox']
        current_area = calculate_bbox_area(current_bbox)
        
        for j in range(i + 1, len(texts)):
            if j in texts_to_remove:
                continue
                
            other_text = texts[j]
            other_bbox = other_text['bbox']
            other_area = calculate_bbox_area(other_bbox)
            
            # ê²¹ì¹¨ ë¹„ìœ¨ ê³„ì‚°
            overlap_ratio = calculate_overlap_ratio(current_bbox, other_bbox)
            
            if overlap_ratio > overlap_threshold:
                # ê²¹ì¹¨ì´ ì„ê³„ê°’ ì´ìƒì´ë©´ ì‘ì€ ë°•ìŠ¤ ì œê±°
                if current_area >= other_area:
                    # currentê°€ ë” í¬ê±°ë‚˜ ê°™ìœ¼ë©´ other ì œê±°
                    texts_to_remove.add(j)
                    if show_details:
                        removal_log.append(f"    ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: '{other_text['text']}' (ê²¹ì¹¨ë¥ : {overlap_ratio:.2f})")
                        removal_log.append(f"      ìœ ì§€: '{current_text['text']}' (ë©´ì : {current_area:.0f})")
                else:
                    # otherê°€ ë” í¬ë©´ current ì œê±°
                    texts_to_remove.add(i)
                    if show_details:
                        removal_log.append(f"    ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: '{current_text['text']}' (ê²¹ì¹¨ë¥ : {overlap_ratio:.2f})")
                        removal_log.append(f"      ìœ ì§€: '{other_text['text']}' (ë©´ì : {other_area:.0f})")
                    break  # currentê°€ ì œê±°ë˜ì—ˆìœ¼ë¯€ë¡œ ë” ì´ìƒ ë¹„êµí•  í•„ìš” ì—†ìŒ
    
    # ì œê±°í•  í…ìŠ¤íŠ¸ë“¤ì„ ì œì™¸í•˜ê³  ìœ ì§€í•  í…ìŠ¤íŠ¸ë“¤ë§Œ ìˆ˜ì§‘
    for i, text in enumerate(texts):
        if i not in texts_to_remove:
            texts_to_keep.append(text)
    
    if show_details and removal_log:
        removal_log.insert(0, f"  ğŸ” ì¤‘ë³µ ê²€ì‚¬: {len(texts)}ê°œ â†’ {len(texts_to_keep)}ê°œ (ì„ê³„ê°’: {overlap_threshold})")
    
    return texts_to_keep, removal_log
    """í…ìŠ¤íŠ¸ë“¤ì„ ì…€ ë‹¨ìœ„ë¡œ ê·¸ë£¹í•‘"""
    cell_groups = {}  # {(row, col): [text_info, ...]}
    
    for text_info in texts:
        bbox = text_info['bbox']
        x_center = (bbox[0] + bbox[2]) / 2
        y_center = (bbox[1] + bbox[3]) / 2
        
        # ì…€ ìœ„ì¹˜ ì°¾ê¸°
        col_idx = 0
        for i in range(len(v_coords) - 1):
            if v_coords[i] <= x_center <= v_coords[i + 1]:
                col_idx = i
                break
        
        row_idx = 0
        for i in range(len(h_coords) - 1):
            if h_coords[i] <= y_center <= h_coords[i + 1]:
                row_idx = i
                break
        
        cell_key = (row_idx, col_idx)
        if cell_key not in cell_groups:
            cell_groups[cell_key] = []
        cell_groups[cell_key].append(text_info)
    
    return cell_groups

def merge_cell_texts(text_list):
    """ê°™ì€ ì…€ ë‚´ì˜ í…ìŠ¤íŠ¸ë“¤ì„ xì¢Œí‘œ ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ í•©ì¹˜ê³  ìƒˆë¡œìš´ bbox ìƒì„±"""
    if not text_list:
        return None
    
    if len(text_list) == 1:
        return text_list[0]
    
    # xì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    sorted_texts = sorted(text_list, key=lambda x: x['bbox'][0])
    
    # í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    merged_text = ' '.join([t['text'].strip() for t in sorted_texts])
    
    # ìƒˆë¡œìš´ bbox ê³„ì‚° (ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ëŠ” ìµœì†Œ ë°•ìŠ¤)
    min_x = min(t['bbox'][0] for t in sorted_texts)
    min_y = min(t['bbox'][1] for t in sorted_texts)
    max_x = max(t['bbox'][2] for t in sorted_texts)
    max_y = max(t['bbox'][3] for t in sorted_texts)
    
    merged_bbox = [min_x, min_y, max_x, max_y]
    
    # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
    avg_confidence = sum(t.get('confidence', 0) for t in sorted_texts) / len(sorted_texts)
    
    return {
        'bbox': merged_bbox,
        'text': merged_text,
        'confidence': avg_confidence,
        'original_count': len(sorted_texts)
    }
    """bboxê°€ ê±¸ì³ì§„ ëª¨ë“  ì—´ ì¸ë±ìŠ¤ ì°¾ê¸°"""
    x1, y1, x2, y2 = bbox
    overlapping_cols = []
    
    for i in range(len(v_coords) - 1):
        col_left = v_coords[i]
        col_right = v_coords[i + 1]
        
        # bboxì™€ ì—´ì´ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
        if not (x2 < col_left or x1 > col_right):  # ê²¹ì¹¨ ì¡°ê±´
            overlapping_cols.append(i)
    
    return overlapping_cols

def find_overlapping_columns(bbox, v_coords):
    """bboxê°€ ê±¸ì³ì§„ ëª¨ë“  ì—´ ì¸ë±ìŠ¤ ì°¾ê¸°"""
    x1, y1, x2, y2 = bbox
    overlapping_cols = []
    
    for i in range(len(v_coords) - 1):
        col_left = v_coords[i]
        col_right = v_coords[i + 1]
        
        # bboxì™€ ì—´ì´ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
        if not (x2 < col_left or x1 > col_right):  # ê²¹ì¹¨ ì¡°ê±´
            overlapping_cols.append(i)
    
    return overlapping_cols

def distribute_rebars_to_cells(rebar_list, overlapping_cols):
    """íŒŒì‹±ëœ ì² ê·¼ë“¤ì„ ê±¸ì³ì§„ ì…€ë“¤ì— ê°œë³„ ë¶„ë°° - ì ˆëŒ€ í•©ì¹˜ì§€ ì•ŠìŒ"""
    distribution = {}
    
    if not rebar_list or not overlapping_cols:
        return distribution
    
    # ê° ì² ê·¼ì„ ê°œë³„ ì…€ì— ë°°ì¹˜
    for i, rebar in enumerate(rebar_list):
        if i < len(overlapping_cols):
            # ì›ë˜ ê±¸ì³ì§„ ì—´ì— ë°°ì¹˜
            col_idx = overlapping_cols[i]
        else:
            # ê±¸ì³ì§„ ì—´ì„ ë²—ì–´ë‚˜ë©´ ë§ˆì§€ë§‰ ì—´ ì´í›„ë¡œ ê³„ì† í™•ì¥
            col_idx = overlapping_cols[-1] + (i - len(overlapping_cols) + 1)
        
        # í•´ë‹¹ ì—´ì— ì´ë¯¸ ê°’ì´ ìˆìœ¼ë©´ ë‹¤ìŒ ì—´ë¡œ
        while col_idx in distribution:
            col_idx += 1
        
        distribution[col_idx] = rebar
    
    return distribution
    return distribution

def merge_text_in_cell(texts):
    """ê°™ì€ ì…€ ë‚´ì˜ í…ìŠ¤íŠ¸ë“¤ì„ xì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ í•©ì¹˜ê¸°"""
    if not texts:
        return ""
    
    # xì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    sorted_texts = sorted(texts, key=lambda x: x['bbox'][0])
    
    # í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    merged_text = " ".join([t['text'].strip() for t in sorted_texts])
    return merged_text.strip()

def parse_rebar_info(text):
    """ì² ê·¼ ì •ë³´ íŒŒì‹±: (ì² ê·¼ì¢…ë¥˜)(ì§ê²½)@(ê°„ê²©) íŒ¨í„´ - ë””ë²„ê¹… ê°•í™”"""
    
    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (íŒŒì´í”„ ë¬¸ì ê³µë°±ìœ¼ë¡œ ë³€ê²½)
    cleaned_text = text.replace('|', ' ').replace('"', '').strip()
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µì¼
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
    
    if '@ ' in cleaned_text or '@' in cleaned_text:
        print(f"ğŸ” íŒŒì‹± ë””ë²„ê¹…:")
        print(f"  ì›ë³¸: '{text}'")
        print(f"  ì •ë¦¬: '{cleaned_text}'")
    
    # 2. ëª¨ë“  ì² ê·¼ íŒ¨í„´ì„ í•œë²ˆì— ì°¾ê¸°
    all_rebar_pattern = r'(UHD|SUHD|SHD|HD|D|UD)\s*(\d+)(?:\+(\d+))?\s*@\s*(\d+)'
    
    # ëª¨ë“  ë§¤ì¹˜ ì°¾ê¸°
    matches = re.findall(all_rebar_pattern, cleaned_text, re.IGNORECASE)
    
    if '@ ' in cleaned_text or '@' in cleaned_text:
        print(f"  ë§¤ì¹˜ ê²°ê³¼: {matches}")
    
    rebar_list = []
    
    for match in matches:
        rebar_type, diameter1, diameter2, spacing = match
        
        if diameter2:  # ë³µí•© ì² ê·¼ (ì˜ˆ: HD 10+13 @ 200)
            rebar_info = f"{rebar_type}{diameter1}+{diameter2}@{spacing}"
        else:  # ê¸°ë³¸ ì² ê·¼ (ì˜ˆ: HD 10 @ 400)
            rebar_info = f"{rebar_type}{diameter1}@{spacing}"
        
        rebar_list.append(rebar_info)
    
    if '@ ' in cleaned_text or '@' in cleaned_text:
        print(f"  ìµœì¢… ê²°ê³¼: {rebar_list}")
    
    # 3. íŒŒì‹± ê²€ì¦ ë° ë””ë²„ê¹…
    if len(rebar_list) == 0 and cleaned_text.strip():
        # ì² ê·¼ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆëŠ”ë° íŒŒì‹±ì´ ì•ˆëœ ê²½ìš°
        iron_keywords = ['SHD', 'UHD', 'SUHD', 'HD', 'D', 'UD']
        has_iron_keyword = any(keyword in cleaned_text.upper() for keyword in iron_keywords)
        has_at_symbol = '@' in cleaned_text
        
        if has_iron_keyword and has_at_symbol:
            print(f"âŒ íŒŒì‹± ì‹¤íŒ¨ - ì¬ì‹œë„: '{cleaned_text}'")
            
            # ë” ê´€ëŒ€í•œ íŒ¨í„´ìœ¼ë¡œ ì¬ì‹œë„
            loose_pattern = r'([A-Z]+)\s*(\d+)(?:\+(\d+))?\s*@\s*(\d+)'
            loose_matches = re.findall(loose_pattern, cleaned_text, re.IGNORECASE)
            
            print(f"  ê´€ëŒ€í•œ ë§¤ì¹˜: {loose_matches}")
            
            for match in loose_matches:
                rebar_type, diameter1, diameter2, spacing = match
                # ìœ íš¨í•œ ì² ê·¼ íƒ€ì…ì¸ì§€ í™•ì¸
                if rebar_type.upper() in iron_keywords:
                    if diameter2:
                        rebar_info = f"{rebar_type.upper()}{diameter1}+{diameter2}@{spacing}"
                    else:
                        rebar_info = f"{rebar_type.upper()}{diameter1}@{spacing}"
                    rebar_list.append(rebar_info)
            
            print(f"  ì¬ì‹œë„ ê²°ê³¼: {rebar_list}")
            
            # ì—¬ì „íˆ íŒŒì‹± ì‹¤íŒ¨ë©´ ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
            if len(rebar_list) == 0:
                rebar_list.append(f"[íŒŒì‹±ì‹¤íŒ¨: {cleaned_text}]")
    
    return rebar_list

def find_overlapping_columns(bbox, v_coords):
    """bboxê°€ ê±¸ì³ì§„ ëª¨ë“  ì—´ ì¸ë±ìŠ¤ ì°¾ê¸°"""
    x1, y1, x2, y2 = bbox
    overlapping_cols = []
    
    for i in range(len(v_coords) - 1):
        col_left = v_coords[i]
        col_right = v_coords[i + 1]
        
        # bboxì™€ ì—´ì´ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
        if not (x2 < col_left or x1 > col_right):  # ê²¹ì¹¨ ì¡°ê±´
            overlapping_cols.append(i)
    
    return overlapping_cols

def group_texts_by_cells(texts, v_coords, h_coords):
    """í…ìŠ¤íŠ¸ë“¤ì„ ì…€ ë‹¨ìœ„ë¡œ ê·¸ë£¹í•‘"""
    cell_groups = {}  # {(row, col): [text_info, ...]}
    
    for text_info in texts:
        bbox = text_info['bbox']
        x_center = (bbox[0] + bbox[2]) / 2
        y_center = (bbox[1] + bbox[3]) / 2
        
        # ì…€ ìœ„ì¹˜ ì°¾ê¸°
        col_idx = 0
        for i in range(len(v_coords) - 1):
            if v_coords[i] <= x_center <= v_coords[i + 1]:
                col_idx = i
                break
        
        row_idx = 0
        for i in range(len(h_coords) - 1):
            if h_coords[i] <= y_center <= h_coords[i + 1]:
                row_idx = i
                break
        
        cell_key = (row_idx, col_idx)
        if cell_key not in cell_groups:
            cell_groups[cell_key] = []
        cell_groups[cell_key].append(text_info)
    
    return cell_groups

def clean_text_content(text):
    """í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ê¸°í˜¸ ì œê±° - ( ) + @ ëŠ” ìœ ì§€"""
    if not text or not isinstance(text, str):
        return text
    
    # ìœ ì§€í•  ë¬¸ì: í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ( ) + @
    # ì •ê·œì‹: í•œê¸€(\uAC00-\uD7AF), ì˜ë¬¸(a-zA-Z), ìˆ«ì(0-9), ê³µë°±(\s), ()+ @
    cleaned = re.sub(r'[^\uAC00-\uD7AFa-zA-Z0-9\s()+ @]', '', text)
    
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µì¼
    cleaned = re.sub(r'\s+', ' ', cleaned)
    
    return cleaned.strip()

def process_regions_by_rows(regions, v_coords, h_coords, show_details=False):
    """regionì„ í–‰ ê¸°ì¤€ìœ¼ë¡œ ì²˜ë¦¬ - ê° regionì„ ê°œë³„ í–‰ìœ¼ë¡œ ì²˜ë¦¬"""
    
    processing_log = []
    table_data = {}  # {unique_key: {col_index: content, y_position: float, cell_label: str}}
    
    skipped_regions = []
    processed_regions = []
    
    # ê° regionì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬ (ë³‘í•©í•˜ì§€ ì•ŠìŒ)
    for region_idx, region in enumerate(regions):
        cell_label = region.get('cell_label', '').strip()
        texts = region.get('texts', [])
        region_bounds = region.get('bounds', None)
        
        # ìŠ¤í‚µ ì¡°ê±´ ì²´í¬ ë° ë¡œê¹…
        if not cell_label:
            skipped_regions.append(f"region#{region_idx}: ë¹ˆ cell_label")
            if show_details:
                processing_log.append(f"â­ï¸ region#{region_idx} ìŠ¤í‚µ: ë¹ˆ cell_label")
            continue
            
        if show_details:
            processing_log.append(f"ğŸ” ì²˜ë¦¬ ì¤‘ì¸ region#{region_idx}: '{cell_label}' (ì›ë³¸ í…ìŠ¤íŠ¸ {len(texts)}ê°œ)")
        
        processed_regions.append(f"region#{region_idx}: {cell_label}")
        
        # ê³ ìœ  í‚¤ ìƒì„± (region ì¸ë±ìŠ¤ ì‚¬ìš©)
        unique_key = f"{cell_label}_region{region_idx}"
        
        # í–‰ ë°ì´í„° ì´ˆê¸°í™” (y ìœ„ì¹˜ ì •ë³´ í¬í•¨)
        if region_bounds:
            y_position = (region_bounds[1] + region_bounds[3]) / 2
        else:
            y_positions = [t['bbox'][1] for t in texts if 'bbox' in t]
            y_position = sum(y_positions) / len(y_positions) if y_positions else 0
        
        table_data[unique_key] = {
            'y_position': y_position, 
            'data': {},
            'cell_label': cell_label,
            'region_idx': region_idx
        }
        
        if show_details:
            processing_log.append(f"  âœ… ìƒˆ í–‰ ìƒì„±: '{unique_key}' (y={y_position:.0f})")
        
        # === 0ë‹¨ê³„: ì¤‘ë³µ bbox ì œê±° ===
        cleaned_texts, removal_log = remove_duplicate_texts(texts, overlap_threshold=0.7, show_details=show_details)
        processing_log.extend(removal_log)
        
        if show_details and len(cleaned_texts) != len(texts):
            processing_log.append(f"  âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(texts)}ê°œ â†’ {len(cleaned_texts)}ê°œ")
        
        # === 1ë‹¨ê³„: ê°™ì€ ì…€ì— ìˆëŠ” í…ìŠ¤íŠ¸ë“¤ ê·¸ë£¹í•‘ ë° ë³‘í•© ===
        cell_groups = group_texts_by_cells(cleaned_texts, v_coords, h_coords)
        merged_texts = []  # ë³‘í•©ëœ í…ìŠ¤íŠ¸ë“¤ì„ ì €ì¥
        
        if show_details:
            processing_log.append(f"  ğŸ“Š 1ë‹¨ê³„ - ì…€ë³„ ê·¸ë£¹í•‘: {len(cell_groups)}ê°œ ì…€ì— ë¶„ì‚°")
        
        for cell_key, cell_texts in cell_groups.items():
            row_idx, col_idx = cell_key
            
            # ê°™ì€ ì…€ ë‚´ í…ìŠ¤íŠ¸ë“¤ ë³‘í•©
            merged_text_info = merge_cell_texts(cell_texts)
            
            if merged_text_info:
                original_count = merged_text_info.get('original_count', 1)
                
                if show_details and original_count > 1:
                    processing_log.append(f"    âœ… ì…€({row_idx},{col_idx}): {original_count}ê°œ í…ìŠ¤íŠ¸ ë³‘í•© â†’ '{merged_text_info['text']}'")
                
                merged_texts.append(merged_text_info)
        
        # === 2ë‹¨ê³„: ë³‘í•©ëœ í…ìŠ¤íŠ¸ë“¤ì„ ëŒ€ìƒìœ¼ë¡œ íŒŒì‹± ë° ë¶„ë°° ===
        if show_details:
            processing_log.append(f"  ğŸ“Š 2ë‹¨ê³„ - íŒŒì‹± ë° ë¶„ë°°: {len(merged_texts)}ê°œ ë³‘í•© í…ìŠ¤íŠ¸ ì²˜ë¦¬")
        
        # ì´ë¯¸ ì‚¬ìš©ëœ ì—´ ì¶”ì ì„ ìœ„í•œ set
        used_columns = set()
        
        for merged_text_info in merged_texts:
            merged_text = merged_text_info['text']
            merged_bbox = merged_text_info['bbox']
            
            # ì² ê·¼ ì •ë³´ íŒŒì‹±
            rebar_list = parse_rebar_info(merged_text)
            
            if rebar_list:
                # ë³‘í•©ëœ bboxë¡œ ê±¸ì³ì§„ ì—´ë“¤ ì°¾ê¸°
                overlapping_cols = find_overlapping_columns(merged_bbox, v_coords)
                
                if show_details:
                    processing_log.append(f"    ğŸ¯ '{merged_text}' â†’ íŒŒì‹±: {rebar_list}")
                    processing_log.append(f"    ğŸ“ bbox ê±¸ì³ì§„ ì—´ë“¤: {overlapping_cols}")
                
                # íŒŒì‹±ëœ ì² ê·¼ë“¤ì„ ê±¸ì³ì§„ ì…€ë“¤ì— ë¶„ë°° (ì¶©ëŒ ê°ì§€ í¬í•¨)
                if overlapping_cols:
                    distribution = distribute_rebars_to_cells_with_conflict_detection(
                        rebar_list, overlapping_cols, used_columns
                    )
                    
                    for col_idx, rebar_content in distribution.items():
                        # ì² ê·¼ ì •ë³´ ì •ë¦¬
                        cleaned_rebar = clean_text_content(rebar_content)
                        table_data[unique_key]['data'][col_idx] = cleaned_rebar
                        used_columns.add(col_idx)  # ì‚¬ìš©ëœ ì—´ ê¸°ë¡
                        
                        if show_details:
                            processing_log.append(f"      ğŸ“ ì—´ {col_idx}: {cleaned_rebar}")
            
            else:
                # ì² ê·¼ì´ ì•„ë‹Œ ê²½ìš° (ë¼ë²¨, ìˆ«ì ë“±) - ì›ë˜ ìœ„ì¹˜ì— ë°°ì¹˜
                x_center = (merged_bbox[0] + merged_bbox[2]) / 2
                target_col = 0
                for i in range(len(v_coords) - 1):
                    if v_coords[i] <= x_center <= v_coords[i + 1]:
                        target_col = i
                        break
                
                # ì¶©ëŒ í™•ì¸ ë° íšŒí”¼
                while target_col in used_columns:
                    target_col += 1
                
                # ë¹„ì² ê·¼ í…ìŠ¤íŠ¸ ì •ë¦¬
                cleaned_text = clean_text_content(merged_text)
                table_data[unique_key]['data'][target_col] = cleaned_text
                used_columns.add(target_col)
                
                if show_details:
                    processing_log.append(f"    ğŸ“ ë¹„ì² ê·¼ í…ìŠ¤íŠ¸ â†’ ì—´ {target_col}: {cleaned_text}")
    
    # ì²˜ë¦¬ ìš”ì•½ ë¡œê·¸ ì¶”ê°€
    if show_details:
        processing_log.append(f"\nğŸ“Š === ì²˜ë¦¬ ìš”ì•½ ===")
        processing_log.append(f"ì „ì²´ regions: {len(regions)}ê°œ")
        processing_log.append(f"ì²˜ë¦¬ëœ regions: {len(processed_regions)}ê°œ")
        processing_log.append(f"ìŠ¤í‚µëœ regions: {len(skipped_regions)}ê°œ")
        processing_log.append(f"ìµœì¢… í–‰ ìˆ˜: {len(table_data)}ê°œ (ê° region = 1í–‰)")
        
        if skipped_regions:
            processing_log.append(f"\nâ­ï¸ ìŠ¤í‚µëœ regions:")
            for skip_reason in skipped_regions:
                processing_log.append(f"  â€¢ {skip_reason}")
        
        # ê°™ì€ ë¼ë²¨ì„ ê°€ì§„ regions í‘œì‹œ (ë³‘í•©í•˜ì§€ ì•Šê³  ì •ë³´ë§Œ)
        label_groups = {}
        for region_info in processed_regions:
            region_num, label = region_info.split(': ', 1)
            if label in label_groups:
                label_groups[label].append(region_num)
            else:
                label_groups[label] = [region_num]
        
        same_label_groups = {k: v for k, v in label_groups.items() if len(v) > 1}
        if same_label_groups:
            processing_log.append(f"\nğŸ“‹ ê°™ì€ ë¼ë²¨ì„ ê°€ì§„ regions (ê°ê° ë³„ë„ í–‰ìœ¼ë¡œ ì²˜ë¦¬):")
            for label, region_nums in same_label_groups.items():
                processing_log.append(f"  â€¢ '{label}': {region_nums}")
    
    return table_data, processing_log

def distribute_rebars_to_cells_with_conflict_detection(rebar_list, overlapping_cols, used_columns):
    """íŒŒì‹±ëœ ì² ê·¼ë“¤ì„ ê±¸ì³ì§„ ì…€ë“¤ì— ê°œë³„ ë¶„ë°° - ì¶©ëŒ ê°ì§€ ë° íšŒí”¼"""
    distribution = {}
    
    if not rebar_list or not overlapping_cols:
        return distribution
    
    # ê° ì² ê·¼ì„ ê°œë³„ ì…€ì— ë°°ì¹˜
    for i, rebar in enumerate(rebar_list):
        if i < len(overlapping_cols):
            # ì›ë˜ ê±¸ì³ì§„ ì—´ì— ë°°ì¹˜ ì‹œë„
            col_idx = overlapping_cols[i]
        else:
            # ê±¸ì³ì§„ ì—´ì„ ë²—ì–´ë‚˜ë©´ ë§ˆì§€ë§‰ ì—´ ì´í›„ë¡œ ê³„ì† í™•ì¥
            col_idx = overlapping_cols[-1] + (i - len(overlapping_cols) + 1)
        
        # ì¶©ëŒ íšŒí”¼: ì´ë¯¸ ì‚¬ìš©ëœ ì—´ì´ë©´ ë‹¤ìŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì—´ ì°¾ê¸°
        while col_idx in used_columns or col_idx in distribution:
            col_idx += 1
        
        distribution[col_idx] = rebar
    
    return distribution

def create_dataframe_from_table_data(table_data):
    """í‘œ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ (ì‹¤ì œ ìœ„ì¹˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬) - ê³ ìœ í‚¤ ì§€ì›"""
    
    if not table_data:
        return pd.DataFrame()
    
    # ëª¨ë“  ì—´ ì¸ë±ìŠ¤ ìˆ˜ì§‘
    all_columns = set()
    for row_info in table_data.values():
        all_columns.update(row_info['data'].keys())
    
    max_col = max(all_columns) if all_columns else 0
    num_cols = max_col + 1
    
    # í–‰ë“¤ì„ y ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìœ„ì—ì„œ ì•„ë˜ë¡œ)
    sorted_rows = sorted(table_data.items(), key=lambda x: x[1]['y_position'])
    
    # ë°ì´í„° ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    data_matrix = []
    row_labels = []
    
    for unique_key, row_info in sorted_rows:
        # ì‹¤ì œ cell_labelì„ í–‰ ë¼ë²¨ë¡œ ì‚¬ìš©
        cell_label = row_info.get('cell_label', unique_key)
        row_labels.append(cell_label)
        
        row_data = []
        for col_index in range(num_cols):
            content = row_info['data'].get(col_index, '')
            row_data.append(content)
        data_matrix.append(row_data)
    
    # DataFrame ìƒì„± (í–‰ ë¼ë²¨ í¬í•¨, y ìœ„ì¹˜ìˆœ ì •ë ¬)
    df = pd.DataFrame(data_matrix, index=row_labels)
    
    return df

def create_excel_file_from_dataframe(df):
    """DataFrameì„ ì—‘ì…€ íŒŒì¼ë¡œ ìƒì„± (ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ ë°˜í™˜)"""
    
    # ë©”ëª¨ë¦¬ì—ì„œ ì—‘ì…€ íŒŒì¼ ìƒì„±
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='í‘œë°ì´í„°', index=True, header=False)
        
        # ì›Œí¬ë¶ê³¼ ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸°
        workbook = writer.book
        worksheet = writer.sheets['í‘œë°ì´í„°']
        
        # ì…€ ìŠ¤íƒ€ì¼ë§ (í…Œë‘ë¦¬ ì¶”ê°€)
        thin_border = Border(
            left=Side(style='thin'),
            right=Side(style='thin'),
            top=Side(style='thin'),
            bottom=Side(style='thin')
        )
        
        # ëª¨ë“  ì…€ì— í…Œë‘ë¦¬ ì ìš©
        for row in range(1, df.shape[0] + 2):  # +2 for header
            for col in range(1, df.shape[1] + 2):  # +2 for index
                cell = worksheet.cell(row=row, column=col)
                cell.border = thin_border
                cell.alignment = Alignment(horizontal='center', vertical='center')
    
    output.seek(0)
    return output.getvalue()

def process_table_to_excel(pickle_file, show_details=False):
    """ë©”ì¸ ì²˜ë¦¬ í•¨ìˆ˜"""
    try:
        # 1. í”¼í´ ë°ì´í„° ë¡œë“œ
        data = load_pickle_data(pickle_file)
        if data is None:
            return None, None, None
        
        horizontal_lines = data.get('horizontal_lines', [])
        vertical_lines = data.get('vertical_lines', [])
        regions = data.get('regions', [])
        
        st.success(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì˜ì—­ {len(regions)}ê°œ")
        
        # === cell_label ë¶„ì„ ì¶”ê°€ ===
        all_labels = []
        empty_labels = 0
        label_counts = {}
        
        for i, region in enumerate(regions):
            cell_label = region.get('cell_label', '').strip()
            if not cell_label:
                empty_labels += 1
                all_labels.append(f"[ë¹ˆë¼ë²¨_{i}]")
            else:
                all_labels.append(cell_label)
                if cell_label in label_counts:
                    label_counts[cell_label] += 1
                else:
                    label_counts[cell_label] = 1
        
        valid_labels = [label for label in all_labels if not label.startswith('[ë¹ˆë¼ë²¨_')]
        unique_label_names = list(set(valid_labels))
        multiple_occurrence_labels = {k: v for k, v in label_counts.items() if v > 1}
        
        # cell_label í†µê³„ í‘œì‹œ
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ì „ì²´ regions", len(regions))
        with col2:
            st.metric("ìœ íš¨í•œ ë¼ë²¨", len(valid_labels))
        with col3:
            st.metric("ë¹ˆ ë¼ë²¨", empty_labels)
        with col4:
            st.metric("ê³ ìœ  ë¼ë²¨ëª…", len(unique_label_names))
        
        if show_details:
            st.write("**ğŸ“‹ cell_label ìƒì„¸ ë¶„ì„:**")
            st.write(f"- ëª¨ë“  ë¼ë²¨: {all_labels}")
            
            if multiple_occurrence_labels:
                st.write("**ğŸ“Š ì—¬ëŸ¬ ë²ˆ ë‚˜íƒ€ë‚˜ëŠ” ë¼ë²¨ë“¤ (ì •ìƒ):**")
                for label, count in multiple_occurrence_labels.items():
                    st.write(f"  â€¢ '{label}': {count}ë²ˆ ë“±ì¥")
            
            if empty_labels > 0:
                st.write(f"**âŒ ë¹ˆ ë¼ë²¨ {empty_labels}ê°œ ë°œê²¬**")
        
        # 2. ê²©ìì„ ì—ì„œ ì¢Œí‘œ ì¶”ì¶œ
        h_coords, v_coords = extract_line_coordinates(horizontal_lines, vertical_lines)
        
        st.write(f"ğŸ“Š ê²©ì ì •ë³´: ìˆ˜í‰ì„  {len(h_coords)}ê°œ, ìˆ˜ì§ì„  {len(v_coords)}ê°œ")
        
        # ë””ë²„ê¹… ì •ë³´ (ì¼ë°˜ ì¶œë ¥)
        if show_details:
            st.write("**ğŸ” ê²©ìì„  ìƒì„¸ ì •ë³´:**")
            st.write(f"- ìˆ˜í‰ì„  ì¢Œí‘œ(y): {h_coords[:10]}{'...' if len(h_coords) > 10 else ''}")
            st.write(f"- ìˆ˜ì§ì„  ì¢Œí‘œ(x): {v_coords[:10]}{'...' if len(v_coords) > 10 else ''}")
            
            # í–‰ ì •ë ¬ ì •ë³´ í‘œì‹œ
            region_positions = []
            region_text_counts = []
            
            for i, region in enumerate(regions):
                cell_label = region.get('cell_label', f'region_{i}').strip()
                region_bounds = region.get('bounds', None)
                texts = region.get('texts', [])
                
                if region_bounds:
                    y_pos = (region_bounds[1] + region_bounds[3]) / 2
                    region_positions.append((cell_label, y_pos, i))
                    region_text_counts.append((cell_label, len(texts)))
            
            region_positions.sort(key=lambda x: x[1])  # y ìœ„ì¹˜ìˆœ ì •ë ¬
            st.write("**ğŸ“ í–‰ ìˆœì„œ (ìœ„â†’ì•„ë˜):**")
            for j, (label, y_pos, region_idx) in enumerate(region_positions[:15]):
                text_count = next((count for name, count in region_text_counts if name == label), 0)
                empty_status = "ğŸ”´" if not label or label.startswith('[ë¹ˆë¼ë²¨_') else "ğŸŸ¢"
                st.write(f"  {j+1}. {empty_status} '{label}' (y={y_pos:.0f}, í…ìŠ¤íŠ¸ {text_count}ê°œ, region#{region_idx})")
            if len(region_positions) > 15:
                st.write(f"  ... ì´ {len(region_positions)}ê°œ í–‰")
            
            # ì „ì²´ í…ìŠ¤íŠ¸ í†µê³„
            total_texts = sum(len(region.get('texts', [])) for region in regions)
            st.write(f"**ğŸ“Š ì „ì²´ OCR í…ìŠ¤íŠ¸: {total_texts}ê°œ**")
        
        # 3. í–‰ ê¸°ì¤€ìœ¼ë¡œ region ë°ì´í„° ì²˜ë¦¬ (h_coords ì¶”ê°€)
        table_data, processing_log = process_regions_by_rows(regions, v_coords, h_coords, show_details)
        
        # === ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦ ===
        processed_labels = list(table_data.keys())
        expected_count = len(valid_labels)  # ë¹ˆ ë¼ë²¨ ì œì™¸í•œ ìœ íš¨í•œ ë¼ë²¨ ê°œìˆ˜
        
        st.write("**ğŸ” ì²˜ë¦¬ ê²°ê³¼ ê²€ì¦:**")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì˜ˆìƒ ì²˜ë¦¬ ê°€ëŠ¥ í–‰", expected_count)
        with col2:
            st.metric("ì‹¤ì œ ì²˜ë¦¬ëœ í–‰", len(processed_labels))
        with col3:
            difference = len(processed_labels) - expected_count
            st.metric("ì°¨ì´", difference, delta=f"{difference:+d}")
        
        if show_details:
            st.write(f"**ğŸ“Š ì²˜ë¦¬ëœ ë¼ë²¨ë“¤:** {processed_labels}")
            
            # ë¹ˆ ë¼ë²¨ë¡œ ì¸í•œ ëˆ„ë½ í™•ì¸
            if empty_labels > 0:
                st.write(f"**â„¹ï¸ {empty_labels}ê°œ regionì€ ë¹ˆ ë¼ë²¨ë¡œ ì¸í•´ ì²˜ë¦¬ ì œì™¸ë¨**")
        
        # 4. DataFrame ìƒì„±
        df = create_dataframe_from_table_data(table_data)
        
        if df.empty:
            st.error("ìƒì„±ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
            return None, None, processing_log
        
        # 5. ì—‘ì…€ íŒŒì¼ ìƒì„±
        excel_data = create_excel_file_from_dataframe(df)
        
        return excel_data, df, processing_log
        
    except Exception as e:
        st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        if show_details:
            st.code(traceback.format_exc())
        return None, None, None
#------------------------------------------













# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Streamlit í˜¸ì¶œë¶€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Slab ì¶”ì¶œ", layout="wide")
st.title("Slab ì¶”ì¶œ")



tab1, tab2, tab3 = st.tabs(["SCD Slab_info extraction", "SDD Slab_info extraction", "Human Error detection"])

with tab1:
    # ì´ˆê¸°í™”
    for key, d in [("boxes",[]),("anchors",[])]:
        if key not in st.session_state: st.session_state[key]=d

    uploaded = st.file_uploader("PDF ì—…ë¡œë“œ", type="pdf")
    if uploaded and st.button("Step 0: PDFâ†’ì´ë¯¸ì§€"):
        paths = convert_pdf_to_images(uploaded)
        st.success(f"{len(paths)}í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ")

    # Step 1
    with st.expander("Step 1: ë ˆì´ì•„ì›ƒ ë¶„ì„"):
        if st.button("ë¶„ì„ ì‹¤í–‰"):
            img, boxes = analyze_first_page()
            if img:
                # 1) ì¤‘ë³µ ë°•ìŠ¤ ì œê±° (IoU > 0.5 ì´ë©´ ê°™ì€ ì˜ì—­ìœ¼ë¡œ ê°„ì£¼)
                unique = []
                for b in boxes:
                    if not any(compute_iou(b, u) > 0.5 for u in unique):
                        unique.append(b)
                boxes = unique

                # 2) ì„¸ì…˜ì— ì›ë³¸ ì´ë¯¸ì§€ì™€ ë°•ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì €ì¥
                st.session_state["first_img"] = img.copy()
                st.session_state["boxes"]    = boxes

                # 3) ì´ë¯¸ì§€ì— ë°•ìŠ¤ì™€ ë²ˆí˜¸ ê·¸ë¦¬ê¸°
                draw = ImageDraw.Draw(img)
                try:
                    font = ImageFont.truetype("arial.ttf", 150)
                except IOError:
                    font = ImageFont.load_default()

                for idx, (x1, y1, x2, y2) in enumerate(boxes):
                    # ë°•ìŠ¤
                    draw.rectangle([(x1, y1), (x2, y2)], outline="blue", width=20)
                    # ë²ˆí˜¸
                    text = str(idx)
                    bbox = draw.textbbox((0, 0), text, font=font)
                    tw, th = bbox[2] - bbox[0], bbox[3] - bbox[1]
                    tx = x1 + (x2 - x1 - tw) // 2
                    ty = max(0, y1 - th - 5)
                    draw.text((tx, ty), text, fill="red", font=font)

                # 4) ê²°ê³¼ í‘œì‹œ
                st.image(img, use_column_width=True)
            else:
                st.error("ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")

    # Step 2
    with st.expander("Step 2: ì•µì»¤ ì„ íƒ", expanded=True):
        first_img = st.session_state.get("first_img", None)
        boxes     = st.session_state.get("boxes", [])

        if first_img is None or not boxes:
            st.info("ë¨¼ì € Step 1ì—ì„œ ë ˆì´ì•„ì›ƒ ë¶„ì„ì„ ì‹¤í–‰í•˜ì„¸ìš”")
        else:
            # ì¸ë±ìŠ¤ ì…ë ¥ UI
            idxs = st.text_input("ë°•ìŠ¤ ì¸ë±ìŠ¤ ì…ë ¥ (ì‰¼í‘œë¡œ êµ¬ë¶„)", key="idxs_input")

            if st.button("âœ… ì•µì»¤ ì €ì¥"):
                # ì„ íƒëœ ì•µì»¤ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
                selected = [int(s) for s in idxs.split(",") if s.strip().isdigit()]
                # ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ í•„í„°
                anchors = [boxes[i] for i in selected if 0 <= i < len(boxes)]
                st.session_state["anchors"] = anchors

                # ë¯¸ë¦¬ë³´ê¸°ìš© ì´ë¯¸ì§€ ìƒì„±
                preview = first_img.copy()
                draw    = ImageDraw.Draw(preview)
                for (x1, y1, x2, y2) in anchors:
                    draw.rectangle([(x1, y1), (x2, y2)], outline="red", width=20)

                # í™”ë©´ì— í‘œì‹œ
                st.image(preview, caption=f"ì„ íƒëœ ì•µì»¤: {selected}", use_column_width=True)
                st.success(f"ì €ì¥ëœ ì•µì»¤ ì¢Œí‘œ: {anchors}")

    # Step 3
    with st.expander("Step 3: ì¶”ì¶œ ì‹¤í–‰", expanded=True):
        if st.button("â¡ï¸ ì¶”ì¶œ ì‹œì‘"):
            anchors = st.session_state.get("anchors", [])
            if not anchors:
                st.error("ë¨¼ì € Step 2ì—ì„œ ì•µì»¤ë¥¼ ì €ì¥í•˜ì„¸ìš”")
            else:
                extract_with_offset(anchors, margin_right=500)


    with st.expander("Step 4: OCR ì ìš©", expanded=True):
        if st.button("ğŸ” OCR ì‹¤í–‰"):
            apply_surya_ocr_to_anchors()


    with st.expander("Step 4: ëª¨ë“  í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ", expanded=True):
        # 1) ì²« í˜ì´ì§€ ì˜ˆì‹œ ì´ë¯¸ì§€ ë³´ì—¬ì£¼ê¸°

        anchor_folder = os.path.join(BASE_DIR, "Slab_anchor_img")
        preview_paths = sorted(glob.glob(os.path.join(anchor_folder, "*page_1_box*.png")))

        if preview_paths:
            cols = st.columns(len(preview_paths))
            for col, img_p in zip(cols, preview_paths):
                col.image(Image.open(img_p), use_column_width=True)
                col.caption(os.path.basename(img_p))
        else:
            st.info("ë¨¼ì € Step 3ì—ì„œ í¬ë¡­ì„ ì™„ë£Œí•˜ì„¸ìš”.")

        # 2) í‚¤ì›Œë“œ ì…ë ¥ (ìµœëŒ€ 4ê°œ)
        st.markdown("**ì¶”ì¶œí•  í‚¤ì›Œë“œë¥¼ ìµœëŒ€ 4ê°œ ì…ë ¥í•˜ì„¸ìš”. ë¹ˆ ì¹¸ì€ ë¬´ì‹œë©ë‹ˆë‹¤.**")
        kws = [st.text_input(f"í‚¤ì›Œë“œ {i+1}", key=f"kw_all_{i}") for i in range(4)]
        keys = [k.strip() for k in kws if k.strip()]

        # 3) ì „ì²´ í˜ì´ì§€ ì¶”ì¶œ ì‹¤í–‰
        if st.button("ğŸš€ ì „ì²´ í˜ì´ì§€ ì¶”ì¶œ ì‹¤í–‰"):
            if not keys:
                st.warning("ìµœì†Œ í•˜ë‚˜ì˜ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # ì „ì²´ í˜ì´ì§€ ì •ë³´ë¥¼ ì¶”ì¶œÂ·ì €ì¥
                all_results = extract_all_pages(keys)

                # ì²« 2í˜ì´ì§€ë§Œ í™”ë©´ì— í…Œì´ë¸”ë¡œ í‘œì‹œ
                for page_num in sorted(all_results)[:2]:
                    info = all_results[page_num]
                    st.subheader(f"â–¶ï¸ Page {page_num}")
                    rows = [{"í‚¤ì›Œë“œ": k, "ê°’": info.get(k) or "â€”"} for k in keys]
                    st.table(rows)

                st.info(f"âš™ï¸ ì´ {len(all_results)}í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ. ë‚˜ë¨¸ì§€ ê²°ê³¼ëŠ” '{Slab_elements}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


    with st.expander("Step 6: í…Œì´ë¸” OCR ì ìš©", expanded=True):
        if st.button("ğŸ” í…Œì´ë¸” OCR ì‹¤í–‰"):
            apply_surya_ocr_to_tables()


    with st.expander("Step 7 : í…Œì´ë¸” ì •ë ¬"):
        y_tol = st.number_input("Y tolerance", value=10, min_value=0)
        x_tol = st.number_input("X tolerance", value=20, min_value=0)

        if st.button("ë³€í™˜ ì‹¤í–‰"):
            # 1) JSON â†’ Excel ë³€í™˜
            result = parse_ocr_jsons_to_excel(y_tol, x_tol)
            if not result:
                st.error("ë³€í™˜í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                st.stop()
            st.success(f"ë³€í™˜ ì™„ë£Œ: {result}")

            # 2) ê²°ê³¼ ë¡œë“œ ë° íŒŒì¼ ì„ íƒ
            df = pd.read_excel(result)
            files = df['file'].unique().tolist()
            selected = st.selectbox("ë¯¸ë¦¬ë³´ê¸°í•  íŒŒì¼ ì„ íƒ", files)
            base = os.path.splitext(selected)[0]

            # 3) ì´ë¯¸ì§€ ë¡œë“œ
            shown = False
            for ext in (".png", ".PNG", ".jpg", ".JPG", ".jpeg", ".JPEG"):
                img_path = os.path.join(Slab_table, base + ext)
                if os.path.exists(img_path):
                    img = Image.open(img_path)
                    st.image(img, caption=os.path.basename(img_path), use_column_width=True)
                    shown = True
                    break
            if not shown:
                st.warning(f"ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {base}(.png/.jpg/.jpeg ë“±)")

            # 4) í‘œ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸° (ê°€ë¡œ ê½‰ ì±„ìš°ê¸°)
            preview = df[df['file'] == selected].drop(columns=["file"])
            st.dataframe(preview, use_container_width=True, height=300)












































#-------------------------------------------------------------------------------------


with tab2:
    uploaded = st.file_uploader("SDD PDF ì—…ë¡œë“œ", type="pdf")
    if uploaded and st.button("Step 0:SDD: PDFâ†’ì´ë¯¸ì§€"):
        paths = convert_pdf_to_images_SDD(uploaded)
        st.success(f"{len(paths)}í˜ì´ì§€ ë³€í™˜ ì™„ë£Œ")



    with st.expander("ë°”ìš´ë”©ë°•ìŠ¤ ê·¸ë¦¬ê¸°_í¬ë¡­ìš©"):
        # "ë°”ìš´ë”©ë°•ìŠ¤ ì‹œì‘" ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ UIê°€ ë‚˜íƒ€ë‚˜ê²Œ
        if "show_box_ui" not in st.session_state:
            st.session_state.show_box_ui = False

        if st.button("ë°”ìš´ë”©ë°•ìŠ¤ ì‹œì‘"):
            st.session_state.show_box_ui = True

        if st.session_state.show_box_ui:
            draw_and_save_bounding_boxes_slab_SDD(canvas_key="box_crop")
            # "ì™„ë£Œí•˜ê¸°" ë²„íŠ¼ì„ ë„£ì–´ì„œ ëˆ„ë¥´ë©´ UI ë‹«í˜
            if st.button("ì™„ë£Œí•˜ê¸°"):
                st.session_state.show_box_ui = False




    with st.expander("ì „ì²´ OCR ë•Œë¦¬ê¸°"):
        # ë‹¨ê³„ 1: OCR
        if st.button("OCR ì‹¤í–‰"):
            apply_surya_ocr_Wall_slab_SDD()




    with st.expander("ë°”ìš´ë”©ë°•ìŠ¤_ë¼ì¸ë”°ê¸°"):
        # "ë°”ìš´ë”©ë°•ìŠ¤ ì‹œì‘" ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ UIê°€ ë‚˜íƒ€ë‚˜ê²Œ
        if "show_box_ui" not in st.session_state:
            st.session_state.show_box_ui = False

        if st.button("ë°”ìš´ë”©ë°•ìŠ¤ ì‹œì‘_ë¼ì¸ë”°ê¸°"):
            st.session_state.show_box_ui = True

        if st.session_state.show_box_ui:
            draw_and_save_bounding_boxes_Slab_SDD(canvas_key="box_line_scroll")
            
            # "ì™„ë£Œí•˜ê¸°" ë²„íŠ¼ì„ ë„£ì–´ì„œ ëˆ„ë¥´ë©´ UI ë‹«í˜
            if st.button("ì™„ë£Œí•˜ê¸°_ë¼ì¸ë”°ê¸°"):
                st.session_state.show_box_ui = False



    with st.expander("ìˆ˜í‰/ìˆ˜ì§ì„  + êµì°¨ì  + í…ìŠ¤íŠ¸ ì¸ì‹", expanded=True):
            img_dir = st.text_input("ì…ë ¥ ì´ë¯¸ì§€ í´ë” ê²½ë¡œ", value=r"D:\4parts_complete\slab\raw_data_column_label_SDD")
            save_dir = st.text_input("ê²°ê³¼ ì €ì¥ í´ë”ëª… (ë¹„ìš°ë©´ ì €ì¥ X)", value=r"D:\4parts_complete\slab\Slab_table_region")
            boxes_coord_dir = st.text_input("ì‚¬ìš©ì ë°•ìŠ¤ ì¢Œí‘œ í´ë” (*_boxes.json)", value=r"D:\4parts_complete\slab\Slab_same_line")
            ocr_coord_dir = st.text_input("OCR ê²°ê³¼ í´ë” (*.json)", value=r"D:\4parts_complete\slab\raw_data_OCR_SDD")
            text_tolerance = st.slider("í…ìŠ¤íŠ¸-ì„  ë§¤ì¹­ í—ˆìš© ì˜¤ì°¨", 5, 30, 15, 5)
            
            mode = st.radio("ì„  ê²€ì¶œ ë°©ì‹", options=["contour", "hough"], index=0)
            min_line_length = st.slider("min_line_length", 20, 300, 80, 5)
            max_line_gap = st.slider("max_line_gap", 2, 40, 10, 2)
            hough_threshold = st.slider("Hough threshold", 30, 300, 100, 5)
            morph_kernel_scale = st.slider("Morph kernel ë¹„ìœ¨", 10, 60, 30, 2)
            block_size = st.slider("adaptiveThreshold blockSize(í™€ìˆ˜)", 7, 31, 15, 2)
            C = st.slider("adaptiveThreshold C", -10, 10, -2, 1)
            tol = st.slider("êµì°¨ì  í—ˆìš© ì˜¤ì°¨ (tol)", 0, 10, 2, 1)

            run_btn = st.button("ì˜ˆì‹œ 5ê°œ ë³´ê¸°")
            save_btn = st.button("ì „ì²´ ì €ì¥")

            if run_btn:
                imgs = detect_and_draw_lines_batch(
                    img_dir=img_dir,
                    save_dir=None,
                    min_line_length=min_line_length,
                    max_line_gap=max_line_gap,
                    hough_threshold=hough_threshold,
                    morph_kernel_scale=morph_kernel_scale,
                    max_examples=5,
                    return_imgs=True,
                    mode=mode,
                    block_size=block_size | 1,
                    C=C,
                    tol=tol,
                    boxes_coord_dir=boxes_coord_dir if boxes_coord_dir else None,
                    ocr_coord_dir=ocr_coord_dir if ocr_coord_dir else None,
                    text_tolerance=text_tolerance
                )
                if not imgs:
                    st.warning("ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
                else:
                    for result in imgs:
                        if len(result) == 4:
                            fname, img, inter_cnt, text_cnt = result
                            st.subheader(f"{fname} (êµì°¨ì : {inter_cnt}ê°œ, ì„ ìƒ í…ìŠ¤íŠ¸: {text_cnt}ê°œ)")
                        else:
                            fname, img, inter_cnt = result[:3]
                            st.subheader(f"{fname} (êµì°¨ì : {inter_cnt}ê°œ)")
                        st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), caption="ê²€ì¶œ ê²°ê³¼", use_column_width=True)

            if save_btn:
                detect_and_draw_lines_batch(
                    img_dir=img_dir,
                    save_dir=save_dir if save_dir else None,
                    min_line_length=min_line_length,
                    max_line_gap=max_line_gap,
                    hough_threshold=hough_threshold,
                    morph_kernel_scale=morph_kernel_scale,
                    max_examples=None,
                    return_imgs=False,
                    mode=mode,
                    block_size=block_size | 1,
                    C=C,
                    tol=tol,
                    boxes_coord_dir=boxes_coord_dir if boxes_coord_dir else None,
                    ocr_coord_dir=ocr_coord_dir if ocr_coord_dir else None,
                    text_tolerance=text_tolerance
                )
                st.success(f"ì „ì²´ ì´ë¯¸ì§€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ! â†’ '{save_dir}' í´ë” í™•ì¸")





    # with st.expander("ğŸ“ í´ë” ë‚´ ëª¨ë“  JSON ì •ì œí•˜ê¸°"):
    #     folder = st.text_input("JSON í´ë” ê²½ë¡œ", value=Slab_table_region)
    #     merge_tol = st.number_input("ë³‘í•© ê¸°ì¤€ í”½ì…€ ê±°ë¦¬ (merge_tol)", min_value=0, max_value=100, value=10, step=1)
    #     if st.button("âœ… ì •ì œ ì‹¤í–‰"):
    #         if not os.path.isdir(folder):
    #             st.error("ìœ íš¨í•œ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    #         else:
    #             json_files = sorted([f for f in os.listdir(folder) if f.lower().endswith(".json")])
    #             if not json_files:
    #                 st.warning("í´ë”ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
    #             else:
    #                 save_folder = os.path.join(folder, "cleaned_merged")
    #                 os.makedirs(save_folder, exist_ok=True)
    #                 downloaded = []

    #                 for fname in json_files:
    #                     path = os.path.join(folder, fname)
    #                     with open(path, encoding="utf-8") as f:
    #                         regions = json.load(f)

    #                     # í•„í„°ë§ + ë³‘í•©
    #                     cleaned = filter_regions_text(regions, merge_tol)

    #                     out_name = fname.replace(".json", "_cleaned.json")
    #                     out_path = os.path.join(save_folder, out_name)
    #                     with open(out_path, "w", encoding="utf-8") as wf:
    #                         json.dump(cleaned, wf, ensure_ascii=False, indent=2)

    #                     downloaded.append((out_name, json.dumps(cleaned, ensure_ascii=False, indent=2)))

    #                 st.success(f"{len(downloaded)}ê°œ íŒŒì¼ ì •ì œ ë° ë³‘í•© ì™„ë£Œ! (í´ë”: {save_folder})")
    #                 for out_name, content in downloaded:
    #                     st.download_button(
    #                         label=f"ğŸ“¥ {out_name} ë‹¤ìš´ë¡œë“œ",
    #                         data=content,
    #                         file_name=out_name,
    #                         mime="application/json"
    #                     )








    # with st.expander("í´ëŸ¬ìŠ¤í„°ë§ ì„¤ì • ë° ë¯¸ë¦¬ë³´ê¸°/ì €ì¥"):
    #     tol = st.number_input("tol ê°’ (í”½ì…€)", min_value=0, max_value=500, value=60, step=10)

    #     # 1ï¸âƒ£ ì‹œë²” ì‹¤í–‰ (ì²« ë²ˆì§¸ í´ë¦°ëœ íŒŒì¼ë§Œ)
    #     if st.button("1ï¸âƒ£ ì‹œë²” ì‹¤í–‰"):
    #         df_preview = preview_first_file(Slab_text_clean, tol)
    #         if df_preview.empty:
    #             st.warning("í´ë¦°ëœ í´ë”ì— JSON íŒŒì¼ì´ ì—†ê±°ë‚˜ tol ê°’ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤")
    #         else:
    #             st.write("### ì²« ë²ˆì§¸ í´ë¦°ëœ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°")
    #             st.dataframe(df_preview)

    #     # ğŸ“‚ ì „ì²´ ì—‘ì…€ ì €ì¥ (í´ë¦°ëœ JSON â†’ ì—‘ì…€)
    #     if st.button("ğŸ“‚ ì „ì²´ ì—‘ì…€ ì €ì¥"):
    #         saved = save_all_files(Slab_text_clean, Slab_table_excel, tol)  # â† ì—¬ê¸°ë§Œ ë³€ê²½
    #         if saved:
    #             st.success(f"{len(saved)}ê°œ íŒŒì¼ì„ '{Slab_table_excel}' í´ë”ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤")
    #         else:
    #             st.warning("í´ë¦°ëœ í´ë”ì— ì €ì¥í•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")





















    with st.expander("ğŸ” í”¼í´ ë°ì´í„° ì™„ì „ ë¶„ì„", expanded=False):
        st.markdown("### í”¼í´ íŒŒì¼ë“¤ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ cell_labelë³„ë¡œ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤")
        
        # í´ë” ê²½ë¡œ ì…ë ¥
        analysis_folder = st.text_input(
            "ğŸ“ ë¶„ì„í•  í”¼í´ íŒŒì¼ í´ë” ê²½ë¡œ",
            placeholder="ì˜ˆ: D:/data/pickle_files",
            help="OCR ì²˜ë¦¬ëœ í‘œ ë°ì´í„°ê°€ ë‹´ê¸´ í”¼í´ íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ",
            key="analysis_folder_unique"
        )
        
        if analysis_folder:
            analyze_button = st.button("ğŸ” ì™„ì „ ë¶„ì„ ì‹œì‘", type="primary", key="analyze_folder_button_unique")
            
            if analyze_button:
                try:
                    # í´ë” ì¡´ì¬ í™•ì¸
                    if not os.path.exists(analysis_folder):
                        st.error(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {analysis_folder}")
                        st.stop()
                    
                    # í”¼í´ íŒŒì¼ ì°¾ê¸°
                    pickle_files = []
                    for file in os.listdir(analysis_folder):
                        if file.lower().endswith('.pkl'):
                            pickle_files.append(os.path.join(analysis_folder, file))
                    
                    if not pickle_files:
                        st.warning(f"âš ï¸ í´ë”ì— í”¼í´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {analysis_folder}")
                        st.stop()
                    
                    st.success(f"âœ… {len(pickle_files)}ê°œì˜ í”¼í´ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                    
                    # ë¶„ì„í•  íŒŒì¼ ì„ íƒ
                    selected_file = st.selectbox(
                        "ë¶„ì„í•  íŒŒì¼ ì„ íƒ",
                        options=pickle_files,
                        format_func=lambda x: os.path.basename(x),
                        key="analysis_file_select"
                    )
                    
                    if selected_file:
                        # ì„ íƒëœ íŒŒì¼ ë¶„ì„
                        with open(selected_file, 'rb') as f:
                            data = pickle.load(f)
                        
                        if data is not None:
                            regions = data.get('regions', [])
                            st.success(f"âœ… í”¼í´ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(regions)}ê°œ regions")
                            
                            # ì „ì²´ í†µê³„
                            total_texts = sum(len(region.get('texts', [])) for region in regions)
                            st.info(f"ğŸ“Š ì „ì²´ OCR í…ìŠ¤íŠ¸: {total_texts}ê°œ")
                            
                            # cell_labelë³„ ìƒì„¸ ë¶„ì„
                            st.markdown("### ğŸ“‹ cell_labelë³„ ìƒì„¸ ë¶„ì„")
                            
                            for i, region in enumerate(regions):
                                cell_label = region.get('cell_label', f'region_{i}').strip()
                                texts = region.get('texts', [])
                                bounds = region.get('bounds', None)
                                
                                with st.container():
                                    st.markdown(f"#### ğŸ·ï¸ **{cell_label}** ({len(texts)}ê°œ í…ìŠ¤íŠ¸)")
                                    
                                    # Region ì •ë³´
                                    col1, col2 = st.columns(2)
                                    with col1:
                                        if bounds:
                                            st.write(f"ğŸ“ ì˜ì—­ bounds: {bounds}")
                                            y_center = (bounds[1] + bounds[3]) / 2
                                            st.write(f"ğŸ“ Y ì¤‘ì‹¬: {y_center:.0f}")
                                    
                                    with col2:
                                        st.write(f"ğŸ“ í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(texts)}")
                                    
                                    # í…ìŠ¤íŠ¸ ìƒì„¸ ì •ë³´
                                    if texts:
                                        st.write("**ğŸ“ ì¸ì‹ëœ í…ìŠ¤íŠ¸ë“¤:**")
                                        
                                        # í…Œì´ë¸” í˜•íƒœë¡œ ì •ë¦¬
                                        text_data = []
                                        for j, text_info in enumerate(texts):
                                            bbox = text_info.get('bbox', [0,0,0,0])
                                            text = text_info.get('text', '')
                                            confidence = text_info.get('confidence', 0)
                                            x_center = (bbox[0] + bbox[2]) / 2 if len(bbox) >= 4 else 0
                                            
                                            # ì² ê·¼ ì •ë³´ì¸ì§€ í™•ì¸
                                            rebar_parsed = parse_rebar_info(text)
                                            is_rebar = "âœ…" if rebar_parsed else "âŒ"
                                            
                                            text_data.append({
                                                'ìˆœë²ˆ': j+1,
                                                'í…ìŠ¤íŠ¸': text,
                                                'Xì¤‘ì‹¬': f"{x_center:.0f}",
                                                'ì‹ ë¢°ë„': f"{confidence:.2f}",
                                                'ì² ê·¼ì •ë³´': is_rebar,
                                                'íŒŒì‹±ê²°ê³¼': ', '.join(rebar_parsed) if rebar_parsed else '-'
                                            })
                                        
                                        # DataFrameìœ¼ë¡œ í‘œì‹œ
                                        df_texts = pd.DataFrame(text_data)
                                        st.dataframe(df_texts, use_container_width=True)
                                        
                                        # ì² ê·¼ ì •ë³´ í†µê³„
                                        rebar_count = sum(1 for text_info in texts if parse_rebar_info(text_info.get('text', '')))
                                        total_parsed = sum(len(parse_rebar_info(text_info.get('text', ''))) for text_info in texts)
                                        
                                        col1, col2, col3 = st.columns(3)
                                        with col1:
                                            st.metric("ì „ì²´ í…ìŠ¤íŠ¸", len(texts))
                                        with col2:
                                            st.metric("ì² ê·¼ í…ìŠ¤íŠ¸", rebar_count)
                                        with col3:
                                            st.metric("íŒŒì‹±ëœ ì² ê·¼", total_parsed)
                                    
                                    else:
                                        st.warning("í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                                    
                                    st.divider()
                            
                            # ì „ì²´ ìš”ì•½
                            st.markdown("### ğŸ“Š ì „ì²´ ìš”ì•½")
                            
                            total_rebar_texts = 0
                            total_parsed_rebars = 0
                            summary_data = []
                            
                            for region in regions:
                                cell_label = region.get('cell_label', '').strip()
                                texts = region.get('texts', [])
                                
                                rebar_texts = sum(1 for text_info in texts if parse_rebar_info(text_info.get('text', '')))
                                parsed_rebars = sum(len(parse_rebar_info(text_info.get('text', ''))) for text_info in texts)
                                
                                total_rebar_texts += rebar_texts
                                total_parsed_rebars += parsed_rebars
                                
                                summary_data.append({
                                    'cell_label': cell_label,
                                    'ì „ì²´_í…ìŠ¤íŠ¸': len(texts),
                                    'ì² ê·¼_í…ìŠ¤íŠ¸': rebar_texts,
                                    'íŒŒì‹±ëœ_ì² ê·¼': parsed_rebars
                                })
                            
                            # ìš”ì•½ í…Œì´ë¸”
                            df_summary = pd.DataFrame(summary_data)
                            st.dataframe(df_summary, use_container_width=True)
                            
                            # ìµœì¢… í†µê³„
                            col1, col2, col3, col4 = st.columns(4)
                            with col1:
                                st.metric("ì „ì²´ OCR í…ìŠ¤íŠ¸", total_texts)
                            with col2:
                                st.metric("ì² ê·¼ ê´€ë ¨ í…ìŠ¤íŠ¸", total_rebar_texts)
                            with col3:
                                st.metric("íŒŒì‹±ëœ ì² ê·¼ ì •ë³´", total_parsed_rebars)
                            with col4:
                                efficiency = (total_parsed_rebars / total_texts * 100) if total_texts > 0 else 0
                                st.metric("íŒŒì‹± íš¨ìœ¨", f"{efficiency:.1f}%")
                                
                except Exception as e:
                    st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            st.info("ğŸ‘† ë¶„ì„í•  í”¼í´ íŒŒì¼ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")

    # í‘œ ë°ì´í„° ì—‘ì…€ ë³€í™˜ ë©”ì¸ expander

# í‘œ ë°ì´í„° ì—‘ì…€ ë³€í™˜ ë©”ì¸ expander
    with st.expander("ğŸ“Š í‘œ ë°ì´í„° ì—‘ì…€ ë³€í™˜", expanded=False):
        st.markdown("### í”¼í´ íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì—¬ í‘œ í˜•íƒœë¡œ ì—‘ì…€ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤")
        
        # í´ë” ê²½ë¡œ ì…ë ¥
        excel_folder = st.text_input(
            "ğŸ“ ë³€í™˜í•  í”¼í´ íŒŒì¼ í´ë” ê²½ë¡œ",
            placeholder="ì˜ˆ: D:/data/pickle_files",
            help="OCR ì²˜ë¦¬ëœ í‘œ ë°ì´í„°ê°€ ë‹´ê¸´ í”¼í´ íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ",
            key="excel_folder_unique"
        )
        
        if excel_folder:
            try:
                # í´ë” ì¡´ì¬ í™•ì¸
                if not os.path.exists(excel_folder):
                    st.error(f"âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {excel_folder}")
                    st.stop()
                
                # í”¼í´ íŒŒì¼ ì°¾ê¸°
                pickle_files = []
                for file in os.listdir(excel_folder):
                    if file.lower().endswith('.pkl'):
                        pickle_files.append(os.path.join(excel_folder, file))
                
                if not pickle_files:
                    st.warning(f"âš ï¸ í´ë”ì— í”¼í´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {excel_folder}")
                    st.stop()
                
                st.success(f"âœ… {len(pickle_files)}ê°œì˜ í”¼í´ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
                
                # ë³€í™˜í•  íŒŒì¼ ì„ íƒ
                selected_file = st.selectbox(
                    "ë³€í™˜í•  íŒŒì¼ ì„ íƒ",
                    options=pickle_files,
                    format_func=lambda x: os.path.basename(x),
                    key="excel_file_select"
                )
                
                if selected_file:
                    # ì²˜ë¦¬ ì˜µì…˜
                    col1, col2 = st.columns(2)
                    with col1:
                        show_details = st.checkbox("ğŸ” ìƒì„¸ ì²˜ë¦¬ ê³¼ì • ë³´ê¸°", value=False, key="table_excel_details_unique")
                    with col2:
                        process_button = st.button("ğŸš€ ë³€í™˜ ì‹œì‘", type="primary", key="table_excel_process_unique")
                    
                    if process_button:
                        with st.spinner("ğŸ“‹ í‘œ ë°ì´í„°ë¥¼ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤..."):
                            # ì„ íƒëœ íŒŒì¼ì„ íŒŒì¼ ê°ì²´ì²˜ëŸ¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ open
                            with open(selected_file, 'rb') as file_obj:
                                # ì²˜ë¦¬ ì‹¤í–‰
                                excel_data, df, processing_log = process_table_to_excel(
                                    file_obj, show_details
                                )
                            
                            if excel_data is not None:
                                st.success("ğŸ‰ ë³€í™˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                                
                                # ê²°ê³¼ ì •ë³´ (íŒŒì‹± í†µê³„ ì¶”ê°€)
                                st.info(f"ğŸ“ ìµœì¢… ë°ì´í„° í¬ê¸°: {df.shape[0]} x {df.shape[1]}")
                                
                                # íŒŒì‹± í†µê³„ ì •ë³´
                                total_parsed_items = 0
                                parsing_stats = []
                                
                                for index, row in df.iterrows():
                                    non_empty_cells = (row != '').sum()
                                    total_parsed_items += non_empty_cells
                                    parsing_stats.append(f"í–‰ '{index}': {non_empty_cells}ê°œ í•­ëª©")
                                
                                st.success(f"ğŸ¯ ì´ íŒŒì‹±ëœ í•­ëª©: {total_parsed_items}ê°œ")
                                
                                # íŒŒì‹± ìƒì„¸ í†µê³„
                                col1, col2 = st.columns(2)
                                with col1:
                                    st.write("**ğŸ“Š í–‰ë³„ íŒŒì‹± í†µê³„:**")
                                    for stat in parsing_stats[:10]:  # ì²˜ìŒ 10ê°œë§Œ
                                        st.write(f"â€¢ {stat}")
                                    if len(parsing_stats) > 10:
                                        st.write(f"â€¢ ... (ì´ {len(parsing_stats)}ê°œ í–‰)")
                                
                                with col2:
                                    st.write("**ğŸ“ˆ ì—´ë³„ ë°ì´í„° ë¶„í¬:**")
                                    for col_idx in range(min(10, df.shape[1])):  # ì²˜ìŒ 10ì—´ë§Œ
                                        col_data_count = (df.iloc[:, col_idx] != '').sum()
                                        st.write(f"â€¢ ì—´ {col_idx}: {col_data_count}ê°œ")
                                    if df.shape[1] > 10:
                                        st.write(f"â€¢ ... (ì´ {df.shape[1]}ê°œ ì—´)")
                                
                                # ë¯¸ë¦¬ë³´ê¸°
                                st.markdown("### ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
                                st.dataframe(df.head(10), use_container_width=True)
                                
                                # ìƒì„¸ ì²˜ë¦¬ ê³¼ì • í‘œì‹œ (ì¼ë°˜ ì¶œë ¥ìœ¼ë¡œ ë³€ê²½)
                                if show_details and processing_log:
                                    st.markdown("### ğŸ” ìƒì„¸ ì²˜ë¦¬ ê³¼ì •")
                                    # ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì˜ì—­ìœ¼ë¡œ í‘œì‹œ
                                    log_text = "\n".join(processing_log)
                                    st.text_area("ì²˜ë¦¬ ë¡œê·¸", log_text, height=200, key="table_excel_log_unique")
                                
                                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ - ì›ë³¸ íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
                                base_filename = os.path.splitext(os.path.basename(selected_file))[0]
                                excel_filename = f"{base_filename}_í‘œë°ì´í„°.xlsx"
                                
                                st.download_button(
                                    label="ğŸ“¥ ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                                    data=excel_data,
                                    file_name=excel_filename,
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                    key="table_excel_download_unique"
                                )
                                
                                # í†µê³„ ì •ë³´ (ì¼ë°˜ ì¶œë ¥ìœ¼ë¡œ ë³€ê²½)
                                st.markdown("### ğŸ“Š ë³€í™˜ í†µê³„")
                                non_empty_cells = (df != '').sum().sum()
                                total_cells = df.shape[0] * df.shape[1]
                                fill_rate = (non_empty_cells / total_cells * 100) if total_cells > 0 else 0
                                
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("ì „ì²´ ì…€", total_cells)
                                with col2:
                                    st.metric("ë°ì´í„° ìˆëŠ” ì…€", non_empty_cells)
                                with col3:
                                    st.metric("ì±„ì›€ë¥ ", f"{fill_rate:.1f}%")
                            else:
                                st.error("âŒ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                
            except Exception as e:
                st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        else:
            st.info("ğŸ‘† ë³€í™˜í•  í”¼í´ íŒŒì¼ í´ë” ê²½ë¡œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")









    # with st.expander("ê²©ì ì…€ â†’ Excel ë³€í™˜"):
    #     # "Excel ë³€í™˜ ì‹œì‘" ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì²˜ë¦¬ ì‹œì‘
    #     if "show_excel_convert_ui" not in st.session_state:
    #         st.session_state.show_excel_convert_ui = False

    #     if st.button("Excel ë³€í™˜ ì‹œì‘"):
    #         st.session_state.show_excel_convert_ui = True

    #     if st.session_state.show_excel_convert_ui:
    #         input_folder = Slab_text_clean
    #         output_folder = Slab_table_excel
            
    #         st.write("### ê²©ì ì…€ â†’ Excel ë³€í™˜")
            
    #         # ì²« ë²ˆì§¸ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°
    #         st.write("#### ğŸ“‹ ì²« ë²ˆì§¸ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°:")
    #         preview_df = preview_first_grid_file(input_folder)
            
    #         if not preview_df.empty:
    #             # êµ¬ì¡° ë¶„ì„ - ì´ ë¶€ë¶„ë§Œ ìˆ˜ì •ë¨
    #             files = [f for f in os.listdir(input_folder) if f.endswith("_grid_cells.json")]
    #             if files:
    #                 with open(os.path.join(input_folder, files[0]), 'r') as f:
    #                     grid_data = json.load(f)  # â† ì´ë¦„ë§Œ ë³€ê²½
                    
    #                 analysis = analyze_grid_structure(grid_data)  # â† grid_dataë¡œ ì „ë‹¬
                    
    #                 col1, col2 = st.columns(2)
    #                 with col1:
    #                     st.write("**êµ¬ì¡° ì •ë³´:**")
    #                     st.write(f"- ì´ ì…€: {analysis['total_cells']}ê°œ")
    #                     st.write(f"- í…ìŠ¤íŠ¸ ìˆëŠ” ì…€: {analysis['filled_cells']}ê°œ")
    #                     st.write(f"- ë¹ˆ ì…€: {analysis['empty_cells']}ê°œ")
                    
    #                 with col2:
    #                     st.write(f"- í–‰ ë²”ìœ„: {analysis['rows_range'][0]} ~ {analysis['rows_range'][1]}")
    #                     st.write(f"- ì—´ ë²”ìœ„: {analysis['cols_range'][0]} ~ {analysis['cols_range'][1]}")
    #                     st.write(f"- í…Œì´ë¸” í¬ê¸°: {analysis['rows_count']} x {analysis['cols_count']}")
                    
    #                 # ì¶”ê°€ ì •ë³´ í‘œì‹œ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
    #                 st.write("**í–‰ ë¼ë²¨ ì •ë³´:**")
    #                 st.write(f"- ì´ í–‰ ë¼ë²¨: {analysis['total_row_labels']}ê°œ")
    #                 st.write(f"- ê³ ìœ  ë¼ë²¨: {analysis['unique_labels']}ê°œ")
    #                 st.write(f"- ì‹¤ì œ í–‰ ê°œìˆ˜: {analysis['rows_count']}ê°œ")
    #                 if analysis.get('missing_rows'):
    #                     st.write(f"- ëˆ„ë½ëœ í–‰: {analysis['missing_rows']}")
                    
    #                 # ë¼ë²¨ ë¶„í¬ í‘œì‹œ (ìƒìœ„ 5ê°œ)
    #                 if 'label_distribution' in analysis:
    #                     st.write("**ì£¼ìš” ë¼ë²¨ ë¶„í¬:**")
    #                     label_dist = analysis['label_distribution']
    #                     for label, count in list(label_dist.items())[:5]:
    #                         if label != 'Unknown':
    #                             st.write(f"- {label}: {count}ê°œ")
                
    #             st.write("**ë³€í™˜ëœ í…Œì´ë¸”:**")
    #             st.dataframe(preview_df)
                
    #             # ì „ì²´ ë³€í™˜ ë²„íŠ¼
    #             if st.button("ğŸš€ ëª¨ë“  íŒŒì¼ Excel ë³€í™˜"):
    #                 with st.spinner("ë³€í™˜ ì¤‘..."):
    #                     saved_paths = save_all_grid_files(input_folder, output_folder)
                    
    #                 st.success(f"âœ… {len(saved_paths)}ê°œ íŒŒì¼ ë³€í™˜ ì™„ë£Œ!")
    #                 for path in saved_paths:
    #                     st.write(f"ğŸ“„ {os.path.basename(path)}")
    #         else:
    #             st.warning("grid_cells.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            
    #         # "ë³€í™˜ ì™„ë£Œ" ë²„íŠ¼ì„ ë„£ì–´ì„œ ëˆ„ë¥´ë©´ UI ë‹«í˜
    #         if st.button("ë³€í™˜ ì™„ë£Œ"):
    #             st.session_state.show_excel_convert_ui = False






    # with st.expander("ê²©ì ì…€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"):
    #     # "ê²©ì ì¶”ì¶œ ì‹œì‘" ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ ì²˜ë¦¬ ì‹œì‘
    #     if "show_grid_extract_ui" not in st.session_state:
    #         st.session_state.show_grid_extract_ui = False

    #     if st.button("ê²©ì ì¶”ì¶œ ì‹œì‘"):
    #         st.session_state.show_grid_extract_ui = True

    #     if st.session_state.show_grid_extract_ui:
    #         extract_texts_by_cell_with_proper_labels()  # ì´ í•¨ìˆ˜ë§Œ ìœ„ì—ì„œ ì—…ë°ì´íŠ¸í•œ ë²„ì „ìœ¼ë¡œ ë°”ë€œ
    #         # "ê²©ì ì¶”ì¶œ ì™„ë£Œ" ë²„íŠ¼ì„ ë„£ì–´ì„œ ëˆ„ë¥´ë©´ UI ë‹«í˜
    #         if st.button("ê²©ì ì¶”ì¶œ ì™„ë£Œ"):
    #             st.session_state.show_grid_extract_ui = False



















    # with st.expander("í†µí•© ê²€ì¶œ: ì„ /êµì°¨ì /ë™ì¼ì„ ìƒ", expanded=True):
    #     img_dir = st.text_input("ì´ë¯¸ì§€ í´ë”", "D:\slab\raw_data_column_label_SDD")
    #     save_dir = st.text_input("ì €ì¥ í´ë” (ë¹„ìš°ë©´ ë¯¸ì €ì¥)", "D:\slab\Slab_table_region")
    #     boxes_dir = st.text_input("ë°•ìŠ¤ í´ë”", "D:\slab\Slab_same_line")
    #     ocr_dir = st.text_input("OCR í´ë”", "D:\slab\raw_data_OCR_SDD")
    #     mode = st.radio("ë°©ì‹", ["contour", "hough"])
    #     min_line_length = st.slider("min_length", 20, 300, 80, 5)
    #     max_line_gap = st.slider("max_gap", 2, 40, 10, 2)
    #     hough_threshold = st.slider("h_thresh", 30, 300, 100, 5)
    #     morph_kernel_scale = st.slider("morph_scale", 10, 60, 30, 2)
    #     resize_scale = st.slider("resize", 0.3, 1.0, 0.7, 0.05)
    #     block_size = st.slider("blockSize", 7, 31, 15, 2)
    #     C = st.slider("C", -10, 10, -2, 1)
    #     tol = st.slider("tol", 0, 300, 50, 1)
    #     run = st.button("ì˜ˆì‹œ 5ê°œ ë³´ê¸°")
    #     save = st.button("ì „ì²´ ì €ì¥")
    #     if run:
    #         imgs = detect_all_features_batch_Slab_SDD_with_crop(
    #             img_dir, None, boxes_dir, ocr_dir, mode,
    #             min_line_length, max_line_gap, hough_threshold,
    #             morph_kernel_scale, resize_scale, tol, 5, True,
    #             block_size, C
    #         )
    #         if not imgs:
    #             st.warning("ì´ë¯¸ì§€ ì—†ìŒ")
    #         for fn, im, ints in imgs:
    #             st.subheader(f"{fn}: êµì°¨ì  {len(ints)}ê°œ")
    #             st.image(im, use_column_width=True)
    #     if save:
    #         detect_all_features_batch_Slab_SDD_with_crop(
    #             img_dir, save_dir, boxes_dir, ocr_dir, mode,
    #             min_line_length, max_line_gap, hough_threshold,
    #             morph_kernel_scale, resize_scale, tol, None, False,
    #             block_size, C
    #         )
    #         st.success(f"ì €ì¥ë¨: {save_dir}")




    # with st.expander("ğŸ‘ï¸ ë°”ìš´ë”©ë°•ìŠ¤ ì‹œê°í™”"):
    #     if "show_viz_ui" not in st.session_state:
    #         st.session_state.show_viz_ui = False

    #     if st.button("ì‹œê°í™” ì‹œì‘"):
    #         st.session_state.show_viz_ui = True

    #     if st.session_state.show_viz_ui:
    #         visualize_saved_bounding_boxes(selectbox_key="viz_select")
            
    #         if st.button("ì‹œê°í™” ì™„ë£Œ"):
    #             st.session_state.show_viz_ui = False


    # with st.expander("ìˆ˜í‰/ìˆ˜ì§ì„  + êµì°¨ì  ì¸ì‹", expanded=False):
    #     img_dir = st.text_input("ì…ë ¥ ì´ë¯¸ì§€ í´ë” ê²½ë¡œ", value=r"D:\slab\Slab_table_region")
    #     save_dir = st.text_input("ê²°ê³¼ ì €ì¥ í´ë”ëª… (ë¹„ìš°ë©´ ì €ì¥ X)", value=r"D:\slab\Slab_table_region\lines_detected")
    #     mode = st.radio("ì„  ê²€ì¶œ ë°©ì‹", options=["contour", "hough"], index=0)
    #     min_line_length = st.slider("min_line_length", 20, 300, 80, 5)
    #     max_line_gap = st.slider("max_line_gap", 2, 40, 10, 2)
    #     hough_threshold = st.slider("Hough threshold", 30, 300, 100, 5)
    #     morph_kernel_scale = st.slider("Morph kernel ë¹„ìœ¨", 10, 60, 30, 2)
    #     resize_scale = st.slider("ì´ë¯¸ì§€ ì¶•ì†Œ ë¹„ìœ¨ (1=ì›ë³¸)", 0.3, 1.0, 0.7, 0.05)
    #     block_size = st.slider("adaptiveThreshold blockSize(í™€ìˆ˜)", 7, 31, 15, 2)
    #     C = st.slider("adaptiveThreshold C", -10, 10, -2, 1)
    #     tol = st.slider("êµì°¨ì  í—ˆìš© ì˜¤ì°¨ (tol)", 0, 10, 2, 1)

    #     run_btn = st.button("ì˜ˆì‹œ 5ê°œ ë³´ê¸°")
    #     save_btn = st.button("ì „ì²´ ì €ì¥")

    #     if run_btn:
    #         imgs = detect_and_draw_lines_batch_Slab_SDD(
    #             img_dir=img_dir,
    #             save_dir=None,
    #             min_line_length=min_line_length,
    #             max_line_gap=max_line_gap,
    #             hough_threshold=hough_threshold,
    #             morph_kernel_scale=morph_kernel_scale,
    #             resize_scale=resize_scale,
    #             max_examples=5,
    #             return_imgs=True,
    #             mode=mode,
    #             block_size=block_size | 1,
    #             C=C,
    #             tol=tol
    #         )
    #         if not imgs:
    #             st.warning("ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
    #         else:
    #             for fname, img, inter_cnt in imgs:
    #                 st.subheader(f"{fname} (êµì°¨ì : {inter_cnt}ê°œ)")
    #                 st.image(cv2.cvtColor(img, cv2.COLOR_BGR2RGB), caption="ê²€ì¶œ ê²°ê³¼", use_column_width=True)

    #     if save_btn:
    #         detect_and_draw_lines_batch_Slab_SDD(
    #             img_dir=img_dir,
    #             save_dir=save_dir if save_dir else None,
    #             min_line_length=min_line_length,
    #             max_line_gap=max_line_gap,
    #             hough_threshold=hough_threshold,
    #             morph_kernel_scale=morph_kernel_scale,
    #             resize_scale=resize_scale,
    #             max_examples=None,
    #             return_imgs=False,
    #             mode=mode,
    #             block_size=block_size | 1,
    #             C=C,
    #             tol=tol
    #         )
    #         st.success(f"ì „ì²´ ì´ë¯¸ì§€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ! â†’ '{save_dir}' í´ë” í™•ì¸")




    # with st.expander("3) í†µí•© ì—‘ì…€ íŒŒì¼ ìƒì„±", expanded=False):
    #     img_dir_cons = st.text_input("ì…ë ¥ ì´ë¯¸ì§€ í´ë”", value=r"D:\slab\Slab_table_region")
    #     save_line_dir_cons = st.text_input("ë¼ì¸ pkl í´ë”", value=r"D:\slab\Slab_table_region\lines_detected")
    #     ocr_dir_cons = st.text_input("OCR json í´ë”", value=r"D:\slab\Slab_table_crop_OCR")
    #     consolidated_path = st.text_input("í†µí•© ì—‘ì…€ íŒŒì¼ ê²½ë¡œ", value=r"D:\slab\Slab_table_region\All_In_One.xlsx")
    #     if st.button("í†µí•© ì—‘ì…€ ì¶”ì¶œ"):
    #         batch_extract_consolidated_Slab_SDD(
    #             img_dir=img_dir_cons,
    #             save_line_dir=save_line_dir_cons,
    #             ocr_dir=ocr_dir_cons,
    #             consolidated_path=consolidated_path,
    #             mode=mode,
    #             min_line_length=min_line_length,
    #             max_line_gap=max_line_gap,
    #             hough_threshold=hough_threshold,
    #             morph_kernel_scale=morph_kernel_scale,
    #             resize_scale=resize_scale,
    #             block_size=block_size|1,
    #             C=C
    #         )
    #         st.success(f"í†µí•© ì—‘ì…€ íŒŒì¼ ìƒì„± ì™„ë£Œ â†’ {consolidated_path}")[


























